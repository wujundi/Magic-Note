{
    "configurations":[
        {
            "cluster-env":{
                "properties_attributes":{

                },
                "properties":{
                    "alerts_repeat_tolerance":"1",
                    "fetch_nonlocal_groups":"true",
                    "ignore_bad_mounts":"false",
                    "ignore_groupsusers_create":"false",
                    "kerberos_domain":"EXAMPLE.COM",
                    "manage_dirs_on_root":"true",
                    "managed_hdfs_resource_property_names":"",
                    "one_dir_per_partition":"false",
                    "override_uid":"true",
                    "recovery_enabled":"true",
                    "recovery_lifetime_max_count":"1024",
                    "recovery_max_count":"6",
                    "recovery_retry_interval":"5",
                    "recovery_type":"AUTO_START",
                    "recovery_window_in_minutes":"60",
                    "repo_suse_rhel_template":"[{{repo_id}}]\nname={{repo_id}}\n{% if mirror_list %}mirrorlist={{mirror_list}}{% else %}baseurl={{base_url}}{% endif %}\n\npath=/\nenabled=1\ngpgcheck=0",
                    "repo_ubuntu_template":"{{package_type}} {{base_url}} {{components}}",
                    "security_enabled":"false",
                    "smokeuser":"ambari-qa",
                    "smokeuser_keytab":"/etc/security/keytabs/smokeuser.headless.keytab",
                    "stack_features":"{\n  \"BIGTOP\": {\n    \"stack_features\": [\n      {\n        \"name\": \"lzo\",\n        \"description\": \"LZO libraries support\",\n        \"min_version\": \"3.2.0\"\n      },\n      {\n        \"name\": \"copy_tarball_to_hdfs\",\n        \"description\": \"Copy tarball to HDFS support (AMBARI-12113)\",\n        \"min_version\": \"3.2.0\"\n      },\n      {\n        \"name\": \"hive_metastore_upgrade_schema\",\n        \"description\": \"Hive metastore upgrade schema support (AMBARI-11176)\",\n        \"min_version\": \"3.2.0\"\n      },\n      {\n        \"name\": \"hive_webhcat_specific_configs\",\n        \"description\": \"Hive webhcat specific configurations support (AMBARI-12364)\",\n        \"min_version\": \"3.2.0\"\n      },\n      {\n        \"name\": \"hive_purge_table\",\n        \"description\": \"Hive purge table support (AMBARI-12260)\",\n        \"min_version\": \"3.2.0\"\n      },\n      {\n        \"name\": \"hive_server2_kerberized_env\",\n        \"description\": \"Hive server2 working on kerberized environment (AMBARI-13749)\",\n        \"min_version\": \"3.2.0\"\n      },\n      {\n        \"name\": \"hive_env_heapsize\",\n        \"description\": \"Hive heapsize property defined in hive-env (AMBARI-12801)\",\n        \"min_version\": \"3.2.0\"\n      },\n      {\n        \"name\": \"hive_metastore_site_support\",\n        \"description\": \"Hive Metastore site support\",\n        \"min_version\": \"3.2.0\"\n      },\n      {\n        \"name\": \"kafka_kerberos\",\n        \"description\": \"Kafka Kerberos support (AMBARI-10984)\",\n        \"min_version\": \"3.2.0\"\n      },\n      {\n        \"name\": \"rolling_upgrade\",\n        \"description\": \"Rolling upgrade support\",\n        \"min_version\": \"3.2.0\"\n      },\n      {\n        \"name\": \"kafka_listeners\",\n        \"description\": \"Kafka listeners (AMBARI-10984)\",\n        \"min_version\": \"3.2.0\"\n      }\n    ]\n  }\n}",
                    "stack_packages":"{\n  \"BIGTOP\": {\n    \"stack-select\": {\n      \"HBASE\": {\n        \"HBASE_CLIENT\": {\n          \"STACK-SELECT-PACKAGE\": \"hbase-client\",\n          \"INSTALL\": [\n            \"hbase-client\"\n          ],\n          \"PATCH\": [\n            \"hbase-client\"\n          ],\n          \"STANDARD\": [\n            \"hbase-client\",\n            \"phoenix-client\",\n            \"hadoop-client\"\n          ]\n        },\n        \"HBASE_MASTER\": {\n          \"STACK-SELECT-PACKAGE\": \"hbase-master\",\n          \"INSTALL\": [\n            \"hbase-master\",\n            \"hbase-client\"\n          ],\n          \"PATCH\": [\n            \"hbase-master\"\n          ],\n          \"STANDARD\": [\n            \"hbase-master\"\n          ]\n        },\n        \"HBASE_REGIONSERVER\": {\n          \"STACK-SELECT-PACKAGE\": \"hbase-regionserver\",\n          \"INSTALL\": [\n            \"hbase-regionserver\",\n            \"hbase-client\"\n          ],\n          \"PATCH\": [\n            \"hbase-regionserver\"\n          ],\n          \"STANDARD\": [\n            \"hbase-regionserver\"\n          ]\n        }\n      },\n      \"HDFS\": {\n        \"DATANODE\": {\n          \"STACK-SELECT-PACKAGE\": \"hadoop-hdfs-datanode\",\n          \"INSTALL\": [\n            \"hadoop-hdfs-datanode\",\n            \"hadoop-client\"\n          ],\n          \"PATCH\": [\n            \"hadoop-hdfs-datanode\"\n          ],\n          \"STANDARD\": [\n            \"hadoop-hdfs-datanode\"\n          ]\n        },\n        \"HDFS_CLIENT\": {\n          \"STACK-SELECT-PACKAGE\": \"hadoop-hdfs-client\",\n          \"INSTALL\": [\n            \"hadoop-hdfs-client\",\n            \"hadoop-client\"\n          ],\n          \"PATCH\": [\n            \"hadoop-hdfs-client\"\n          ],\n          \"STANDARD\": [\n            \"hadoop-client\"\n          ]\n        },\n        \"NAMENODE\": {\n          \"STACK-SELECT-PACKAGE\": \"hadoop-hdfs-namenode\",\n          \"INSTALL\": [\n            \"hadoop-hdfs-namenode\",\n            \"hadoop-client\"\n          ],\n          \"PATCH\": [\n            \"hadoop-hdfs-namenode\"\n          ],\n          \"STANDARD\": [\n            \"hadoop-hdfs-namenode\"\n          ]\n        },\n        \"JOURNALNODE\": {\n          \"STACK-SELECT-PACKAGE\": \"hadoop-hdfs-journalnode\",\n          \"INSTALL\": [\n            \"hadoop-hdfs-journalnode\",\n            \"hadoop-client\"\n          ],\n          \"PATCH\": [\n            \"hadoop-hdfs-journalnode\"\n          ],\n          \"STANDARD\": [\n            \"hadoop-hdfs-journalnode\"\n          ]\n        },\n        \"SECONDARY_NAMENODE\": {\n          \"STACK-SELECT-PACKAGE\": \"hadoop-hdfs-secondarynamenode\",\n          \"INSTALL\": [\n            \"hadoop-hdfs-secondarynamenode\",\n            \"hadoop-client\"\n          ],\n          \"PATCH\": [\n            \"hadoop-hdfs-secondarynamenode\"\n          ],\n          \"STANDARD\": [\n            \"hadoop-hdfs-secondarynamenode\"\n          ]\n        },\n        \"ZKFC\": {\n          \"STACK-SELECT-PACKAGE\": \"hadoop-hdfs-zkfc\",\n          \"INSTALL\": [\n            \"hadoop-hdfs-zkfc\",\n            \"hadoop-client\"\n          ],\n          \"PATCH\": [\n            \"hadoop-hdfs-zkfc\"\n          ],\n          \"STANDARD\": [\n            \"hadoop-hdfs-zkfc\"\n          ]\n        }\n      },\n      \"HIVE\": {\n        \"HIVE_METASTORE\": {\n          \"STACK-SELECT-PACKAGE\": \"hive-metastore\",\n          \"INSTALL\": [\n            \"hive-metastore\",\n            \"hive-client\"\n          ],\n          \"PATCH\": [\n            \"hive-metastore\"\n          ],\n          \"STANDARD\": [\n            \"hive-metastore\"\n          ]\n        },\n        \"HIVE_SERVER\": {\n          \"STACK-SELECT-PACKAGE\": \"hive-server2\",\n          \"INSTALL\": [\n            \"hive-server2\",\n            \"hive-client\"\n          ],\n          \"PATCH\": [\n            \"hive-server2\"\n          ],\n          \"STANDARD\": [\n            \"hive-server2\"\n          ]\n        },\n        \"HIVE_CLIENT\": {\n          \"STACK-SELECT-PACKAGE\": \"hive-client\",\n          \"INSTALL\": [\n            \"hive-client\"\n          ],\n          \"PATCH\": [\n            \"hive-client\"\n          ],\n          \"STANDARD\": [\n            \"hadoop-client\"\n          ]\n        },\n        \"WEBHCAT_SERVER\": {\n          \"STACK-SELECT-PACKAGE\": \"hive-webhcat\",\n          \"INSTALL\": [\n            \"hive-webhcat\",\n            \"hive-client\"\n          ],\n          \"PATCH\": [\n            \"hive-webhcat\"\n          ],\n          \"STANDARD\": [\n            \"hive-webhcat\"\n          ]\n        },\n        \"HCAT\": {\n          \"STACK-SELECT-PACKAGE\": \"hive-webhcat\",\n          \"INSTALL\": [\n            \"hive-webhcat\",\n            \"hive-client\"\n          ],\n          \"PATCH\": [\n            \"hive-webhcat\"\n          ],\n          \"STANDARD\": [\n            \"hive-webhcat\"\n          ]\n        }\n      },\n      \"KAFKA\": {\n        \"KAFKA_BROKER\": {\n          \"STACK-SELECT-PACKAGE\": \"kafka-broker\",\n          \"INSTALL\": [\n            \"kafka-broker\"\n          ],\n          \"PATCH\": [\n            \"kafka-broker\"\n          ],\n          \"STANDARD\": [\n            \"kafka-broker\"\n          ]\n        }\n      },\n      \"MAPREDUCE2\": {\n        \"HISTORYSERVER\": {\n          \"STACK-SELECT-PACKAGE\": \"hadoop-mapreduce-historyserver\",\n          \"INSTALL\": [\n            \"hadoop-mapreduce-historyserver\",\n            \"hadoop-client\"\n          ],\n          \"PATCH\": [\n            \"hadoop-mapreduce-historyserver\"\n          ],\n          \"STANDARD\": [\n            \"hadoop-mapreduce-historyserver\"\n          ]\n        },\n        \"MAPREDUCE2_CLIENT\": {\n          \"STACK-SELECT-PACKAGE\": \"hadoop-mapreduce-client\",\n          \"INSTALL\": [\n            \"hadoop-mapreduce-client\",\n            \"hadoop-client\"\n          ],\n          \"PATCH\": [\n            \"hadoop-mapreduce-client\"\n          ],\n          \"STANDARD\": [\n            \"hadoop-client\"\n          ]\n        }\n      },\n      \"SPARK\": {\n        \"SPARK_CLIENT\": {\n          \"STACK-SELECT-PACKAGE\": \"spark-client\",\n          \"INSTALL\": [\n            \"spark-client\"\n          ],\n          \"PATCH\": [\n            \"spark-client\"\n          ],\n          \"STANDARD\": [\n            \"spark-client\"\n          ]\n        },\n        \"SPARK_JOBHISTORYSERVER\": {\n          \"STACK-SELECT-PACKAGE\": \"spark-historyserver\",\n          \"INSTALL\": [\n            \"spark-historyserver\",\n            \"spark-client\"\n          ],\n          \"PATCH\": [\n            \"spark-historyserver\"\n          ],\n          \"STANDARD\": [\n            \"spark-historyserver\"\n          ]\n        },\n        \"SPARK_THRIFTSERVER\": {\n          \"STACK-SELECT-PACKAGE\": \"spark-thriftserver\",\n          \"INSTALL\": [\n            \"spark-thriftserver\",\n            \"spark-client\"\n          ],\n          \"PATCH\": [\n            \"spark-thriftserver\"\n          ],\n          \"STANDARD\": [\n            \"spark-thriftserver\"\n          ]\n        }\n      },\n      \"FLINK\": {\n        \"FLINK_CLIENT\": {\n          \"STACK-SELECT-PACKAGE\": \"flink-client\",\n          \"INSTALL\": [\n            \"flink-client\"\n          ],\n          \"PATCH\": [\n            \"flink-client\"\n          ],\n          \"STANDARD\": [\n            \"flink-client\"\n          ]\n        },\n        \"FLINK_HISTORYSERVER\": {\n          \"STACK-SELECT-PACKAGE\": \"flink-historyserver\",\n          \"INSTALL\": [\n            \"flink-historyserver\",\n            \"flink-client\"\n          ],\n          \"PATCH\": [\n            \"flink-historyserver\"\n          ],\n          \"STANDARD\": [\n            \"flink-historyserver\"\n          ]\n        }\n      },\n      \"SOLR\": {\n        \"SOLR_SERVER\": {\n          \"STACK-SELECT-PACKAGE\": \"solr-server\",\n          \"INSTALL\": [\n            \"solr-server\"\n          ],\n          \"PATCH\": [\n            \"solr-server\"\n          ],\n          \"STANDARD\": [\n            \"solr-server\"\n          ]\n        }\n      },\n      \"TEZ\": {\n        \"TEZ_CLIENT\": {\n          \"STACK-SELECT-PACKAGE\": \"tez-client\",\n          \"INSTALL\": [\n            \"tez-client\"\n          ],\n          \"PATCH\": [\n            \"tez-client\"\n          ],\n          \"STANDARD\": [\n            \"hadoop-client\"\n          ]\n        }\n      },\n      \"YARN\": {\n        \"NODEMANAGER\": {\n          \"STACK-SELECT-PACKAGE\": \"hadoop-yarn-nodemanager\",\n          \"INSTALL\": [\n            \"hadoop-yarn-nodemanager\",\n            \"hadoop-client\"\n          ],\n          \"PATCH\": [\n            \"hadoop-yarn-nodemanager\"\n          ],\n          \"STANDARD\": [\n            \"hadoop-yarn-nodemanager\"\n          ]\n        },\n        \"RESOURCEMANAGER\": {\n          \"STACK-SELECT-PACKAGE\": \"hadoop-yarn-resourcemanager\",\n          \"INSTALL\": [\n            \"hadoop-yarn-resourcemanager\",\n            \"hadoop-client\"\n          ],\n          \"PATCH\": [\n            \"hadoop-yarn-resourcemanager\"\n          ],\n          \"STANDARD\": [\n            \"hadoop-yarn-resourcemanager\"\n          ]\n        },\n        \"YARN_CLIENT\": {\n          \"STACK-SELECT-PACKAGE\": \"hadoop-yarn-client\",\n          \"INSTALL\": [\n            \"hadoop-yarn-client\",\n            \"hadoop-client\"\n          ],\n          \"PATCH\": [\n            \"hadoop-yarn-client\"\n          ],\n          \"STANDARD\": [\n            \"hadoop-client\"\n          ]\n        }\n      },\n      \"ZEPPELIN\": {\n        \"ZEPPELIN_SERVER\": {\n          \"STACK-SELECT-PACKAGE\": \"zeppelin-server\",\n          \"INSTALL\": [\n            \"zeppelin-server\"\n          ],\n          \"PATCH\": [\n            \"zeppelin-server\"\n          ],\n          \"STANDARD\": [\n            \"zeppelin-server\"\n          ]\n        }\n      },\n      \"ZOOKEEPER\": {\n        \"ZOOKEEPER_CLIENT\": {\n          \"STACK-SELECT-PACKAGE\": \"zookeeper-client\",\n          \"INSTALL\": [\n            \"zookeeper-client\"\n          ],\n          \"PATCH\": [\n            \"zookeeper-client\"\n          ],\n          \"STANDARD\": [\n            \"zookeeper-client\"\n          ]\n        },\n        \"ZOOKEEPER_SERVER\": {\n          \"STACK-SELECT-PACKAGE\": \"zookeeper-server\",\n          \"INSTALL\": [\n            \"zookeeper-server\"\n          ],\n          \"PATCH\": [\n            \"zookeeper-server\"\n          ],\n          \"STANDARD\": [\n            \"zookeeper-server\"\n          ]\n        }\n      }\n    },\n    \"conf-select\": {\n      \"hadoop\": [\n        {\n          \"conf_dir\": \"/etc/hadoop/conf\",\n          \"current_dir\": \"{0}/current/hadoop-client/conf\",\n          \"component\": \"hadoop-client\"\n        }\n      ],\n      \"hbase\": [\n        {\n          \"conf_dir\": \"/etc/hbase/conf\",\n          \"current_dir\": \"{0}/current/hbase-client/conf\",\n          \"component\": \"hbase-client\"\n        }\n      ],\n      \"hive\": [\n        {\n          \"conf_dir\": \"/etc/hive/conf\",\n          \"current_dir\": \"{0}/current/hive-client/conf\",\n          \"component\": \"hive-client\"\n        }\n      ],\n      \"hive-hcatalog\": [\n        {\n          \"conf_dir\": \"/etc/hive-webhcat/conf\",\n          \"prefix\": \"/etc/hive-webhcat\",\n          \"current_dir\": \"{0}/current/hive-webhcat/etc/webhcat\",\n          \"component\": \"hive-webhcat\"\n        },\n        {\n          \"conf_dir\": \"/etc/hive-hcatalog/conf\",\n          \"prefix\": \"/etc/hive-hcatalog\",\n          \"current_dir\": \"{0}/current/hive-webhcat/etc/hcatalog\",\n          \"component\": \"hive-webhcat\"\n        }\n      ],\n      \"kafka\": [\n        {\n          \"conf_dir\": \"/etc/kafka/conf\",\n          \"current_dir\": \"{0}/current/kafka-broker/conf\",\n          \"component\": \"kafka-broker\"\n        }\n      ],\n      \"phoenix\": [\n        {\n          \"conf_dir\": \"/etc/phoenix/conf\",\n          \"current_dir\": \"{0}/current/phoenix-client/conf\",\n          \"component\": \"phoenix-client\"\n        }\n      ],\n      \"spark\": [\n        {\n          \"conf_dir\": \"/etc/spark/conf\",\n          \"current_dir\": \"{0}/current/spark-client/conf\",\n          \"component\": \"spark-client\"\n        }\n      ],\n      \"flink\": [\n        {\n          \"conf_dir\": \"/etc/flink/conf\",\n          \"current_dir\": \"{0}/current/flink-client/conf\",\n          \"component\": \"flink-client\"\n        }\n      ],\n      \"solr\": [\n        {\n          \"conf_dir\": \"/etc/solr/conf\",\n          \"current_dir\": \"{0}/current/solr-server/conf\",\n          \"component\": \"solr-server\"\n        }\n      ],\n      \"tez\": [\n        {\n          \"conf_dir\": \"/etc/tez/conf\",\n          \"current_dir\": \"{0}/current/tez-client/conf\",\n          \"component\": \"tez-client\"\n        }\n      ],\n      \"zeppelin\": [\n        {\n          \"conf_dir\": \"/etc/zeppelin/conf\",\n          \"current_dir\": \"{0}/current/zeppelin-server/conf\",\n          \"component\": \"zeppelin-server\"\n        }\n      ],\n      \"zookeeper\": [\n        {\n          \"conf_dir\": \"/etc/zookeeper/conf\",\n          \"current_dir\": \"{0}/current/zookeeper-client/conf\",\n          \"component\": \"zookeeper-client\"\n        }\n      ]\n    },\n    \"conf-select-patching\": {\n      \"HBASE\": {\n        \"packages\": [\n          \"hbase\"\n        ]\n      },\n      \"HDFS\": {\n        \"packages\": []\n      },\n      \"HIVE\": {\n        \"packages\": [\n          \"hive\",\n          \"hive-hcatalog\"\n        ]\n      },\n      \"KAFKA\": {\n        \"packages\": [\n          \"kafka\"\n        ]\n      },\n      \"MAPREDUCE2\": {\n        \"packages\": []\n      },\n      \"SPARK\": {\n        \"packages\": [\n          \"spark\"\n        ]\n      },\n      \"FLINK\": {\n        \"packages\": [\n          \"flink\"\n        ]\n      },\n      \"SOLR\": {\n        \"packages\": [\n          \"solr\"\n        ]\n      },\n      \"TEZ\": {\n        \"packages\": [\n          \"tez\"\n        ]\n      },\n      \"YARN\": {\n        \"packages\": []\n      },\n      \"ZEPPELIN\": {\n        \"packages\": [\n          \"zeppelin\"\n        ]\n      },\n      \"ZOOKEEPER\": {\n        \"packages\": [\n          \"zookeeper\"\n        ]\n      }\n    },\n    \"upgrade-dependencies\": {\n      \"HIVE\": [\n        \"TEZ\",\n        \"MAPREDUCE2\"\n      ],\n      \"TEZ\": [\n        \"HIVE\"\n      ],\n      \"MAPREDUCE2\": [\n        \"HIVE\"\n      ]\n    }\n  }\n}",
                    "stack_root":"{\"BIGTOP\":\"/usr/bigtop\"}",
                    "stack_tools":"{\n  \"BIGTOP\": {\n    \"stack_selector\": [\n      \"distro-select\",\n      \"/usr/lib/bigtop-select/distro-select\",\n      \"bigtop-select\"\n    ],\n    \"conf_selector\": [\n      \"conf-select\",\n      \"/usr/lib/bigtop-select/conf-select\",\n      \"conf-select\"\n    ]\n  }\n}",
                    "sysprep_skip_copy_tarballs_hdfs":"false",
                    "sysprep_skip_create_users_and_groups":"false",
                    "sysprep_skip_hive_schema_create":"false",
                    "user_group":"hadoop"
                }
            }
        },
        {
            "core-site":{
                "properties_attributes":{

                },
                "properties":{
                    "hadoop.proxyuser.hdfs.groups":"*",
                    "hadoop.proxyuser.hdfs.hosts":"*",
                    "hadoop.proxyuser.hcat.hosts":"ambari-server",
                    "hadoop.proxyuser.hive.groups":"*",
                    "hadoop.proxyuser.root.groups":"*",
                    "hadoop.proxyuser.hcat.groups":"*",
                    "hadoop.proxyuser.root.hosts":"ambari-server",
                    "hadoop.proxyuser.hive.hosts":"ambari-server",
                    "fs.azure.user.agent.prefix":"User-Agent: APN/1.0 Hortonworks/1.0 HDP/{{version}}",
                    "fs.defaultFS":"hdfs://ambari-server:8020",
                    "fs.gs.application.name.suffix":" (GPN:Hortonworks; version 1.0) HDP/{{version}}",
                    "fs.gs.path.encoding":"uri-path",
                    "fs.gs.working.dir":"/",
                    "fs.s3a.user.agent.prefix":"User-Agent: APN/1.0 Hortonworks/1.0 HDP/{{version}}",
                    "fs.trash.interval":"360",
                    "ha.failover-controller.active-standby-elector.zk.op.retries":"120",
                    "hadoop.http.authentication.type":"simple",
                    "hadoop.proxyuser.*":"root",
                    "hadoop.security.auth_to_local":"DEFAULT",
                    "hadoop.security.authentication":"simple",
                    "hadoop.security.authorization":"false",
                    "io.compression.codecs":"org.apache.hadoop.io.compress.GzipCodec,org.apache.hadoop.io.compress.DefaultCodec,org.apache.hadoop.io.compress.SnappyCodec",
                    "io.file.buffer.size":"131072",
                    "io.serializations":"org.apache.hadoop.io.serializer.WritableSerialization",
                    "ipc.client.connect.max.retries":"50",
                    "ipc.client.connection.maxidletime":"30000",
                    "ipc.client.idlethreshold":"8000",
                    "ipc.server.tcpnodelay":"true",
                    "mapreduce.jobtracker.webinterface.trusted":"false",
                    "net.topology.script.file.name":"/etc/hadoop/conf/topology_script.py"
                }
            }
        },
        {
            "hadoop-env":{
                "properties_attributes":{

                },
                "properties":{
                    "content":"\n# Set Hadoop-specific environment variables here.\n\n# The only required environment variable is JAVA_HOME.  All others are\n# optional.  When running a distributed configuration it is best to\n# set JAVA_HOME in this file, so that it is correctly defined on\n# remote nodes.\n\n# The java implementation to use.  Required.\nexport JAVA_HOME={{java_home}}\nexport HADOOP_HOME_WARN_SUPPRESS=1\n\n# Hadoop home directory\nexport HADOOP_HOME=${HADOOP_HOME:-{{hadoop_home}}}\nexport HADOOP_COMMON_HOME={{hadoop_home}}\nexport HADOOP_HDFS_HOME={{hadoop_hdfs_home}}\nexport HADOOP_MAPRED_HOME={{hadoop_mapred_home}}\nexport HADOOP_YARN_HOME={{hadoop_yarn_home}}\n\n# Hadoop Configuration Directory\n#TODO: if env var set that can cause problems\nexport HADOOP_CONF_DIR=${HADOOP_CONF_DIR:-{{hadoop_conf_dir}}}\n\n\n# Path to jsvc required by secure datanode\nexport JSVC_HOME={{jsvc_path}}\n\n\n# The maximum amount of heap to use, in MB. Default is 1000.\nif [[ (\"$SERVICE\" = \"hiveserver2\") || (\"$SERVICE\" = \"metastore\") || ( \"$SERVICE\" = \"cli\") ]]; then\n  if [ \"$HADOOP_HEAPSIZE\" = \"\" ]; then\n    export HADOOP_HEAPSIZE=\"{{hadoop_heapsize}}\"\n  fi\nelse\n  export HADOOP_HEAPSIZE=\"{{hadoop_heapsize}}\"\nfi\n\n\nexport HADOOP_NAMENODE_INIT_HEAPSIZE=\"-Xms{{namenode_heapsize}}\"\n\n{% if zk_principal_user is defined %}\nHADOOP_OPTS=\"-Dzookeeper.sasl.client.username={{zk_principal_user}} $HADOOP_OPTS\"\n{% endif %}\n\n# Extra Java runtime options.  Empty by default.\nexport HADOOP_OPTS=\"-Djava.net.preferIPv4Stack=true ${HADOOP_OPTS}\"\n\nUSER=\"$(whoami)\"\n# Command specific options appended to HADOOP_OPTS when specified\n\n{% if java_version < 8 %}\nexport HADOOP_NAMENODE_OPTS=\"-server -XX:ParallelGCThreads=8 -XX:+UseConcMarkSweepGC -XX:ErrorFile={{hdfs_log_dir_prefix}}/$USER/hs_err_pid%p.log -XX:NewSize={{namenode_opt_newsize}} -XX:MaxNewSize={{namenode_opt_maxnewsize}} -XX:PermSize={{namenode_opt_permsize}} -XX:MaxPermSize={{namenode_opt_maxpermsize}} -Xloggc:{{hdfs_log_dir_prefix}}/$USER/gc.log-`date +'%Y%m%d%H%M'` -verbose:gc -XX:+PrintGCDetails -XX:+PrintGCTimeStamps -XX:+PrintGCDateStamps -XX:CMSInitiatingOccupancyFraction=70 -XX:+UseCMSInitiatingOccupancyOnly -Xms{{namenode_heapsize}} -Xmx{{namenode_heapsize}} -Dhadoop.security.logger=INFO,DRFAS -Dhdfs.audit.logger=INFO,DRFAAUDIT -Dorg.mortbay.jetty.Request.maxFormContentSize=-1  ${HADOOP_NAMENODE_OPTS}\"\nexport HADOOP_SECONDARYNAMENODE_OPTS=\"-server -XX:ParallelGCThreads=8 -XX:+UseConcMarkSweepGC -XX:ErrorFile={{hdfs_log_dir_prefix}}/$USER/hs_err_pid%p.log -XX:NewSize={{namenode_opt_newsize}} -XX:MaxNewSize={{namenode_opt_maxnewsize}} -XX:PermSize={{namenode_opt_permsize}} -XX:MaxPermSize={{namenode_opt_maxpermsize}} -Xloggc:{{hdfs_log_dir_prefix}}/$USER/gc.log-`date +'%Y%m%d%H%M'` -verbose:gc -XX:+PrintGCDetails -XX:+PrintGCTimeStamps -XX:+PrintGCDateStamps -XX:CMSInitiatingOccupancyFraction=70 -XX:+UseCMSInitiatingOccupancyOnly ${HADOOP_NAMENODE_INIT_HEAPSIZE} -Xmx{{namenode_heapsize}} -Dhadoop.security.logger=INFO,DRFAS -Dhdfs.audit.logger=INFO,DRFAAUDIT ${HADOOP_SECONDARYNAMENODE_OPTS}\"\n\n# The following applies to multiple commands (fs, dfs, fsck, distcp etc)\nexport HADOOP_CLIENT_OPTS=\"-Xmx${HADOOP_HEAPSIZE}m -XX:MaxPermSize=512m $HADOOP_CLIENT_OPTS\"\n{% else %}\nexport HADOOP_NAMENODE_OPTS=\"-server -XX:ParallelGCThreads=8 -XX:+UseConcMarkSweepGC -XX:ErrorFile={{hdfs_log_dir_prefix}}/$USER/hs_err_pid%p.log -XX:NewSize={{namenode_opt_newsize}} -XX:MaxNewSize={{namenode_opt_maxnewsize}} -Xloggc:{{hdfs_log_dir_prefix}}/$USER/gc.log-`date +'%Y%m%d%H%M'` -verbose:gc -XX:+PrintGCDetails -XX:+PrintGCTimeStamps -XX:+PrintGCDateStamps -XX:CMSInitiatingOccupancyFraction=70 -XX:+UseCMSInitiatingOccupancyOnly -Xms{{namenode_heapsize}} -Xmx{{namenode_heapsize}} -Dhadoop.security.logger=INFO,DRFAS -Dhdfs.audit.logger=INFO,DRFAAUDIT -Dorg.mortbay.jetty.Request.maxFormContentSize=-1 ${HADOOP_NAMENODE_OPTS}\"\nexport HADOOP_SECONDARYNAMENODE_OPTS=\"-server -XX:ParallelGCThreads=8 -XX:+UseConcMarkSweepGC -XX:ErrorFile={{hdfs_log_dir_prefix}}/$USER/hs_err_pid%p.log -XX:NewSize={{namenode_opt_newsize}} -XX:MaxNewSize={{namenode_opt_maxnewsize}} -Xloggc:{{hdfs_log_dir_prefix}}/$USER/gc.log-`date +'%Y%m%d%H%M'` -verbose:gc -XX:+PrintGCDetails -XX:+PrintGCTimeStamps -XX:+PrintGCDateStamps -XX:CMSInitiatingOccupancyFraction=70 -XX:+UseCMSInitiatingOccupancyOnly ${HADOOP_NAMENODE_INIT_HEAPSIZE} -Xmx{{namenode_heapsize}} -Dhadoop.security.logger=INFO,DRFAS -Dhdfs.audit.logger=INFO,DRFAAUDIT ${HADOOP_SECONDARYNAMENODE_OPTS}\"\n\n# The following applies to multiple commands (fs, dfs, fsck, distcp etc)\nexport HADOOP_CLIENT_OPTS=\"-Xmx${HADOOP_HEAPSIZE}m $HADOOP_CLIENT_OPTS\"\n{% endif %}\nHADOOP_JOBTRACKER_OPTS=\"-server -XX:ParallelGCThreads=8 -XX:+UseConcMarkSweepGC -XX:ErrorFile={{hdfs_log_dir_prefix}}/$USER/hs_err_pid%p.log -XX:NewSize={{jtnode_opt_newsize}} -XX:MaxNewSize={{jtnode_opt_maxnewsize}} -Xloggc:{{hdfs_log_dir_prefix}}/$USER/gc.log-`date +'%Y%m%d%H%M'` -verbose:gc -XX:+PrintGCDetails -XX:+PrintGCTimeStamps -XX:+PrintGCDateStamps -Xmx{{jtnode_heapsize}} -Dhadoop.security.logger=INFO,DRFAS -Dmapred.audit.logger=INFO,MRAUDIT -Dhadoop.mapreduce.jobsummary.logger=INFO,JSA ${HADOOP_JOBTRACKER_OPTS}\"\n\nHADOOP_TASKTRACKER_OPTS=\"-server -Xmx{{ttnode_heapsize}} -Dhadoop.security.logger=ERROR,console -Dmapred.audit.logger=ERROR,console ${HADOOP_TASKTRACKER_OPTS}\"\nHADOOP_DATANODE_OPTS=\"-XX:CMSInitiatingOccupancyFraction=70 -XX:+UseCMSInitiatingOccupancyOnly -XX:ConcGCThreads=4 -XX:+UseConcMarkSweepGC -Xmx{{dtnode_heapsize}} -Dhadoop.security.logger=ERROR,DRFAS ${HADOOP_DATANODE_OPTS}\"\nHADOOP_BALANCER_OPTS=\"-server -Xmx{{hadoop_heapsize}}m ${HADOOP_BALANCER_OPTS}\"\n\n# On secure datanodes, user to run the datanode as after dropping privileges\nexport HADOOP_SECURE_DN_USER=${HADOOP_SECURE_DN_USER:-{{hadoop_secure_dn_user}}}\n\n# Extra ssh options.  Empty by default.\nexport HADOOP_SSH_OPTS=\"-o ConnectTimeout=5 -o SendEnv=HADOOP_CONF_DIR\"\n\n# Where log files are stored.  $HADOOP_HOME/logs by default.\nexport HADOOP_LOG_DIR={{hdfs_log_dir_prefix}}/$USER\n\n# History server logs\nexport HADOOP_MAPRED_LOG_DIR={{mapred_log_dir_prefix}}/$USER\n\n# Where log files are stored in the secure data environment.\nexport HADOOP_SECURE_DN_LOG_DIR={{hdfs_log_dir_prefix}}/$HADOOP_SECURE_DN_USER\n\n# File naming remote slave hosts.  $HADOOP_HOME/conf/slaves by default.\n# export HADOOP_SLAVES=${HADOOP_HOME}/conf/slaves\n\n# host:path where hadoop code should be rsync'd from.  Unset by default.\n# export HADOOP_MASTER=master:/home/$USER/src/hadoop\n\n# Seconds to sleep between slave commands.  Unset by default.  This\n# can be useful in large clusters, where, e.g., slave rsyncs can\n# otherwise arrive faster than the master can service them.\n# export HADOOP_SLAVE_SLEEP=0.1\n\n# The directory where pid files are stored. /tmp by default.\nexport HADOOP_PID_DIR={{hadoop_pid_dir_prefix}}/$USER\nexport HADOOP_SECURE_DN_PID_DIR={{hadoop_pid_dir_prefix}}/$HADOOP_SECURE_DN_USER\n\n# History server pid\nexport HADOOP_MAPRED_PID_DIR={{mapred_pid_dir_prefix}}/$USER\n\nYARN_RESOURCEMANAGER_OPTS=\"-Dyarn.server.resourcemanager.appsummary.logger=INFO,RMSUMMARY -Drm.audit.logger=INFO,RMAUDIT\"\n\n# A string representing this instance of hadoop. $USER by default.\nexport HADOOP_IDENT_STRING=$USER\n\n# The scheduling priority for daemon processes.  See 'man nice'.\n\n# export HADOOP_NICENESS=10\n\n# Add database libraries\nJAVA_JDBC_LIBS=\"\"\nif [ -d \"/usr/share/java\" ]; then\n  for jarFile in `ls /usr/share/java | grep -E \"(mysql|ojdbc|postgresql|sqljdbc)\" 2>/dev/null`\n  do\n    JAVA_JDBC_LIBS=${JAVA_JDBC_LIBS}:$jarFile\n  done\nfi\n\n# Add libraries to the hadoop classpath - some may not need a colon as they already include it\nexport HADOOP_CLASSPATH=${HADOOP_CLASSPATH}${JAVA_JDBC_LIBS}\n\nif [ -d \"{{tez_home}}\" ]; then\n  export HADOOP_CLASSPATH=$HADOOP_CLASSPATH:{{tez_home}}/*:{{tez_home}}/lib/*:{{tez_conf_dir}}\nfi\n\n# Setting path to hdfs command line\nexport HADOOP_LIBEXEC_DIR={{hadoop_libexec_dir}}\n\n#Mostly required for hadoop 2.0\n#export JAVA_LIBRARY_PATH=${JAVA_LIBRARY_PATH}:{{hadoop_lib_home}}/native/Linux-{{architecture}}-64\nexport JAVA_LIBRARY_PATH=${JAVA_LIBRARY_PATH}:{{hadoop_lib_home}}/native\n\n{% if is_datanode_max_locked_memory_set %}\n# Fix temporary bug, when ulimit from conf files is not picked up, without full relogin. \n# Makes sense to fix only when runing DN as root \nif [ \"$command\" == \"datanode\" ] && [ \"$EUID\" -eq 0 ] && [ -n \"$HADOOP_SECURE_DN_USER\" ]; then\n  ulimit -l {{datanode_max_locked_memory}}\nfi\n{% endif %}",
                    "dtnode_heapsize":"1024",
                    "hadoop_heapsize":"1024",
                    "hadoop_pid_dir_prefix":"/var/run/hadoop",
                    "hadoop_root_logger":"INFO,RFA",
                    "hdfs_log_dir_prefix":"/var/log/hadoop",
                    "hdfs_principal_name":null,
                    "hdfs_tmp_dir":"/tmp",
                    "hdfs_user_keytab":null,
                    "hdfs_user_nofile_limit":"128000",
                    "hdfs_user_nproc_limit":"65536",
                    "namenode_backup_dir":"/tmp/upgrades",
                    "namenode_heapsize":"23552",
                    "namenode_opt_maxnewsize":"5888",
                    "namenode_opt_maxpermsize":"256",
                    "namenode_opt_newsize":"5888",
                    "namenode_opt_permsize":"128",
                    "hdfs_user":"hdfs"
                }
            }
        },
        {
            "hadoop-policy":{
                "properties_attributes":{

                },
                "properties":{
                    "security.admin.operations.protocol.acl":"hadoop",
                    "security.client.datanode.protocol.acl":"*",
                    "security.client.protocol.acl":"*",
                    "security.datanode.protocol.acl":"*",
                    "security.inter.datanode.protocol.acl":"*",
                    "security.inter.tracker.protocol.acl":"*",
                    "security.job.client.protocol.acl":"*",
                    "security.job.task.protocol.acl":"*",
                    "security.namenode.protocol.acl":"*",
                    "security.refresh.policy.protocol.acl":"hadoop",
                    "security.refresh.usertogroups.mappings.protocol.acl":"hadoop"
                }
            }
        },
        {
            "hdfs-log4j":{
                "properties_attributes":{

                },
                "properties":{
                    "content":"\n#\n# Licensed to the Apache Software Foundation (ASF) under one\n# or more contributor license agreements.  See the NOTICE file\n# distributed with this work for additional information\n# regarding copyright ownership.  The ASF licenses this file\n# to you under the Apache License, Version 2.0 (the\n# \"License\"); you may not use this file except in compliance\n# with the License.  You may obtain a copy of the License at\n#\n#  http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing,\n# software distributed under the License is distributed on an\n# \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n# KIND, either express or implied.  See the License for the\n# specific language governing permissions and limitations\n# under the License.\n#\n\n\n# Define some default values that can be overridden by system properties\n# To change daemon root logger use hadoop_root_logger in hadoop-env\nhadoop.root.logger=INFO,console\nhadoop.log.dir=.\nhadoop.log.file=hadoop.log\n\n\n# Define the root logger to the system property \"hadoop.root.logger\".\nlog4j.rootLogger=${hadoop.root.logger}, EventCounter\n\n# Logging Threshold\nlog4j.threshhold=ALL\n\n#\n# Daily Rolling File Appender\n#\n\nlog4j.appender.DRFA=org.apache.log4j.DailyRollingFileAppender\nlog4j.appender.DRFA.File=${hadoop.log.dir}/${hadoop.log.file}\n\n# Rollver at midnight\nlog4j.appender.DRFA.DatePattern=.yyyy-MM-dd\n\n# 30-day backup\n#log4j.appender.DRFA.MaxBackupIndex=30\nlog4j.appender.DRFA.layout=org.apache.log4j.PatternLayout\n\n# Pattern format: Date LogLevel LoggerName LogMessage\nlog4j.appender.DRFA.layout.ConversionPattern=%d{ISO8601} %p %c: %m%n\n# Debugging Pattern format\n#log4j.appender.DRFA.layout.ConversionPattern=%d{ISO8601} %-5p %c{2} (%F:%M(%L)) - %m%n\n\n\n#\n# console\n# Add \"console\" to rootlogger above if you want to use this\n#\n\nlog4j.appender.console=org.apache.log4j.ConsoleAppender\nlog4j.appender.console.target=System.err\nlog4j.appender.console.layout=org.apache.log4j.PatternLayout\nlog4j.appender.console.layout.ConversionPattern=%d{yy/MM/dd HH:mm:ss} %p %c{2}: %m%n\n\n#\n# TaskLog Appender\n#\n\n#Default values\nhadoop.tasklog.taskid=null\nhadoop.tasklog.iscleanup=false\nhadoop.tasklog.noKeepSplits=4\nhadoop.tasklog.totalLogFileSize=100\nhadoop.tasklog.purgeLogSplits=true\nhadoop.tasklog.logsRetainHours=12\n\nlog4j.appender.TLA=org.apache.hadoop.mapred.TaskLogAppender\nlog4j.appender.TLA.taskId=${hadoop.tasklog.taskid}\nlog4j.appender.TLA.isCleanup=${hadoop.tasklog.iscleanup}\nlog4j.appender.TLA.totalLogFileSize=${hadoop.tasklog.totalLogFileSize}\n\nlog4j.appender.TLA.layout=org.apache.log4j.PatternLayout\nlog4j.appender.TLA.layout.ConversionPattern=%d{ISO8601} %p %c: %m%n\n\n#\n#Security audit appender\n#\nhadoop.security.logger=INFO,console\nhadoop.security.log.maxfilesize={{hadoop_security_log_max_backup_size}}MB\nhadoop.security.log.maxbackupindex={{hadoop_security_log_number_of_backup_files}}\nlog4j.category.SecurityLogger=${hadoop.security.logger}\nhadoop.security.log.file=SecurityAuth.audit\nlog4j.appender.DRFAS=org.apache.log4j.DailyRollingFileAppender\nlog4j.appender.DRFAS.File=${hadoop.log.dir}/${hadoop.security.log.file}\nlog4j.appender.DRFAS.layout=org.apache.log4j.PatternLayout\nlog4j.appender.DRFAS.layout.ConversionPattern=%d{ISO8601} %p %c: %m%n\nlog4j.appender.DRFAS.DatePattern=.yyyy-MM-dd\n\nlog4j.appender.RFAS=org.apache.log4j.RollingFileAppender\nlog4j.appender.RFAS.File=${hadoop.log.dir}/${hadoop.security.log.file}\nlog4j.appender.RFAS.layout=org.apache.log4j.PatternLayout\nlog4j.appender.RFAS.layout.ConversionPattern=%d{ISO8601} %p %c: %m%n\nlog4j.appender.RFAS.MaxFileSize=${hadoop.security.log.maxfilesize}\nlog4j.appender.RFAS.MaxBackupIndex=${hadoop.security.log.maxbackupindex}\n\n#\n# hdfs audit logging\n#\nhdfs.audit.logger=INFO,console\nlog4j.logger.org.apache.hadoop.hdfs.server.namenode.FSNamesystem.audit=${hdfs.audit.logger}\nlog4j.additivity.org.apache.hadoop.hdfs.server.namenode.FSNamesystem.audit=false\nlog4j.appender.DRFAAUDIT=org.apache.log4j.DailyRollingFileAppender\nlog4j.appender.DRFAAUDIT.File=${hadoop.log.dir}/hdfs-audit.log\nlog4j.appender.DRFAAUDIT.layout=org.apache.log4j.PatternLayout\nlog4j.appender.DRFAAUDIT.layout.ConversionPattern=%d{ISO8601} %p %c{2}: %m%n\nlog4j.appender.DRFAAUDIT.DatePattern=.yyyy-MM-dd\n\n#\n# mapred audit logging\n#\nmapred.audit.logger=INFO,console\nlog4j.logger.org.apache.hadoop.mapred.AuditLogger=${mapred.audit.logger}\nlog4j.additivity.org.apache.hadoop.mapred.AuditLogger=false\nlog4j.appender.MRAUDIT=org.apache.log4j.DailyRollingFileAppender\nlog4j.appender.MRAUDIT.File=${hadoop.log.dir}/mapred-audit.log\nlog4j.appender.MRAUDIT.layout=org.apache.log4j.PatternLayout\nlog4j.appender.MRAUDIT.layout.ConversionPattern=%d{ISO8601} %p %c{2}: %m%n\nlog4j.appender.MRAUDIT.DatePattern=.yyyy-MM-dd\n\n#\n# Rolling File Appender\n#\n\nlog4j.appender.RFA=org.apache.log4j.RollingFileAppender\nlog4j.appender.RFA.File=${hadoop.log.dir}/${hadoop.log.file}\n\n# Logfile size and and 30-day backups\nlog4j.appender.RFA.MaxFileSize={{hadoop_log_max_backup_size}}MB\nlog4j.appender.RFA.MaxBackupIndex={{hadoop_log_number_of_backup_files}}\n\nlog4j.appender.RFA.layout=org.apache.log4j.PatternLayout\nlog4j.appender.RFA.layout.ConversionPattern=%d{ISO8601} %-5p %c{2} - %m%n\nlog4j.appender.RFA.layout.ConversionPattern=%d{ISO8601} %-5p %c{2} (%F:%M(%L)) - %m%n\n\n\n# Custom Logging levels\n\nhadoop.metrics.log.level=INFO\n#log4j.logger.org.apache.hadoop.mapred.JobTracker=DEBUG\n#log4j.logger.org.apache.hadoop.mapred.TaskTracker=DEBUG\n#log4j.logger.org.apache.hadoop.fs.FSNamesystem=DEBUG\nlog4j.logger.org.apache.hadoop.metrics2=${hadoop.metrics.log.level}\n\n# Jets3t library\nlog4j.logger.org.jets3t.service.impl.rest.httpclient.RestS3Service=ERROR\n\n#\n# Null Appender\n# Trap security logger on the hadoop client side\n#\nlog4j.appender.NullAppender=org.apache.log4j.varia.NullAppender\n\n#\n# Event Counter Appender\n# Sends counts of logging messages at different severity levels to Hadoop Metrics.\n#\nlog4j.appender.EventCounter=org.apache.hadoop.log.metrics.EventCounter\n\n# Removes \"deprecated\" messages\nlog4j.logger.org.apache.hadoop.conf.Configuration.deprecation=WARN\n\n# Adding logging for 3rd party library\nlog4j.logger.org.apache.commons.beanutils=WARN",
                    "hadoop_log_max_backup_size":"256",
                    "hadoop_log_number_of_backup_files":"10",
                    "hadoop_security_log_max_backup_size":"256",
                    "hadoop_security_log_number_of_backup_files":"20"
                }
            }
        },
        {
            "hdfs-site":{
                "properties_attributes":{

                },
                "properties":{
                    "dfs.block.access.token.enable":"true",
                    "dfs.blockreport.initialDelay":"120",
                    "dfs.blocksize":"134217728",
                    "dfs.client.read.shortcircuit":"true",
                    "dfs.client.read.shortcircuit.streams.cache.size":"4096",
                    "dfs.cluster.administrators":" hdfs",
                    "dfs.datanode.address":"0.0.0.0:50010",
                    "dfs.datanode.balance.bandwidthPerSec":"6250000",
                    "dfs.datanode.data.dir":"/hadoop/hdfs/data",
                    "dfs.datanode.data.dir.perm":"750",
                    "dfs.datanode.du.reserved":"135137647104",
                    "dfs.datanode.failed.volumes.tolerated":"0",
                    "dfs.datanode.http.address":"0.0.0.0:50075",
                    "dfs.datanode.https.address":"0.0.0.0:50475",
                    "dfs.datanode.ipc.address":"0.0.0.0:8010",
                    "dfs.datanode.max.transfer.threads":"1024",
                    "dfs.domain.socket.path":"/var/lib/hadoop-hdfs/dn_socket",
                    "dfs.heartbeat.interval":"3",
                    "dfs.hosts.exclude":"/etc/hadoop/conf/dfs.exclude",
                    "dfs.http.policy":"HTTP_ONLY",
                    "dfs.https.port":"50470",
                    "dfs.journalnode.edits.dir":"/grid/0/hdfs/journal",
                    "dfs.journalnode.http-address":"0.0.0.0:8480",
                    "dfs.journalnode.https-address":"0.0.0.0:8481",
                    "dfs.namenode.accesstime.precision":"0",
                    "dfs.namenode.avoid.read.stale.datanode":"true",
                    "dfs.namenode.avoid.write.stale.datanode":"true",
                    "dfs.namenode.checkpoint.dir":"/hadoop/hdfs/namesecondary",
                    "dfs.namenode.checkpoint.edits.dir":"${dfs.namenode.checkpoint.dir}",
                    "dfs.namenode.checkpoint.period":"21600",
                    "dfs.namenode.checkpoint.txns":"1000000",
                    "dfs.namenode.handler.count":"100",
                    "dfs.namenode.http-address":"ambari-server:50070",
                    "dfs.namenode.https-address":"ambari-server:50470",
                    "dfs.namenode.name.dir":"/hadoop/hdfs/namenode",
                    "dfs.namenode.name.dir.restore":"true",
                    "dfs.namenode.rpc-address":"ambari-server:8020",
                    "dfs.namenode.safemode.threshold-pct":"0.999",
                    "dfs.namenode.secondary.http-address":"ambari-server:50090",
                    "dfs.namenode.stale.datanode.interval":"30000",
                    "dfs.namenode.write.stale.datanode.ratio":"1.0f",
                    "dfs.permissions.enabled":"true",
                    "dfs.replication":"3",
                    "dfs.replication.max":"50",
                    "dfs.support.append":"true",
                    "dfs.webhdfs.enabled":"true",
                    "fs.permissions.umask-mode":"022",
                    "manage.include.files":"false",
                    "dfs.permissions.superusergroup":"hdfs"
                }
            }
        },
        {
            "ssl-client":{
                "properties_attributes":{

                },
                "properties":{
                    "ssl.client.keystore.location":"",
                    "ssl.client.keystore.password":"",
                    "ssl.client.keystore.type":"jks",
                    "ssl.client.truststore.location":"",
                    "ssl.client.truststore.password":"",
                    "ssl.client.truststore.reload.interval":"10000",
                    "ssl.client.truststore.type":"jks"
                }
            }
        },
        {
            "ssl-server":{
                "properties_attributes":{

                },
                "properties":{
                    "ssl.server.keystore.keypassword":"bigdata",
                    "ssl.server.keystore.location":"/etc/security/serverKeys/keystore.jks",
                    "ssl.server.keystore.password":"bigdata",
                    "ssl.server.keystore.type":"jks",
                    "ssl.server.truststore.location":"/etc/security/serverKeys/all.jks",
                    "ssl.server.truststore.password":"bigdata",
                    "ssl.server.truststore.reload.interval":"10000",
                    "ssl.server.truststore.type":"jks"
                }
            }
        },
        {
            "capacity-scheduler":{
                "properties_attributes":{

                },
                "properties":{
                    "yarn.scheduler.capacity.default.minimum-user-limit-percent":"100",
                    "yarn.scheduler.capacity.maximum-am-resource-percent":"0.2",
                    "yarn.scheduler.capacity.maximum-applications":"10000",
                    "yarn.scheduler.capacity.node-locality-delay":"40",
                    "yarn.scheduler.capacity.root.acl_administer_queue":"*",
                    "yarn.scheduler.capacity.root.capacity":"100",
                    "yarn.scheduler.capacity.root.default.acl_administer_jobs":"*",
                    "yarn.scheduler.capacity.root.default.acl_submit_applications":"*",
                    "yarn.scheduler.capacity.root.default.capacity":"100",
                    "yarn.scheduler.capacity.root.default.maximum-capacity":"100",
                    "yarn.scheduler.capacity.root.default.state":"RUNNING",
                    "yarn.scheduler.capacity.root.default.user-limit-factor":"1",
                    "yarn.scheduler.capacity.root.queues":"default"
                }
            }
        },
        {
            "container-executor":{
                "properties_attributes":{

                },
                "properties":{
                    "content":"{#\n# Licensed to the Apache Software Foundation (ASF) under one\n# or more contributor license agreements.  See the NOTICE file\n# distributed with this work for additional information\n# regarding copyright ownership.  The ASF licenses this file\n# to you under the Apache License, Version 2.0 (the\n# \"License\"); you may not use this file except in compliance\n# with the License.  You may obtain a copy of the License at\n#\n#   http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n#}\n\n#/*\n# * Licensed to the Apache Software Foundation (ASF) under one\n# * or more contributor license agreements.  See the NOTICE file\n# * distributed with this work for additional information\n# * regarding copyright ownership.  The ASF licenses this file\n# * to you under the Apache License, Version 2.0 (the\n# * \"License\"); you may not use this file except in compliance\n# * with the License.  You may obtain a copy of the License at\n# *\n# *     http://www.apache.org/licenses/LICENSE-2.0\n# *\n# * Unless required by applicable law or agreed to in writing, software\n# * distributed under the License is distributed on an \"AS IS\" BASIS,\n# * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# * See the License for the specific language governing permissions and\n# * limitations under the License.\n# */\nyarn.nodemanager.local-dirs={{nm_local_dirs}}\nyarn.nodemanager.log-dirs={{nm_log_dirs}}\nyarn.nodemanager.linux-container-executor.group={{yarn_executor_container_group}}\nbanned.users=hdfs,yarn,mapred,bin\nmin.user.id={{min_user_id}}"
                }
            }
        },
        {
            "yarn-env":{
                "properties_attributes":{

                },
                "properties":{
                    "content":"\nexport HADOOP_YARN_HOME={{hadoop_yarn_home}}\nUSER=\"$(whoami)\"\nexport HADOOP_LOG_DIR={{yarn_log_dir_prefix}}/$USER\nexport HADOOP_PID_DIR={{yarn_pid_dir_prefix}}/$USER\nexport HADOOP_LIBEXEC_DIR={{hadoop_libexec_dir}}\nexport JAVA_HOME={{java64_home}}\nexport JAVA_LIBRARY_PATH=\"${JAVA_LIBRARY_PATH}:{{hadoop_java_io_tmpdir}}\"\n\n# User for YARN daemons\nexport HADOOP_YARN_USER=${HADOOP_YARN_USER:-yarn}\n\n# resolve links - $0 may be a softlink\nexport HADOOP_CONF_DIR=\"${HADOOP_CONF_DIR:-$HADOOP_YARN_HOME/etc/hadoop}\"\n\n# some Java parameters\n# export JAVA_HOME=/home/y/libexec/jdk1.6.0/\nif [ \"$JAVA_HOME\" != \"\" ]; then\n  #echo \"run java in $JAVA_HOME\"\n  JAVA_HOME=$JAVA_HOME\nfi\n\nif [ \"$JAVA_HOME\" = \"\" ]; then\n  echo \"Error: JAVA_HOME is not set.\"\n  exit 1\nfi\n\nJAVA=$JAVA_HOME/bin/java\nJAVA_HEAP_MAX=-Xmx1000m\n\n# For setting YARN specific HEAP sizes please use this\n# Parameter and set appropriately\nYARN_HEAPSIZE={{yarn_heapsize}}\n\n# check envvars which might override default args\nif [ \"$YARN_HEAPSIZE\" != \"\" ]; then\n  JAVA_HEAP_MAX=\"-Xmx\"\"$YARN_HEAPSIZE\"\"m\"\nfi\n\n# Resource Manager specific parameters\n\n# Specify the max Heapsize for the ResourceManager using a numerical value\n# in the scale of MB. For example, to specify an jvm option of -Xmx1000m, set\n# the value to 1000.\n# This value will be overridden by an Xmx setting specified in either HADOOP_OPTS\n# and/or YARN_RESOURCEMANAGER_OPTS.\n# If not specified, the default value will be picked from either YARN_HEAPMAX\n# or JAVA_HEAP_MAX with YARN_HEAPMAX as the preferred option of the two.\nexport YARN_RESOURCEMANAGER_HEAPSIZE={{resourcemanager_heapsize}}\n\n# Specify the JVM options to be used when starting the ResourceManager.\n# These options will be appended to the options specified as HADOOP_OPTS\n# and therefore may override any similar flags set in HADOOP_OPTS\n#export YARN_RESOURCEMANAGER_OPTS=\n\n# Node Manager specific parameters\n\n# Specify the max Heapsize for the NodeManager using a numerical value\n# in the scale of MB. For example, to specify an jvm option of -Xmx1000m, set\n# the value to 1000.\n# This value will be overridden by an Xmx setting specified in either HADOOP_OPTS\n# and/or YARN_NODEMANAGER_OPTS.\n# If not specified, the default value will be picked from either YARN_HEAPMAX\n# or JAVA_HEAP_MAX with YARN_HEAPMAX as the preferred option of the two.\nexport YARN_NODEMANAGER_HEAPSIZE={{nodemanager_heapsize}}\n\n# Specify the max Heapsize for the HistoryManager using a numerical value\n# in the scale of MB. For example, to specify an jvm option of -Xmx1000m, set\n# the value to 1024.\n# This value will be overridden by an Xmx setting specified in either HADOOP_OPTS\n# and/or YARN_HISTORYSERVER_OPTS.\n# If not specified, the default value will be picked from either YARN_HEAPMAX\n# or JAVA_HEAP_MAX with YARN_HEAPMAX as the preferred option of the two.\nexport YARN_HISTORYSERVER_HEAPSIZE={{apptimelineserver_heapsize}}\n\n# Specify the JVM options to be used when starting the NodeManager.\n# These options will be appended to the options specified as HADOOP_OPTS\n# and therefore may override any similar flags set in HADOOP_OPTS\n#export YARN_NODEMANAGER_OPTS=\n\n# so that filenames w/ spaces are handled correctly in loops below\nIFS=\n\n\n# default log directory and file\nif [ \"$HADOOP_LOG_DIR\" = \"\" ]; then\n  HADOOP_LOG_DIR=\"$HADOOP_YARN_HOME/logs\"\nfi\nif [ \"$HADOOP_LOGFILE\" = \"\" ]; then\n  HADOOP_LOGFILE='yarn.log'\nfi\n\n# default policy file for service-level authorization\nif [ \"$YARN_POLICYFILE\" = \"\" ]; then\n  YARN_POLICYFILE=\"hadoop-policy.xml\"\nfi\n\n# restore ordinary behaviour\nunset IFS\n\n\nHADOOP_OPTS=\"$HADOOP_OPTS -Dhadoop.log.dir=$HADOOP_LOG_DIR\"\nHADOOP_OPTS=\"$HADOOP_OPTS -Dyarn.log.dir=$HADOOP_LOG_DIR\"\nHADOOP_OPTS=\"$HADOOP_OPTS -Dhadoop.log.file=$HADOOP_LOGFILE\"\nHADOOP_OPTS=\"$HADOOP_OPTS -Dyarn.log.file=$HADOOP_LOGFILE\"\nHADOOP_OPTS=\"$HADOOP_OPTS -Dyarn.home.dir=$HADOOP_YARN_HOME\"\nHADOOP_OPTS=\"$HADOOP_OPTS -Dyarn.id.str=$HADOOP_IDENT_STRING\"\nHADOOP_OPTS=\"$HADOOP_OPTS -Dhadoop.root.logger=${HADOOP_ROOT_LOGGER:-INFO,console}\"\nHADOOP_OPTS=\"$HADOOP_OPTS -Dyarn.root.logger=${HADOOP_ROOT_LOGGER:-INFO,console}\"\nexport YARN_NODEMANAGER_OPTS=\"$YARN_NODEMANAGER_OPTS -Dnm.audit.logger=INFO,NMAUDIT\"\nexport YARN_RESOURCEMANAGER_OPTS=\"$YARN_RESOURCEMANAGER_OPTS -Drm.audit.logger=INFO,RMAUDIT\"\nif [ \"x$JAVA_LIBRARY_PATH\" != \"x\" ]; then\n  HADOOP_OPTS=\"$HADOOP_OPTS -Djava.library.path=$JAVA_LIBRARY_PATH\"\nfi\nHADOOP_OPTS=\"$HADOOP_OPTS -Dyarn.policy.file=$YARN_POLICYFILE\"\nHADOOP_OPTS=\"$HADOOP_OPTS -Djava.io.tmpdir={{hadoop_java_io_tmpdir}}\"\n\n{% if rm_security_opts is defined %}\nHADOOP_OPTS=\"{{rm_security_opts}} $HADOOP_OPTS\"\n{% endif %}",
                    "is_supported_yarn_ranger":"false",
                    "min_user_id":"1000",
                    "nodemanager_heapsize":"1024",
                    "resourcemanager_heapsize":"1024",
                    "service_check.queue.name":"default",
                    "yarn_heapsize":"1024",
                    "yarn_log_dir_prefix":"/var/log/hadoop-yarn",
                    "yarn_pid_dir_prefix":"/var/run/hadoop-yarn",
                    "yarn_user_nofile_limit":"32768",
                    "yarn_user_nproc_limit":"65536",
                    "yarn_user":"yarn"
                }
            }
        },
        {
            "yarn-log4j":{
                "properties_attributes":{

                },
                "properties":{
                    "content":"\n#Relative to Yarn Log Dir Prefix\nyarn.log.dir=.\n#\n# Job Summary Appender\n#\n# Use following logger to send summary to separate file defined by\n# hadoop.mapreduce.jobsummary.log.file rolled daily:\n# hadoop.mapreduce.jobsummary.logger=INFO,JSA\n#\nhadoop.mapreduce.jobsummary.logger=${hadoop.root.logger}\nhadoop.mapreduce.jobsummary.log.file=hadoop-mapreduce.jobsummary.log\nlog4j.appender.JSA=org.apache.log4j.DailyRollingFileAppender\n# Set the ResourceManager summary log filename\nyarn.server.resourcemanager.appsummary.log.file=hadoop-mapreduce.jobsummary.log\n# Set the ResourceManager summary log level and appender\nyarn.server.resourcemanager.appsummary.logger=${hadoop.root.logger}\n#yarn.server.resourcemanager.appsummary.logger=INFO,RMSUMMARY\n\n# To enable AppSummaryLogging for the RM,\n# set yarn.server.resourcemanager.appsummary.logger to\n# LEVEL,RMSUMMARY in hadoop-env.sh\n\n# Appender for ResourceManager Application Summary Log\n# Requires the following properties to be set\n#    - hadoop.log.dir (Hadoop Log directory)\n#    - yarn.server.resourcemanager.appsummary.log.file (resource manager app summary log filename)\n#    - yarn.server.resourcemanager.appsummary.logger (resource manager app summary log level and appender)\nlog4j.appender.RMSUMMARY=org.apache.log4j.RollingFileAppender\nlog4j.appender.RMSUMMARY.File=${yarn.log.dir}/${yarn.server.resourcemanager.appsummary.log.file}\nlog4j.appender.RMSUMMARY.MaxFileSize={{yarn_rm_summary_log_max_backup_size}}MB\nlog4j.appender.RMSUMMARY.MaxBackupIndex={{yarn_rm_summary_log_number_of_backup_files}}\nlog4j.appender.RMSUMMARY.layout=org.apache.log4j.PatternLayout\nlog4j.appender.RMSUMMARY.layout.ConversionPattern=%d{ISO8601} %p %c{2}: %m%n\nlog4j.appender.JSA.layout=org.apache.log4j.PatternLayout\nlog4j.appender.JSA.layout.ConversionPattern=%d{yy/MM/dd HH:mm:ss} %p %c{2}: %m%n\nlog4j.appender.JSA.DatePattern=.yyyy-MM-dd\nlog4j.appender.JSA.layout=org.apache.log4j.PatternLayout\nlog4j.logger.org.apache.hadoop.yarn.server.resourcemanager.RMAppManager$ApplicationSummary=${yarn.server.resourcemanager.appsummary.logger}\nlog4j.additivity.org.apache.hadoop.yarn.server.resourcemanager.RMAppManager$ApplicationSummary=false\n\n# Audit logging for ResourceManager\nrm.audit.logger=${hadoop.root.logger}\nlog4j.logger.org.apache.hadoop.yarn.server.resourcemanager.RMAuditLogger=${rm.audit.logger}\nlog4j.additivity.org.apache.hadoop.yarn.server.resourcemanager.RMAuditLogger=false\nlog4j.appender.RMAUDIT=org.apache.log4j.DailyRollingFileAppender\nlog4j.appender.RMAUDIT.File=${yarn.log.dir}/rm-audit.log\nlog4j.appender.RMAUDIT.layout=org.apache.log4j.PatternLayout\nlog4j.appender.RMAUDIT.layout.ConversionPattern=%d{ISO8601} %p %c{2}: %m%n\nlog4j.appender.RMAUDIT.DatePattern=.yyyy-MM-dd\n\n# Audit logging for NodeManager\nnm.audit.logger=${hadoop.root.logger}\nlog4j.logger.org.apache.hadoop.yarn.server.nodemanager.NMAuditLogger=${nm.audit.logger}\nlog4j.additivity.org.apache.hadoop.yarn.server.nodemanager.NMAuditLogger=false\nlog4j.appender.NMAUDIT=org.apache.log4j.DailyRollingFileAppender\nlog4j.appender.NMAUDIT.File=${yarn.log.dir}/nm-audit.log\nlog4j.appender.NMAUDIT.layout=org.apache.log4j.PatternLayout\nlog4j.appender.NMAUDIT.layout.ConversionPattern=%d{ISO8601} %p %c{2}: %m%n\nlog4j.appender.NMAUDIT.DatePattern=.yyyy-MM-dd",
                    "yarn_rm_summary_log_max_backup_size":"256",
                    "yarn_rm_summary_log_number_of_backup_files":"20"
                }
            }
        },
        {
            "yarn-site":{
                "properties_attributes":{

                },
                "properties":{
                    "yarn.timeline-service.http-authentication.proxyuser.root.groups":"*",
                    "yarn.timeline-service.http-authentication.proxyuser.root.hosts":"ambari-server",
                    "manage.include.files":"false",
                    "yarn.acl.enable":"false",
                    "yarn.admin.acl":"",
                    "yarn.application.classpath":"{{hadoop_conf_dir}},{{hadoop_home}}/*,{{hadoop_home}}/lib/*,{{hadoop_hdfs_home}}/*,{{hadoop_hdfs_home}}/lib/*,{{hadoop_yarn_home}}/*,{{hadoop_yarn_home}}/lib/*",
                    "yarn.authorization-provider":null,
                    "yarn.http.policy":"HTTP_ONLY",
                    "yarn.log-aggregation-enable":"true",
                    "yarn.log-aggregation.retain-seconds":"2592000",
                    "yarn.log.server.url":"http://ambari-server:19888/jobhistory/logs",
                    "yarn.nodemanager.address":"0.0.0.0:45454",
                    "yarn.nodemanager.admin-env":"MALLOC_ARENA_MAX=$MALLOC_ARENA_MAX",
                    "yarn.nodemanager.aux-services":"mapreduce_shuffle",
                    "yarn.nodemanager.aux-services.mapreduce_shuffle.class":"org.apache.hadoop.mapred.ShuffleHandler",
                    "yarn.nodemanager.container-executor.class":"org.apache.hadoop.yarn.server.nodemanager.DefaultContainerExecutor",
                    "yarn.nodemanager.container-monitor.interval-ms":"3000",
                    "yarn.nodemanager.delete.debug-delay-sec":"0",
                    "yarn.nodemanager.disk-health-checker.min-healthy-disks":"0.25",
                    "yarn.nodemanager.health-checker.interval-ms":"135000",
                    "yarn.nodemanager.health-checker.script.timeout-ms":"60000",
                    "yarn.nodemanager.linux-container-executor.group":"hadoop",
                    "yarn.nodemanager.local-dirs":"/hadoop/yarn/local",
                    "yarn.nodemanager.log-aggregation.compression-type":"gz",
                    "yarn.nodemanager.log-dirs":"/hadoop/yarn/log",
                    "yarn.nodemanager.log.retain-seconds":"604800",
                    "yarn.nodemanager.remote-app-log-dir":"/app-logs",
                    "yarn.nodemanager.remote-app-log-dir-suffix":"logs",
                    "yarn.nodemanager.resource.memory-mb":"46080",
                    "yarn.nodemanager.vmem-check-enabled":"false",
                    "yarn.nodemanager.vmem-pmem-ratio":"2.1",
                    "yarn.resourcemanager.address":"ambari-server:8050",
                    "yarn.resourcemanager.admin.address":"ambari-server:8141",
                    "yarn.resourcemanager.am.max-attempts":"2",
                    "yarn.resourcemanager.hostname":"ambari-server",
                    "yarn.resourcemanager.nodes.exclude-path":"/etc/hadoop/conf/yarn.exclude",
                    "yarn.resourcemanager.resource-tracker.address":"ambari-server:8025",
                    "yarn.resourcemanager.scheduler.address":"ambari-server:8030",
                    "yarn.resourcemanager.scheduler.class":"org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CapacityScheduler",
                    "yarn.resourcemanager.webapp.address":"ambari-server:8088",
                    "yarn.resourcemanager.webapp.https.address":"ambari-server:8090",
                    "yarn.scheduler.maximum-allocation-mb":"46080",
                    "yarn.scheduler.minimum-allocation-mb":"15360",
                    "yarn.timeline-service.address":"localhost:10200",
                    "yarn.timeline-service.enabled":"true",
                    "yarn.timeline-service.entity-group-fs-store.active-dir":"/ats/active/",
                    "yarn.timeline-service.entity-group-fs-store.cleaner-interval-seconds":"3600",
                    "yarn.timeline-service.entity-group-fs-store.done-dir":"/ats/done/",
                    "yarn.timeline-service.entity-group-fs-store.group-id-plugin-classes":"",
                    "yarn.timeline-service.entity-group-fs-store.retain-seconds":"604800",
                    "yarn.timeline-service.entity-group-fs-store.scan-interval-seconds":"60",
                    "yarn.timeline-service.entity-group-fs-store.summary-store":"org.apache.hadoop.yarn.server.timeline.RollingLevelDBTimelineStore",
                    "yarn.timeline-service.generic-application-history.store-class":"org.apache.hadoop.yarn.server.applicationhistoryservice.NullApplicationHistoryStore",
                    "yarn.timeline-service.leveldb-timeline-store.path":"/var/log/hadoop-yarn/timeline",
                    "yarn.timeline-service.leveldb-timeline-store.ttl-interval-ms":"300000",
                    "yarn.timeline-service.recovery.enabled":"true",
                    "yarn.timeline-service.store-class":"org.apache.hadoop.yarn.server.timeline.LeveldbTimelineStore",
                    "yarn.timeline-service.ttl-enable":"true",
                    "yarn.timeline-service.ttl-ms":"2678400000",
                    "yarn.timeline-service.webapp.address":"localhost:8188",
                    "yarn.timeline-service.webapp.https.address":"localhost:8190"
                }
            }
        },
        {
            "mapred-env":{
                "properties_attributes":{

                },
                "properties":{
                    "content":"\n\nexport HADOOP_JOB_HISTORYSERVER_HEAPSIZE={{jobhistory_heapsize}}\n\nexport HADOOP_ROOT_LOGGER=INFO,RFA\n\n# Could be enabled from deployment option if necessary\nexport HADOOP_LOG_DIR={{mapred_log_dir_prefix}}/$USER  # Where log files are stored.  $HADOOP_MAPRED_HOME/logs by default.\nexport MAPRED_HISTORYSERVER_OPTS=\"$MAPRED_HISTORYSERVER_OPTS -Dhadoop.log.dir=$HADOOP_LOG_DIR\"\n#export HADOOP_JHS_LOGGER=INFO,RFA # Hadoop JobSummary logger.\n#export HADOOP_PID_DIR= # The pid files are stored. /tmp by default.\n#export HADOOP_IDENT_STRING= #A string representing this instance of hadoop. $USER by default\n#export HADOOP_NICENESS= #The scheduling priority for daemons. Defaults to 0.",
                    "jobhistory_heapsize":"900",
                    "mapred_log_dir_prefix":"/var/log/hadoop-mapreduce",
                    "mapred_pid_dir_prefix":"/var/run/hadoop-mapreduce",
                    "mapred_user_nofile_limit":"32768",
                    "mapred_user_nproc_limit":"65536",
                    "mapred_user":"mapred"
                }
            }
        },
        {
            "mapred-site":{
                "properties_attributes":{

                },
                "properties":{
                    "mapreduce.admin.map.child.java.opts":"-server -XX:NewRatio=8 -Djava.net.preferIPv4Stack=true -Dhadoop.metrics.log.level=WARN",
                    "mapreduce.admin.reduce.child.java.opts":"-server -XX:NewRatio=8 -Djava.net.preferIPv4Stack=true -Dhadoop.metrics.log.level=WARN",
                    "mapreduce.admin.user.env":"LD_LIBRARY_PATH={{hadoop_home}}/lib/native",
                    "mapreduce.am.max-attempts":"2",
                    "mapreduce.application.classpath":"{{hadoop_conf_dir}},{{hadoop_home}}/*,{{hadoop_home}}/lib/*,{{hadoop_hdfs_home}}/*,{{hadoop_hdfs_home}}/lib/*,{{hadoop_yarn_home}}/*,{{hadoop_yarn_home}}/lib/*,{{hadoop_mapred_home}}/*,{{hadoop_mapred_home}}/lib/*",
                    "mapreduce.cluster.administrators":" hadoop",
                    "mapreduce.framework.name":"yarn",
                    "mapreduce.job.counters.max":"130",
                    "mapreduce.job.queuename":"default",
                    "mapreduce.job.reduce.slowstart.completedmaps":"0.05",
                    "mapreduce.jobhistory.address":"ambari-server:10020",
                    "mapreduce.jobhistory.done-dir":"/mr-history/done",
                    "mapreduce.jobhistory.http.policy":"HTTP_ONLY",
                    "mapreduce.jobhistory.intermediate-done-dir":"/mr-history/tmp",
                    "mapreduce.jobhistory.webapp.address":"ambari-server:19888",
                    "mapreduce.map.java.opts":"-Xmx12288m",
                    "mapreduce.map.log.level":"INFO",
                    "mapreduce.map.memory.mb":"15360",
                    "mapreduce.map.output.compress":"false",
                    "mapreduce.map.sort.spill.percent":"0.7",
                    "mapreduce.map.speculative":"false",
                    "mapreduce.output.fileoutputformat.compress":"false",
                    "mapreduce.output.fileoutputformat.compress.type":"BLOCK",
                    "mapreduce.reduce.input.buffer.percent":"0.0",
                    "mapreduce.reduce.java.opts":"-Xmx12288m",
                    "mapreduce.reduce.log.level":"INFO",
                    "mapreduce.reduce.memory.mb":"15360",
                    "mapreduce.reduce.shuffle.input.buffer.percent":"0.7",
                    "mapreduce.reduce.shuffle.merge.percent":"0.66",
                    "mapreduce.reduce.shuffle.parallelcopies":"30",
                    "mapreduce.reduce.speculative":"false",
                    "mapreduce.shuffle.port":"13562",
                    "mapreduce.task.io.sort.factor":"100",
                    "mapreduce.task.io.sort.mb":"1024",
                    "mapreduce.task.timeout":"300000",
                    "yarn.app.mapreduce.am.admin-command-opts":"-server -XX:NewRatio=8 -Djava.net.preferIPv4Stack=true -Dhadoop.metrics.log.level=WARN",
                    "yarn.app.mapreduce.am.command-opts":"-Xmx12288m",
                    "yarn.app.mapreduce.am.log.level":"INFO",
                    "yarn.app.mapreduce.am.resource.mb":"15360",
                    "yarn.app.mapreduce.am.staging-dir":"/user"
                }
            }
        },
        {
            "tez-env":{
                "properties_attributes":{

                },
                "properties":{
                    "content":"\n# Tez specific configuration\nexport TEZ_CONF_DIR={{tez_conf_dir}}\n\n# Set HADOOP_HOME to point to a specific hadoop install directory\nexport HADOOP_HOME=${HADOOP_HOME:-{{hadoop_home}}}\n\n# The java implementation to use.\nexport JAVA_HOME={{java64_home}}",
                    "enable_heap_dump":"false",
                    "heap_dump_location":"/tmp",
                    "tez_user":"tez"
                }
            }
        },
        {
            "tez-site":{
                "properties_attributes":{

                },
                "properties":{
                    "tez.history.logging.service.class":"org.apache.tez.dag.history.logging.proto.ProtoHistoryLoggingService",
                    "tez.history.logging.proto-base-dir":"/warehouse/tablespace/external/hive/sys.db",
                    "tez.queue.name":"default",
                    "tez.am.java.opts":"-server -Xmx12288m -Djava.net.preferIPv4Stack=true",
                    "tez.am.am-rm.heartbeat.interval-ms.max":"250",
                    "tez.am.container.idle.release-timeout-max.millis":"20000",
                    "tez.am.container.idle.release-timeout-min.millis":"10000",
                    "tez.am.container.reuse.enabled":"true",
                    "tez.am.container.reuse.locality.delay-allocation-millis":"250",
                    "tez.am.container.reuse.non-local-fallback.enabled":"false",
                    "tez.am.container.reuse.rack-fallback.enabled":"true",
                    "tez.am.launch.cluster-default.cmd-opts":"-server -Djava.net.preferIPv4Stack=true",
                    "tez.am.launch.cmd-opts":"-XX:+PrintGCDetails -verbose:gc -XX:+PrintGCTimeStamps -XX:+UseNUMA -XX:+UseG1GC -XX:+ResizeTLAB{{heap_dump_opts}}",
                    "tez.am.launch.env":"LD_LIBRARY_PATH={{hadoop_home}}/lib/native",
                    "tez.am.log.level":"INFO",
                    "tez.am.max.app.attempts":"2",
                    "tez.am.maxtaskfailures.per.node":"10",
                    "tez.am.resource.memory.mb":"15360",
                    "tez.am.tez-ui.history-url.template":"__HISTORY_URL_BASE__?viewPath=%2F%23%2Ftez-app%2F__APPLICATION_ID__",
                    "tez.am.view-acls":"*",
                    "tez.cluster.additional.classpath.prefix":"/etc/hadoop/conf/secure",
                    "tez.counters.max":"10000",
                    "tez.counters.max.groups":"3000",
                    "tez.generate.debug.artifacts":"false",
                    "tez.grouping.max-size":"1073741824",
                    "tez.grouping.min-size":"16777216",
                    "tez.grouping.split-waves":"1.7",
                    "tez.history.logging.timeline-cache-plugin.old-num-dags-per-group":"5",
                    "tez.lib.uris":"{{tez_lib_uris}}",
                    "tez.lib.uris.classpath":"{{hadoop_conf_dir}},{{hadoop_home}}/*,{{hadoop_home}}/lib/*,{{hadoop_hdfs_home}}/*,{{hadoop_hdfs_home}}/lib/*,{{hadoop_yarn_home}}/*,{{hadoop_yarn_home}}/lib/*,{{tez_home}}/*,{{tez_home}}/lib/*,{{tez_conf_dir}}",
                    "tez.runtime.compress":"true",
                    "tez.runtime.compress.codec":"org.apache.hadoop.io.compress.SnappyCodec",
                    "tez.runtime.convert.user-payload.to.history-text":"false",
                    "tez.runtime.io.sort.mb":"4055",
                    "tez.runtime.optimize.local.fetch":"true",
                    "tez.runtime.pipelined.sorter.sort.threads":"2",
                    "tez.runtime.shuffle.fetch.buffer.percent":"0.6",
                    "tez.runtime.shuffle.memory.limit.percent":"0.25",
                    "tez.runtime.sorter.class":"PIPELINED",
                    "tez.runtime.unordered.output.buffer.size-mb":"1152",
                    "tez.session.am.dag.submit.timeout.secs":"600",
                    "tez.session.client.timeout.secs":"-1",
                    "tez.shuffle-vertex-manager.max-src-fraction":"0.4",
                    "tez.shuffle-vertex-manager.min-src-fraction":"0.2",
                    "tez.staging-dir":"/tmp/${user.name}/staging",
                    "tez.task.am.heartbeat.counter.interval-ms.max":"4000",
                    "tez.task.generate.counters.per.io":"true",
                    "tez.task.get-task.sleep.interval-ms.max":"200",
                    "tez.task.launch.cluster-default.cmd-opts":"-server -Djava.net.preferIPv4Stack=true",
                    "tez.task.launch.cmd-opts":"-XX:+PrintGCDetails -verbose:gc -XX:+PrintGCTimeStamps -XX:+UseNUMA -XX:+UseG1GC -XX:+ResizeTLAB{{heap_dump_opts}}",
                    "tez.task.launch.env":"LD_LIBRARY_PATH={{hadoop_home}}/lib/native",
                    "tez.task.max-events-per-heartbeat":"500",
                    "tez.task.resource.memory.mb":"15360",
                    "tez.tez-ui.history-url.base":null,
                    "tez.use.cluster.hadoop-libs":"false",
                    "yarn.timeline-service.enabled":"false"
                }
            }
        },
        {
            "beeline-log4j2":{
                "properties_attributes":{

                },
                "properties":{
                    "content":"\n# Licensed to the Apache Software Foundation (ASF) under one\n# or more contributor license agreements.  See the NOTICE file\n# distributed with this work for additional information\n# regarding copyright ownership.  The ASF licenses this file\n# to you under the Apache License, Version 2.0 (the\n# \"License\"); you may not use this file except in compliance\n# with the License.  You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\nstatus = INFO\nname = BeelineLog4j2\npackages = org.apache.hadoop.hive.ql.log\n\n# list of properties\nproperty.hive.log.level = {{hive_log_level}}\nproperty.hive.root.logger = console\n\n# list of all appenders\nappenders = console\n\n# console appender\nappender.console.type = Console\nappender.console.name = console\nappender.console.target = SYSTEM_ERR\nappender.console.layout.type = PatternLayout\nappender.console.layout.pattern = %d{yy/MM/dd HH:mm:ss} [%t]: %p %c{2}: %m%n\n\n# list of all loggers\nloggers = HiveConnection\n\n# HiveConnection logs useful info for dynamic service discovery\nlogger.HiveConnection.name = org.apache.hive.jdbc.HiveConnection\nlogger.HiveConnection.level = INFO\n\n# root logger\nrootLogger.level = ${sys:hive.log.level}\nrootLogger.appenderRefs = root\nrootLogger.appenderRef.root.ref = ${sys:hive.root.logger}"
                }
            }
        },
        {
            "hcat-env":{
                "properties_attributes":{

                },
                "properties":{
                    "content":"\n      # Licensed to the Apache Software Foundation (ASF) under one\n      # or more contributor license agreements. See the NOTICE file\n      # distributed with this work for additional information\n      # regarding copyright ownership. The ASF licenses this file\n      # to you under the Apache License, Version 2.0 (the\n      # \"License\"); you may not use this file except in compliance\n      # with the License. You may obtain a copy of the License at\n      #\n      # http://www.apache.org/licenses/LICENSE-2.0\n      #\n      # Unless required by applicable law or agreed to in writing, software\n      # distributed under the License is distributed on an \"AS IS\" BASIS,\n      # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n      # See the License for the specific language governing permissions and\n      # limitations under the License.\n\n      JAVA_HOME={{java64_home}}\n      HCAT_PID_DIR={{hcat_pid_dir}}/\n      HCAT_LOG_DIR={{hcat_log_dir}}/\n      HCAT_CONF_DIR={{hcat_conf_dir}}\n      HADOOP_HOME=${HADOOP_HOME:-{{hadoop_home}}}\n      #DBROOT is the path where the connector jars are downloaded\n      DBROOT={{hcat_dbroot}}\n      USER={{webhcat_user}}\n      METASTORE_PORT={{hive_metastore_port}}"
                }
            }
        },
        {
            "hive-atlas-application.properties":{
                "properties_attributes":{

                },
                "properties":{
                    "atlas.hook.hive.keepAliveTime":"10",
                    "atlas.hook.hive.maxThreads":"5",
                    "atlas.hook.hive.minThreads":"5",
                    "atlas.hook.hive.numRetries":"3",
                    "atlas.hook.hive.queueSize":"1000",
                    "atlas.hook.hive.synchronous":"false"
                }
            }
        },
        {
            "hive-env":{
                "properties_attributes":{

                },
                "properties":{
                    "alert_ldap_password":"",
                    "alert_ldap_username":"",
                    "beeline_jdbc_url_default":"container",
                    "content":"\n# The heap size of the jvm, and jvm args stared by hive shell script can be controlled via:\nif [ \"$SERVICE\" = \"metastore\" ]; then\n\n  export HADOOP_HEAPSIZE={{hive_metastore_heapsize}} # Setting for HiveMetastore\n  export HADOOP_OPTS=\"$HADOOP_OPTS -Xloggc:{{hive_log_dir}}/hivemetastore-gc-%t.log -XX:+UseG1GC -XX:+PrintGCDetails -XX:+PrintGCTimeStamps -XX:+PrintGCCause -XX:+UseGCLogFileRotation -XX:NumberOfGCLogFiles=10 -XX:GCLogFileSize=10M -XX:+HeapDumpOnOutOfMemoryError -XX:HeapDumpPath={{hive_log_dir}}/hms_heapdump.hprof -Dhive.log.dir={{hive_log_dir}} -Dhive.log.file=hivemetastore.log\"\n\nfi\n\nif [ \"$SERVICE\" = \"hiveserver2\" ]; then\n\n  export HADOOP_HEAPSIZE={{hive_heapsize}} # Setting for HiveServer2 and Client\n  export HADOOP_OPTS=\"$HADOOP_OPTS -Xloggc:{{hive_log_dir}}/hiveserver2-gc-%t.log -XX:+UseG1GC -XX:+PrintGCDetails -XX:+PrintGCTimeStamps -XX:+PrintGCCause -XX:+UseGCLogFileRotation -XX:NumberOfGCLogFiles=10 -XX:GCLogFileSize=10M -XX:+HeapDumpOnOutOfMemoryError -XX:HeapDumpPath={{hive_log_dir}}/hs2_heapdump.hprof -Dhive.log.dir={{hive_log_dir}} -Dhive.log.file=hiveserver2.log\"\n\nfi\n\n{% if security_enabled %}\nexport HADOOP_OPTS=\"$HADOOP_OPTS -Dzookeeper.sasl.client.username={{zk_principal_user}}\"\n{% endif %}\n\nexport HADOOP_CLIENT_OPTS=\"$HADOOP_CLIENT_OPTS  -Xmx${HADOOP_HEAPSIZE}m\"\nexport HADOOP_CLIENT_OPTS=\"$HADOOP_CLIENT_OPTS{{heap_dump_opts}}\"\n\n# Larger heap size may be required when running queries over large number of files or partitions.\n# By default hive shell scripts use a heap size of 256 (MB).  Larger heap size would also be\n# appropriate for hive server (hwi etc).\n\n\n# Set HADOOP_HOME to point to a specific hadoop install directory\nHADOOP_HOME=${HADOOP_HOME:-{{hadoop_home}}}\n\nexport HIVE_HOME=${HIVE_HOME:-{{hive_home}}}\n\n# Hive Configuration Directory can be controlled by:\nexport HIVE_CONF_DIR=${HIVE_CONF_DIR:-{{hive_conf_dir}}}\n\n# Folder containing extra libraries required for hive compilation/execution can be controlled by:\nif [ \"${HIVE_AUX_JARS_PATH}\" != \"\" ]; then\n  export HIVE_AUX_JARS_PATH=${HIVE_AUX_JARS_PATH}\nelif [ -d \"{{hive_hcatalog_home}}\" ]; then\n  export HIVE_AUX_JARS_PATH={{hive_hcatalog_home}}/share/hcatalog/hive-hcatalog-core-*.jar\nelse\n  export HIVE_AUX_JARS_PATH={{hive_hcatalog_home}}/share/hcatalog/hcatalog-core.jar\nfi\nexport METASTORE_PORT={{hive_metastore_port}}\n\n{% if sqla_db_used or lib_dir_available %}\nexport LD_LIBRARY_PATH=\"$LD_LIBRARY_PATH:{{jdbc_libs_dir}}\"\nexport JAVA_LIBRARY_PATH=\"$JAVA_LIBRARY_PATH:{{jdbc_libs_dir}}\"\n{% endif %}",
                    "enable_heap_dump":"false",
                    "hcat_log_dir":"/var/log/webhcat",
                    "hcat_pid_dir":"/var/run/webhcat",
                    "heap_dump_location":"/tmp",
                    "hive.atlas.hook":"false",
                    "hive.heapsize":"24115",
                    "hive.log.level":"INFO",
                    "hive.metastore.heapsize":"8038",
                    "hive_ambari_database":"MySQL",
                    "hive_database":"New MySQL Database",
                    "hive_database_name":"hive",
                    "hive_database_type":"mysql",
                    "hive_log_dir":"/var/log/hive",
                    "hive_pid_dir":"/var/run/hive",
                    "hive_security_authorization":"None",
                    "hive_timeline_logging_enabled":"false",
                    "hive_user_nofile_limit":"32000",
                    "hive_user_nproc_limit":"16000",
                    "test_db_connection":"",
                    "hive_user":"hive",
                    "webhcat_user":"hcat"
                }
            }
        },
        {
            "hive-exec-log4j2":{
                "properties_attributes":{

                },
                "properties":{
                    "content":"\n# Licensed to the Apache Software Foundation (ASF) under one\n# or more contributor license agreements.  See the NOTICE file\n# distributed with this work for additional information\n# regarding copyright ownership.  The ASF licenses this file\n# to you under the Apache License, Version 2.0 (the\n# \"License\"); you may not use this file except in compliance\n# with the License.  You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\nstatus = INFO\nname = HiveExecLog4j2\npackages = org.apache.hadoop.hive.ql.log\n\n# list of properties\nproperty.hive.log.level = {{hive_log_level}}\nproperty.hive.root.logger = FA\nproperty.hive.query.id = hadoop\nproperty.hive.log.dir = ${sys:java.io.tmpdir}/${sys:user.name}\nproperty.hive.log.file = ${sys:hive.query.id}.log\n\n# list of all appenders\nappenders = console, FA\n\n# console appender\nappender.console.type = Console\nappender.console.name = console\nappender.console.target = SYSTEM_ERR\nappender.console.layout.type = PatternLayout\nappender.console.layout.pattern = %d{yy/MM/dd HH:mm:ss} [%t]: %p %c{2}: %m%n\n\n# simple file appender\nappender.FA.type = File\nappender.FA.name = FA\nappender.FA.fileName = ${sys:hive.log.dir}/${sys:hive.log.file}\nappender.FA.layout.type = PatternLayout\nappender.FA.layout.pattern = %d{ISO8601} %-5p [%t]: %c{2} (%F:%M(%L)) - %m%n\n\n# list of all loggers\nloggers = NIOServerCnxn, ClientCnxnSocketNIO, DataNucleus, Datastore, JPOX\n\nlogger.NIOServerCnxn.name = org.apache.zookeeper.server.NIOServerCnxn\nlogger.NIOServerCnxn.level = WARN\n\nlogger.ClientCnxnSocketNIO.name = org.apache.zookeeper.ClientCnxnSocketNIO\nlogger.ClientCnxnSocketNIO.level = WARN\n\nlogger.DataNucleus.name = DataNucleus\nlogger.DataNucleus.level = ERROR\n\nlogger.Datastore.name = Datastore\nlogger.Datastore.level = ERROR\n\nlogger.JPOX.name = JPOX\nlogger.JPOX.level = ERROR\n\n# root logger\nrootLogger.level = ${sys:hive.log.level}\nrootLogger.appenderRefs = root\nrootLogger.appenderRef.root.ref = ${sys:hive.root.logger}"
                }
            }
        },
        {
            "hive-log4j2":{
                "properties_attributes":{

                },
                "properties":{
                    "content":"\n# Licensed to the Apache Software Foundation (ASF) under one\n# or more contributor license agreements.  See the NOTICE file\n# distributed with this work for additional information\n# regarding copyright ownership.  The ASF licenses this file\n# to you under the Apache License, Version 2.0 (the\n# \"License\"); you may not use this file except in compliance\n# with the License.  You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\nstatus = INFO\nname = HiveLog4j2\npackages = org.apache.hadoop.hive.ql.log\n\n# list of properties\nproperty.hive.log.level = {{hive_log_level}}\nproperty.hive.root.logger = DRFA\nproperty.hive.log.dir = ${sys:java.io.tmpdir}/${sys:user.name}\nproperty.hive.log.file = hive.log\n\n# list of all appenders\nappenders = console, DRFA\n\n# console appender\nappender.console.type = Console\nappender.console.name = console\nappender.console.target = SYSTEM_ERR\nappender.console.layout.type = PatternLayout\nappender.console.layout.pattern = %d{yy/MM/dd HH:mm:ss} [%t]: %p %c{2}: %m%n\n\n# daily rolling file appender\nappender.DRFA.type = RollingFile\nappender.DRFA.name = DRFA\nappender.DRFA.fileName = ${sys:hive.log.dir}/${sys:hive.log.file}\n# Use %pid in the filePattern to append process-id@host-name to the filename if you want separate log files for different CLI session\nappender.DRFA.filePattern = ${sys:hive.log.dir}/${sys:hive.log.file}.%d{yyyy-MM-dd}_%i.gz\nappender.DRFA.layout.type = PatternLayout\nappender.DRFA.layout.pattern = %d{ISO8601} %-5p [%t]: %c{2} (%F:%M(%L)) - %m%n\nappender.DRFA.policies.type = Policies\nappender.DRFA.policies.time.type = TimeBasedTriggeringPolicy\nappender.DRFA.policies.time.interval = 1\nappender.DRFA.policies.time.modulate = true\nappender.DRFA.strategy.type = DefaultRolloverStrategy\nappender.DRFA.strategy.max = {{hive2_log_maxbackupindex}}\nappender.DRFA.policies.fsize.type = SizeBasedTriggeringPolicy\nappender.DRFA.policies.fsize.size = {{hive2_log_maxfilesize}}MB\n\n# list of all loggers\nloggers = NIOServerCnxn, ClientCnxnSocketNIO, DataNucleus, Datastore, JPOX\n\nlogger.NIOServerCnxn.name = org.apache.zookeeper.server.NIOServerCnxn\nlogger.NIOServerCnxn.level = WARN\n\nlogger.ClientCnxnSocketNIO.name = org.apache.zookeeper.ClientCnxnSocketNIO\nlogger.ClientCnxnSocketNIO.level = WARN\n\nlogger.DataNucleus.name = DataNucleus\nlogger.DataNucleus.level = ERROR\n\nlogger.Datastore.name = Datastore\nlogger.Datastore.level = ERROR\n\nlogger.JPOX.name = JPOX\nlogger.JPOX.level = ERROR\n\n# root logger\nrootLogger.level = ${sys:hive.log.level}\nrootLogger.appenderRefs = root\nrootLogger.appenderRef.root.ref = ${sys:hive.root.logger}",
                    "hive2_log_maxbackupindex":"30",
                    "hive2_log_maxfilesize":"256"
                }
            }
        },
        {
            "hive-site":{
                "properties_attributes":{

                },
                "properties":{
                    "ambari.hive.db.schema.name":"hive",
                    "atlas.hook.hive.maxThreads":"1",
                    "atlas.hook.hive.minThreads":"1",
                    "datanucleus.autoCreateSchema":"false",
                    "datanucleus.cache.level2.type":"none",
                    "datanucleus.fixedDatastore":"true",
                    "hive.auto.convert.join":"true",
                    "hive.auto.convert.join.noconditionaltask":"true",
                    "hive.auto.convert.join.noconditionaltask.size":"4294967296",
                    "hive.auto.convert.sortmerge.join":"true",
                    "hive.auto.convert.sortmerge.join.to.mapjoin":"true",
                    "hive.cbo.enable":"true",
                    "hive.cli.print.header":"false",
                    "hive.cluster.delegation.token.store.class":"org.apache.hadoop.hive.thrift.ZooKeeperTokenStore",
                    "hive.cluster.delegation.token.store.zookeeper.connectString":"ambari-server:2181",
                    "hive.cluster.delegation.token.store.zookeeper.znode":"/hive/cluster/delegation",
                    "hive.compactor.abortedtxn.threshold":"1000",
                    "hive.compactor.check.interval":"300",
                    "hive.compactor.delta.num.threshold":"10",
                    "hive.compactor.delta.pct.threshold":"0.1f",
                    "hive.compactor.initiator.on":"true",
                    "hive.compactor.worker.threads":"1",
                    "hive.compactor.worker.timeout":"86400",
                    "hive.compute.query.using.stats":"true",
                    "hive.convert.join.bucket.mapjoin.tez":"false",
                    "hive.create.as.insert.only":"false",
                    "hive.default.fileformat":"TextFile",
                    "hive.default.fileformat.managed":"ORC",
                    "hive.driver.parallel.compilation":"true",
                    "hive.enforce.sortmergebucketmapjoin":"true",
                    "hive.exec.compress.intermediate":"false",
                    "hive.exec.compress.output":"false",
                    "hive.exec.dynamic.partition":"true",
                    "hive.exec.dynamic.partition.mode":"nonstrict",
                    "hive.exec.failure.hooks":"",
                    "hive.exec.max.created.files":"100000",
                    "hive.exec.max.dynamic.partitions":"5000",
                    "hive.exec.max.dynamic.partitions.pernode":"2000",
                    "hive.exec.orc.split.strategy":"HYBRID",
                    "hive.exec.parallel":"false",
                    "hive.exec.parallel.thread.number":"8",
                    "hive.exec.post.hooks":" ",
                    "hive.exec.pre.hooks":"",
                    "hive.exec.reducers.bytes.per.reducer":"67108864",
                    "hive.exec.reducers.max":"1009",
                    "hive.exec.scratchdir":"/tmp/hive",
                    "hive.exec.submit.local.task.via.child":"true",
                    "hive.exec.submitviachild":"false",
                    "hive.execution.mode":"container",
                    "hive.fetch.task.aggr":"false",
                    "hive.fetch.task.conversion":"more",
                    "hive.fetch.task.conversion.threshold":"1073741824",
                    "hive.heapsize":"1024",
                    "hive.hook.proto.base-directory":"{hive_metastore_warehouse_external_dir}/sys.db/query_data/",
                    "hive.limit.optimize.enable":"true",
                    "hive.limit.pushdown.memory.usage":"0.04",
                    "hive.load.data.owner":"hive",
                    "hive.lock.manager":"",
                    "hive.map.aggr":"true",
                    "hive.map.aggr.hash.force.flush.memory.threshold":"0.9",
                    "hive.map.aggr.hash.min.reduction":"0.5",
                    "hive.map.aggr.hash.percentmemory":"0.5",
                    "hive.mapjoin.bucket.cache.size":"10000",
                    "hive.mapjoin.hybridgrace.hashtable":"false",
                    "hive.mapjoin.optimized.hashtable":"true",
                    "hive.mapred.reduce.tasks.speculative.execution":"false",
                    "hive.materializedview.rewriting.incremental":"false",
                    "hive.merge.mapfiles":"true",
                    "hive.merge.mapredfiles":"false",
                    "hive.merge.nway.joins":"false",
                    "hive.merge.orcfile.stripe.level":"true",
                    "hive.merge.rcfile.block.level":"true",
                    "hive.merge.size.per.task":"256000000",
                    "hive.merge.smallfiles.avgsize":"16000000",
                    "hive.merge.tezfiles":"false",
                    "hive.metastore.authorization.storage.checks":"false",
                    "hive.metastore.cache.pinobjtypes":"Table,Database,Type,FieldSchema,Order",
                    "hive.metastore.client.connect.retry.delay":"5s",
                    "hive.metastore.client.socket.timeout":"1800s",
                    "hive.metastore.connect.retries":"24",
                    "hive.metastore.db.type":"{{hive_metastore_db_type}}",
                    "hive.metastore.dml.events":"true",
                    "hive.metastore.event.db.notification.api.auth":"true",
                    "hive.metastore.event.listeners":"",
                    "hive.metastore.execute.setugi":"true",
                    "hive.metastore.failure.retries":"24",
                    "hive.metastore.kerberos.keytab.file":"/etc/security/keytabs/hive.service.keytab",
                    "hive.metastore.kerberos.principal":"hive/_HOST@EXAMPLE.COM",
                    "hive.metastore.pre.event.listeners":"org.apache.hadoop.hive.ql.security.authorization.AuthorizationPreEventListener",
                    "hive.metastore.sasl.enabled":"false",
                    "hive.metastore.server.max.threads":"100000",
                    "hive.metastore.transactional.event.listeners":"org.apache.hive.hcatalog.listener.DbNotificationListener",
                    "hive.metastore.uris":"thrift://ambari-server:9083",
                    "hive.metastore.warehouse.dir":"/warehouse/tablespace/managed/hive",
                    "hive.metastore.warehouse.external.dir":"/warehouse/tablespace/external/hive",
                    "hive.optimize.bucketmapjoin":"true",
                    "hive.optimize.bucketmapjoin.sortedmerge":"false",
                    "hive.optimize.constant.propagation":"true",
                    "hive.optimize.dynamic.partition.hashjoin":"true",
                    "hive.optimize.index.filter":"true",
                    "hive.optimize.metadataonly":"true",
                    "hive.optimize.null.scan":"true",
                    "hive.optimize.reducededuplication":"true",
                    "hive.optimize.reducededuplication.min.reducer":"4",
                    "hive.optimize.sort.dynamic.partition":"false",
                    "hive.orc.compute.splits.num.threads":"10",
                    "hive.orc.splits.include.file.footer":"false",
                    "hive.prewarm.enabled":"false",
                    "hive.prewarm.numcontainers":"3",
                    "hive.repl.cm.enabled":"",
                    "hive.repl.cmrootdir":"",
                    "hive.repl.rootdir":"",
                    "hive.security.metastore.authenticator.manager":"org.apache.hadoop.hive.ql.security.HadoopDefaultMetastoreAuthenticator",
                    "hive.security.metastore.authorization.auth.reads":"true",
                    "hive.security.metastore.authorization.manager":"org.apache.hadoop.hive.ql.security.authorization.StorageBasedAuthorizationProvider",
                    "hive.server2.allow.user.substitution":"true",
                    "hive.server2.authentication":"NONE",
                    "hive.server2.authentication.ldap.baseDN":null,
                    "hive.server2.authentication.pam.services":null,
                    "hive.server2.authentication.spnego.keytab":"HTTP/_HOST@EXAMPLE.COM",
                    "hive.server2.authentication.spnego.principal":"/etc/security/keytabs/spnego.service.keytab",
                    "hive.server2.custom.authentication.class":null,
                    "hive.server2.enable.doAs":"true",
                    "hive.server2.idle.operation.timeout":"6h",
                    "hive.server2.idle.session.timeout":"1d",
                    "hive.server2.logging.operation.enabled":"true",
                    "hive.server2.logging.operation.log.location":"/tmp/hive/operation_logs",
                    "hive.server2.materializedviews.registry.impl":"DUMMY",
                    "hive.server2.max.start.attempts":"5",
                    "hive.server2.support.dynamic.service.discovery":"true",
                    "hive.server2.table.type.mapping":"CLASSIC",
                    "hive.server2.tez.default.queues":"default",
                    "hive.server2.tez.initialize.default.sessions":"false",
                    "hive.server2.tez.sessions.per.default.queue":"1",
                    "hive.server2.thrift.http.port":"10001",
                    "hive.server2.thrift.max.worker.threads":"500",
                    "hive.server2.thrift.port":"10000",
                    "hive.server2.thrift.sasl.qop":"auth",
                    "hive.server2.transport.mode":"binary",
                    "hive.server2.use.SSL":"false",
                    "hive.server2.webui.cors.allowed.headers":"X-Requested-With,Content-Type,Accept,Origin,X-Requested-By,x-requested-by",
                    "hive.server2.webui.enable.cors":"true",
                    "hive.server2.webui.port":"10002",
                    "hive.server2.webui.use.ssl":"false",
                    "hive.server2.zookeeper.namespace":"hiveserver2",
                    "hive.service.metrics.codahale.reporter.classes":"org.apache.hadoop.hive.common.metrics.metrics2.JsonFileMetricsReporter,org.apache.hadoop.hive.common.metrics.metrics2.JmxMetricsReporter,org.apache.hadoop.hive.common.metrics.metrics2.Metrics2Reporter",
                    "hive.smbjoin.cache.rows":"10000",
                    "hive.stats.autogather":"true",
                    "hive.stats.dbclass":"fs",
                    "hive.stats.fetch.column.stats":"true",
                    "hive.stats.fetch.partition.stats":"true",
                    "hive.strict.managed.tables":"false",
                    "hive.support.concurrency":"true",
                    "hive.tez.auto.reducer.parallelism":"true",
                    "hive.tez.bucket.pruning":"true",
                    "hive.tez.cartesian-product.enabled":"true",
                    "hive.tez.container.size":"15360",
                    "hive.tez.cpu.vcores":"-1",
                    "hive.tez.dynamic.partition.pruning":"true",
                    "hive.tez.dynamic.partition.pruning.max.data.size":"104857600",
                    "hive.tez.dynamic.partition.pruning.max.event.size":"1048576",
                    "hive.tez.exec.print.summary":"true",
                    "hive.tez.input.format":"org.apache.hadoop.hive.ql.io.HiveInputFormat",
                    "hive.tez.input.generate.consistent.splits":"true",
                    "hive.tez.java.opts":"-server -Djava.net.preferIPv4Stack=true -XX:NewRatio=8 -XX:+UseNUMA -XX:+UseG1GC -XX:+ResizeTLAB -XX:+PrintGCDetails -verbose:gc -XX:+PrintGCTimeStamps",
                    "hive.tez.log.level":"INFO",
                    "hive.tez.max.partition.factor":"2.0",
                    "hive.tez.min.partition.factor":"0.25",
                    "hive.tez.smb.number.waves":"0.5",
                    "hive.txn.manager":"org.apache.hadoop.hive.ql.lockmgr.DbTxnManager",
                    "hive.txn.max.open.batch":"1000",
                    "hive.txn.strict.locking.mode":"false",
                    "hive.txn.timeout":"300",
                    "hive.user.install.directory":"/user/",
                    "hive.vectorized.adaptor.usage.mode":"chosen",
                    "hive.vectorized.execution.enabled":"true",
                    "hive.vectorized.execution.mapjoin.minmax.enabled":"true",
                    "hive.vectorized.execution.mapjoin.native.enabled":"true",
                    "hive.vectorized.execution.mapjoin.native.fast.hashtable.enabled":"true",
                    "hive.vectorized.execution.reduce.enabled":"true",
                    "hive.vectorized.groupby.checkinterval":"4096",
                    "hive.vectorized.groupby.flush.percent":"0.1",
                    "hive.vectorized.groupby.maxentries":"100000",
                    "hive.zookeeper.client.port":"2181",
                    "hive.zookeeper.namespace":"hive_zookeeper_namespace",
                    "hive.zookeeper.quorum":"ambari-server:2181",
                    "javax.jdo.option.ConnectionDriverName":"com.mysql.jdbc.Driver",
                    "javax.jdo.option.ConnectionPassword":"hive",
                    "javax.jdo.option.ConnectionURL":"jdbc:mysql://ambari-server/hive?createDatabaseIfNotExist=true",
                    "javax.jdo.option.ConnectionUserName":"hive",
                    "metastore.create.as.acid":"false",
                    "tez.session.am.dag.submit.timeout.secs":"0"
                }
            }
        },
        {
            "hivemetastore-site":{
                "properties_attributes":{

                },
                "properties":{
                    "hive.compactor.initiator.on":"true",
                    "hive.compactor.worker.threads":"5",
                    "hive.metastore.dml.events":"true",
                    "hive.metastore.event.listeners":"",
                    "hive.metastore.metrics.enabled":"true",
                    "hive.metastore.transactional.event.listeners":"org.apache.hive.hcatalog.listener.DbNotificationListener",
                    "hive.server2.metrics.enabled":"true",
                    "hive.service.metrics.hadoop2.component":"hivemetastore",
                    "hive.service.metrics.reporter":"HADOOP2"
                }
            }
        },
        {
            "hiveserver2-site":{
                "properties_attributes":{

                },
                "properties":{
                    "hive.metastore.metrics.enabled":"true",
                    "hive.security.authorization.enabled":"false",
                    "hive.server2.metrics.enabled":"true",
                    "hive.service.metrics.hadoop2.component":"hiveserver2",
                    "hive.service.metrics.reporter":"HADOOP2"
                }
            }
        },
        {
            "llap-cli-log4j2":{
                "properties_attributes":{

                },
                "properties":{
                    "content":"\n# Licensed to the Apache Software Foundation (ASF) under one\n# or more contributor license agreements.  See the NOTICE file\n# distributed with this work for additional information\n# regarding copyright ownership.  The ASF licenses this file\n# to you under the Apache License, Version 2.0 (the\n# \"License\"); you may not use this file except in compliance\n# with the License.  You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\nstatus = WARN\nname = LlapCliLog4j2\npackages = org.apache.hadoop.hive.ql.log\n\n# list of properties\nproperty.hive.log.level = WARN\nproperty.hive.root.logger = console\nproperty.hive.log.dir = ${sys:java.io.tmpdir}/${sys:user.name}\nproperty.hive.log.file = llap-cli.log\nproperty.hive.llapstatus.consolelogger.level = INFO\n\n# list of all appenders\nappenders = console, DRFA, llapstatusconsole\n\n# console appender\nappender.console.type = Console\nappender.console.name = console\nappender.console.target = SYSTEM_ERR\nappender.console.layout.type = PatternLayout\nappender.console.layout.pattern = %p %c{2}: %m%n\n\n# llapstatusconsole appender\nappender.llapstatusconsole.type = Console\nappender.llapstatusconsole.name = llapstatusconsole\nappender.llapstatusconsole.target = SYSTEM_OUT\nappender.llapstatusconsole.layout.type = PatternLayout\nappender.llapstatusconsole.layout.pattern = %m%n\n\n# daily rolling file appender\nappender.DRFA.type = RollingRandomAccessFile\nappender.DRFA.name = DRFA\nappender.DRFA.fileName = ${sys:hive.log.dir}/${sys:hive.log.file}\n# Use %pid in the filePattern to append process-id@host-name to the filename if you want separate log files for different CLI session\nappender.DRFA.filePattern = ${sys:hive.log.dir}/${sys:hive.log.file}.%d{yyyy-MM-dd}_%i\nappender.DRFA.layout.type = PatternLayout\nappender.DRFA.layout.pattern = %d{ISO8601} %5p [%t] %c{2}: %m%n\nappender.DRFA.policies.type = Policies\nappender.DRFA.policies.time.type = TimeBasedTriggeringPolicy\nappender.DRFA.policies.time.interval = 1\nappender.DRFA.policies.time.modulate = true\nappender.DRFA.strategy.type = DefaultRolloverStrategy\nappender.DRFA.strategy.max = {{llap_cli_log_maxbackupindex}}\nappender.DRFA.policies.fsize.type = SizeBasedTriggeringPolicy\nappender.DRFA.policies.fsize.size = {{llap_cli_log_maxfilesize}}MB\n\n# list of all loggers\nloggers = ZooKeeper, DataNucleus, Datastore, JPOX, HadoopConf, LlapStatusServiceDriverConsole\n\nlogger.ZooKeeper.name = org.apache.zookeeper\nlogger.ZooKeeper.level = WARN\n\nlogger.DataNucleus.name = DataNucleus\nlogger.DataNucleus.level = ERROR\n\nlogger.Datastore.name = Datastore\nlogger.Datastore.level = ERROR\n\nlogger.JPOX.name = JPOX\nlogger.JPOX.level = ERROR\n\nlogger.HadoopConf.name = org.apache.hadoop.conf.Configuration\nlogger.HadoopConf.level = ERROR\n\nlogger.LlapStatusServiceDriverConsole.name = LlapStatusServiceDriverConsole\nlogger.LlapStatusServiceDriverConsole.additivity = false\nlogger.LlapStatusServiceDriverConsole.level = ${sys:hive.llapstatus.consolelogger.level}\n\n\n# root logger\nrootLogger.level = ${sys:hive.log.level}\nrootLogger.appenderRefs = root, DRFA\nrootLogger.appenderRef.root.ref = ${sys:hive.root.logger}\nrootLogger.appenderRef.DRFA.ref = DRFA\nlogger.LlapStatusServiceDriverConsole.appenderRefs = llapstatusconsole, DRFA\nlogger.LlapStatusServiceDriverConsole.appenderRef.llapstatusconsole.ref = llapstatusconsole\nlogger.LlapStatusServiceDriverConsole.appenderRef.DRFA.ref = DRFA",
                    "llap_cli_log_maxbackupindex":"30",
                    "llap_cli_log_maxfilesize":"256"
                }
            }
        },
        {
            "llap-daemon-log4j":{
                "properties_attributes":{

                },
                "properties":{
                    "content":"\n# Licensed to the Apache Software Foundation (ASF) under one\n# or more contributor license agreements.  See the NOTICE file\n# distributed with this work for additional information\n# regarding copyright ownership.  The ASF licenses this file\n# to you under the Apache License, Version 2.0 (the\n# \"License\"); you may not use this file except in compliance\n# with the License.  You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n\n# This is the log4j2 properties file used by llap-daemons. There's several loggers defined, which\n# can be selected while configuring LLAP.\n# Based on the one selected - UI links etc need to be manipulated in the system.\n# Note: Some names and logic is common to this file and llap LogHelpers. Make sure to change that\n# as well, if changing this file.\n\nstatus = INFO\nname = LlapDaemonLog4j2\npackages = org.apache.hadoop.hive.ql.log\n\n# list of properties\nproperty.llap.daemon.log.level = {{hive_log_level}}\nproperty.llap.daemon.root.logger = console\nproperty.llap.daemon.log.dir = .\nproperty.llap.daemon.log.file = llapdaemon.log\nproperty.llap.daemon.historylog.file = llapdaemon_history.log\nproperty.llap.daemon.log.maxfilesize = {{hive_llap_log_maxfilesize}}MB\nproperty.llap.daemon.log.maxbackupindex = {{hive_llap_log_maxbackupindex}}\n\n# list of all appenders\nappenders = console, RFA, HISTORYAPPENDER, query-routing\n\n# console appender\nappender.console.type = Console\nappender.console.name = console\nappender.console.target = SYSTEM_ERR\nappender.console.layout.type = PatternLayout\nappender.console.layout.pattern = %d{ISO8601} %5p [%t (%X{fragmentId})] %c{2}: %m%n\n\n# rolling file appender\nappender.RFA.type = RollingRandomAccessFile\nappender.RFA.name = RFA\nappender.RFA.fileName = ${sys:llap.daemon.log.dir}/${sys:llap.daemon.log.file}\nappender.RFA.filePattern = ${sys:llap.daemon.log.dir}/${sys:llap.daemon.log.file}_%d{yyyy-MM-dd-HH}_%i.done\nappender.RFA.layout.type = PatternLayout\nappender.RFA.layout.pattern = %d{ISO8601} %-5p [%t (%X{fragmentId})] %c: %m%n\nappender.RFA.policies.type = Policies\nappender.RFA.policies.time.type = TimeBasedTriggeringPolicy\nappender.RFA.policies.time.interval = 1\nappender.RFA.policies.time.modulate = true\nappender.RFA.policies.size.type = SizeBasedTriggeringPolicy\nappender.RFA.policies.size.size = ${sys:llap.daemon.log.maxfilesize}\nappender.RFA.strategy.type = DefaultRolloverStrategy\nappender.RFA.strategy.max = ${sys:llap.daemon.log.maxbackupindex}\n\n# history file appender\nappender.HISTORYAPPENDER.type = RollingRandomAccessFile\nappender.HISTORYAPPENDER.name = HISTORYAPPENDER\nappender.HISTORYAPPENDER.fileName = ${sys:llap.daemon.log.dir}/${sys:llap.daemon.historylog.file}\nappender.HISTORYAPPENDER.filePattern = ${sys:llap.daemon.log.dir}/${sys:llap.daemon.historylog.file}_%d{yyyy-MM-dd}_%i.done\nappender.HISTORYAPPENDER.layout.type = PatternLayout\nappender.HISTORYAPPENDER.layout.pattern = %m%n\nappender.HISTORYAPPENDER.policies.type = Policies\nappender.HISTORYAPPENDER.policies.size.type = SizeBasedTriggeringPolicy\nappender.HISTORYAPPENDER.policies.size.size = ${sys:llap.daemon.log.maxfilesize}\nappender.HISTORYAPPENDER.policies.time.type = TimeBasedTriggeringPolicy\nappender.HISTORYAPPENDER.policies.time.interval = 1\nappender.HISTORYAPPENDER.policies.time.modulate = true\nappender.HISTORYAPPENDER.strategy.type = DefaultRolloverStrategy\nappender.HISTORYAPPENDER.strategy.max = ${sys:llap.daemon.log.maxbackupindex}\n\n# queryId based routing file appender\nappender.query-routing.type = Routing\nappender.query-routing.name = query-routing\nappender.query-routing.routes.type = Routes\nappender.query-routing.routes.pattern = $${ctx:queryId}\n#Purge polciy for query-based Routing Appender\nappender.query-routing.purgePolicy.type = LlapRoutingAppenderPurgePolicy\n# Note: Do not change this name without changing the corresponding entry in LlapConstants\nappender.query-routing.purgePolicy.name = llapLogPurgerQueryRouting\n# default route\nappender.query-routing.routes.route-default.type = Route\nappender.query-routing.routes.route-default.key = $${ctx:queryId}\nappender.query-routing.routes.route-default.ref = RFA\n# queryId based route\nappender.query-routing.routes.route-mdc.type = Route\nappender.query-routing.routes.route-mdc.file-mdc.type = LlapWrappedAppender\nappender.query-routing.routes.route-mdc.file-mdc.name = IrrelevantName-query-routing\nappender.query-routing.routes.route-mdc.file-mdc.app.type = RandomAccessFile\nappender.query-routing.routes.route-mdc.file-mdc.app.name = file-mdc\nappender.query-routing.routes.route-mdc.file-mdc.app.fileName = ${sys:llap.daemon.log.dir}/${ctx:queryId}-${ctx:dagId}.log\nappender.query-routing.routes.route-mdc.file-mdc.app.layout.type = PatternLayout\nappender.query-routing.routes.route-mdc.file-mdc.app.layout.pattern = %d{ISO8601} %5p [%t (%X{fragmentId})] %c{2}: %m%n\n\n# list of all loggers\nloggers = PerfLogger, EncodedReader, NIOServerCnxn, ClientCnxnSocketNIO, DataNucleus, Datastore, JPOX, HistoryLogger, LlapIoImpl, LlapIoOrc, LlapIoCache, LlapIoLocking, TezSM, TezSS, TezHC\n\nlogger.TezSM.name = org.apache.tez.runtime.library.common.shuffle.impl.ShuffleManager.fetch\nlogger.TezSM.level = WARN\nlogger.TezSS.name = org.apache.tez.runtime.library.common.shuffle.orderedgrouped.ShuffleScheduler.fetch\nlogger.TezSS.level = WARN\nlogger.TezHC.name = org.apache.tez.http.HttpConnection.url\nlogger.TezHC.level = WARN\n\nlogger.PerfLogger.name = org.apache.hadoop.hive.ql.log.PerfLogger\nlogger.PerfLogger.level = DEBUG\n\nlogger.EncodedReader.name = org.apache.hadoop.hive.ql.io.orc.encoded.EncodedReaderImpl\nlogger.EncodedReader.level = INFO\n\nlogger.LlapIoImpl.name = LlapIoImpl\nlogger.LlapIoImpl.level = INFO\n\nlogger.LlapIoOrc.name = LlapIoOrc\nlogger.LlapIoOrc.level = WARN\n\nlogger.LlapIoCache.name = LlapIoCache\nlogger.LlapIoCache.level = WARN\n\nlogger.LlapIoLocking.name = LlapIoLocking\nlogger.LlapIoLocking.level = WARN\n\nlogger.NIOServerCnxn.name = org.apache.zookeeper.server.NIOServerCnxn\nlogger.NIOServerCnxn.level = WARN\n\nlogger.ClientCnxnSocketNIO.name = org.apache.zookeeper.ClientCnxnSocketNIO\nlogger.ClientCnxnSocketNIO.level = WARN\n\nlogger.DataNucleus.name = DataNucleus\nlogger.DataNucleus.level = ERROR\n\nlogger.Datastore.name = Datastore\nlogger.Datastore.level = ERROR\n\nlogger.JPOX.name = JPOX\nlogger.JPOX.level = ERROR\n\nlogger.HistoryLogger.name = org.apache.hadoop.hive.llap.daemon.HistoryLogger\nlogger.HistoryLogger.level = INFO\nlogger.HistoryLogger.additivity = false\nlogger.HistoryLogger.appenderRefs = HistoryAppender\nlogger.HistoryLogger.appenderRef.HistoryAppender.ref = HISTORYAPPENDER\n\n# root logger\nrootLogger.level = ${sys:llap.daemon.log.level}\nrootLogger.appenderRefs = root\nrootLogger.appenderRef.root.ref = ${sys:llap.daemon.root.logger}",
                    "hive_llap_log_maxbackupindex":"240",
                    "hive_llap_log_maxfilesize":"256"
                }
            }
        },
        {
            "parquet-logging":{
                "properties_attributes":{

                },
                "properties":{
                    "content":"\n# Licensed to the Apache Software Foundation (ASF) under one\n# or more contributor license agreements.  See the NOTICE file\n# distributed with this work for additional information\n# regarding copyright ownership.  The ASF licenses this file\n# to you under the Apache License, Version 2.0 (the\n# \"License\"); you may not use this file except in compliance\n# with the License.  You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n# Properties file which configures the operation of the JDK\n# logging facility.\n\n# The system will look for this config file, first using\n# a System property specified at startup:\n#\n# >java -Djava.util.logging.config.file=myLoggingConfigFilePath\n#\n# If this property is not specified, then the config file is\n# retrieved from its default location at:\n#\n# JDK_HOME/jre/lib/logging.properties\n\n# Global logging properties.\n# ------------------------------------------\n# The set of handlers to be loaded upon startup.\n# Comma-separated list of class names.\n# (? LogManager docs say no comma here, but JDK example has comma.)\n# handlers=java.util.logging.ConsoleHandler\norg.apache.parquet.handlers= java.util.logging.FileHandler\n\n# Default global logging level.\n# Loggers and Handlers may override this level\n.level=INFO\n\n# Handlers\n# -----------------------------------------\n\n# --- ConsoleHandler ---\n# Override of global logging level\njava.util.logging.ConsoleHandler.level=INFO\njava.util.logging.ConsoleHandler.formatter=java.util.logging.SimpleFormatter\njava.util.logging.SimpleFormatter.format=[%1$tc] %4$s: %2$s - %5$s %6$s%n\n\n# --- FileHandler ---\n# Override of global logging level\njava.util.logging.FileHandler.level=ALL\n\n# Naming style for the output file:\n# (The output file is placed in the system temporary directory.\n# %u is used to provide unique identifier for the file.\n# For more information refer\n# https://docs.oracle.com/javase/7/docs/api/java/util/logging/FileHandler.html)\njava.util.logging.FileHandler.pattern=%t/parquet-%u.log\n\n# Limiting size of output file in bytes:\njava.util.logging.FileHandler.limit=50000000\n\n# Number of output files to cycle through, by appending an\n# integer to the base file name:\njava.util.logging.FileHandler.count=1\n\n# Style of output (Simple or XML):\njava.util.logging.FileHandler.formatter=java.util.logging.SimpleFormatter"
                }
            }
        },
        {
            "ranger-hive-audit":{
                "properties_attributes":{

                },
                "properties":{
                    "ranger.plugin.hive.ambari.cluster.name":"{{cluster_name}}",
                    "xasecure.audit.destination.hdfs":"true",
                    "xasecure.audit.destination.hdfs.batch.filespool.dir":"/var/log/hive/audit/hdfs/spool",
                    "xasecure.audit.destination.hdfs.dir":"hdfs://NAMENODE_HOSTNAME:8020/ranger/audit",
                    "xasecure.audit.destination.solr":"false",
                    "xasecure.audit.destination.solr.batch.filespool.dir":"/var/log/hive/audit/solr/spool",
                    "xasecure.audit.destination.solr.urls":"",
                    "xasecure.audit.destination.solr.zookeepers":"NONE",
                    "xasecure.audit.is.enabled":"true",
                    "xasecure.audit.provider.summary.enabled":"false"
                }
            }
        },
        {
            "ranger-hive-plugin-properties":{
                "properties_attributes":{

                },
                "properties":{
                    "REPOSITORY_CONFIG_PASSWORD":"hive",
                    "REPOSITORY_CONFIG_USERNAME":"hive",
                    "common.name.for.certificate":"",
                    "external_admin_password":"",
                    "external_admin_username":"",
                    "external_ranger_admin_password":"",
                    "external_ranger_admin_username":"",
                    "jdbc.driverClassName":"org.apache.hive.jdbc.HiveDriver",
                    "policy_user":"ambari-qa"
                }
            }
        },
        {
            "ranger-hive-policymgr-ssl":{
                "properties_attributes":{

                },
                "properties":{
                    "xasecure.policymgr.clientssl.keystore":"",
                    "xasecure.policymgr.clientssl.keystore.credential.file":"jceks://file{{credential_file}}",
                    "xasecure.policymgr.clientssl.keystore.password":"myKeyFilePassword",
                    "xasecure.policymgr.clientssl.truststore":"",
                    "xasecure.policymgr.clientssl.truststore.credential.file":"jceks://file{{credential_file}}",
                    "xasecure.policymgr.clientssl.truststore.password":"changeit"
                }
            }
        },
        {
            "ranger-hive-security":{
                "properties_attributes":{

                },
                "properties":{
                    "ranger.plugin.hive.policy.cache.dir":"/etc/ranger/{{repo_name}}/policycache",
                    "ranger.plugin.hive.policy.pollIntervalMs":"30000",
                    "ranger.plugin.hive.policy.rest.ssl.config.file":"{{ranger_hive_ssl_config_file}}",
                    "ranger.plugin.hive.policy.rest.url":"{{policymgr_mgr_url}}",
                    "ranger.plugin.hive.policy.source.impl":"org.apache.ranger.admin.client.RangerAdminRESTClient",
                    "ranger.plugin.hive.service.name":"{{repo_name}}",
                    "ranger.plugin.hive.urlauth.filesystem.schemes":"hdfs:,file:,wasb:,adl:",
                    "xasecure.hive.update.xapolicies.on.grant.revoke":"true"
                }
            }
        },
        {
            "webhcat-env":{
                "properties_attributes":{

                },
                "properties":{
                    "content":"\n# The file containing the running pid\nPID_FILE={{webhcat_pid_file}}\n\nTEMPLETON_LOG_DIR={{templeton_log_dir}}/\n\n\nWEBHCAT_LOG_DIR={{templeton_log_dir}}/\n\n# The console error log\nERROR_LOG={{templeton_log_dir}}/webhcat-console-error.log\n\n# The console log\nCONSOLE_LOG={{templeton_log_dir}}/webhcat-console.log\n\n#TEMPLETON_JAR=templeton_jar_name\n\n#HADOOP_PREFIX=hadoop_prefix\n\n#HCAT_PREFIX=hive_prefix\n\n# Set HADOOP_HOME to point to a specific hadoop install directory\nexport HADOOP_HOME=${HADOOP_HOME:-{{hadoop_home}}}"
                }
            }
        },
        {
            "webhcat-log4j":{
                "properties_attributes":{

                },
                "properties":{
                    "content":"\n# Licensed to the Apache Software Foundation (ASF) under one\n# or more contributor license agreements.  See the NOTICE file\n# distributed with this work for additional information\n# regarding copyright ownership.  The ASF licenses this file\n# to you under the Apache License, Version 2.0 (the\n# \"License\"); you may not use this file except in compliance\n# with the License.  You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing,\n# software distributed under the License is distributed on an\n# \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n# KIND, either express or implied.  See the License for the\n# specific language governing permissions and limitations\n# under the License.\n\n# Define some default values that can be overridden by system properties\nwebhcat.root.logger = INFO, standard\nwebhcat.log.dir = .\nwebhcat.log.file = webhcat.log\n\nlog4j.rootLogger = ${webhcat.root.logger}\n\n# Logging Threshold\nlog4j.threshhold = DEBUG\n\nlog4j.appender.standard  =  org.apache.log4j.DailyRollingFileAppender\nlog4j.appender.standard.File = ${webhcat.log.dir}/${webhcat.log.file}\nlog4j.appender.standard.MaxFileSize = {{webhcat_log_maxfilesize}}MB\nlog4j.appender.standard.MaxBackupIndex = {{webhcat_log_maxbackupindex}}\n\n# Rollver at midnight\nlog4j.appender.DRFA.DatePattern = .yyyy-MM-dd\n\nlog4j.appender.DRFA.layout = org.apache.log4j.PatternLayout\n\nlog4j.appender.standard.layout = org.apache.log4j.PatternLayout\nlog4j.appender.standard.layout.conversionPattern = %-5p | %d{DATE} | %c | %m%n\n\n# Class logging settings\nlog4j.logger.com.sun.jersey = DEBUG\nlog4j.logger.com.sun.jersey.spi.container.servlet.WebComponent = ERROR\nlog4j.logger.org.apache.hadoop = INFO\nlog4j.logger.org.apache.hadoop.conf = WARN\nlog4j.logger.org.apache.zookeeper = WARN\nlog4j.logger.org.eclipse.jetty = INFO",
                    "webhcat_log_maxbackupindex":"20",
                    "webhcat_log_maxfilesize":"256"
                }
            }
        },
        {
            "webhcat-site":{
                "properties_attributes":{

                },
                "properties":{
                    "templeton.exec.timeout":"60000",
                    "templeton.hadoop":"/usr/bin/hadoop",
                    "templeton.hadoop.conf.dir":"/etc/hadoop/conf",
                    "templeton.hadoop.queue.name":"default",
                    "templeton.hcat":"/usr/bin/hcat",
                    "templeton.hcat.home":"hive.tar.gz/hive/hcatalog",
                    "templeton.hive.archive":"hdfs:///apps/webhcat/hive.tar.gz",
                    "templeton.hive.home":"hive.tar.gz/hive",
                    "templeton.hive.path":"hive.tar.gz/hive/bin/hive",
                    "templeton.hive.properties":"hive.metastore.local=false,hive.metastore.uris=thrift://ambari-server:9083,hive.metastore.sasl.enabled=false,hive.metastore.execute.setugi=true",
                    "templeton.jar":"{{hive_hcatalog_home}}/share/webhcat/svr/webhcat.jar",
                    "templeton.libjars":"{{zk_home}}/zookeeper.jar",
                    "templeton.override.enabled":"false",
                    "templeton.port":"50111",
                    "templeton.python":"${env.PYTHON_CMD}",
                    "templeton.storage.class":"org.apache.hive.hcatalog.templeton.tool.ZooKeeperStorage",
                    "templeton.streaming.jar":"hdfs:///apps/webhcat/hadoop-streaming.jar",
                    "templeton.zookeeper.hosts":"ambari-server:2181"
                }
            }
        },
        {
            "hbase-env":{
                "properties_attributes":{

                },
                "properties":{
                    "content":"\n# Set environment variables here.\n\n# The java implementation to use. Java 1.6 required.\nexport JAVA_HOME={{java64_home}}\n\n# HBase Configuration directory\nexport HBASE_CONF_DIR=${HBASE_CONF_DIR:-{{hbase_conf_dir}}}\n\n# Extra Java CLASSPATH elements. Optional.\nexport HBASE_CLASSPATH=${HBASE_CLASSPATH}\n\n\n# The maximum amount of heap to use, in MB. Default is 1000.\n# export HBASE_HEAPSIZE=1000\n\n# Extra Java runtime options.\n# Below are what we set by default. May only work with SUN JVM.\n# For more on why as well as other possible settings,\n# see http://wiki.apache.org/hadoop/PerformanceTuning\nexport SERVER_GC_OPTS=\"-verbose:gc -XX:-PrintGCCause -XX:+PrintAdaptiveSizePolicy -XX:+PrintGCDetails -XX:+PrintGCDateStamps -Xloggc:{{log_dir}}/gc.log-`date +'%Y%m%d%H%M'`\"\n# Uncomment below to enable java garbage collection logging.\n# export HBASE_OPTS=\"$HBASE_OPTS -verbose:gc -XX:+PrintGCDetails -XX:+PrintGCDateStamps -Xloggc:$HBASE_HOME/logs/gc-hbase.log\"\n\n# Uncomment and adjust to enable JMX exporting\n# See jmxremote.password and jmxremote.access in $JRE_HOME/lib/management to configure remote password access.\n# More details at: http://java.sun.com/javase/6/docs/technotes/guides/management/agent.html\n#\n# export HBASE_JMX_BASE=\"-Dcom.sun.management.jmxremote.ssl=false -Dcom.sun.management.jmxremote.authenticate=false\"\n# If you want to configure BucketCache, specify '-XX: MaxDirectMemorySize=' with proper direct memory size\n# export HBASE_THRIFT_OPTS=\"$HBASE_JMX_BASE -Dcom.sun.management.jmxremote.port=10103\"\n# export HBASE_ZOOKEEPER_OPTS=\"$HBASE_JMX_BASE -Dcom.sun.management.jmxremote.port=10104\"\n\n# File naming hosts on which HRegionServers will run. $HBASE_HOME/conf/regionservers by default.\nexport HBASE_REGIONSERVERS=${HBASE_CONF_DIR}/regionservers\n\n# Extra ssh options. Empty by default.\n# export HBASE_SSH_OPTS=\"-o ConnectTimeout=1 -o SendEnv=HBASE_CONF_DIR\"\n\n# Where log files are stored. $HBASE_HOME/logs by default.\nexport HBASE_LOG_DIR={{log_dir}}\n\n# A string representing this instance of hbase. $USER by default.\n# export HBASE_IDENT_STRING=$USER\n\n# The scheduling priority for daemon processes. See 'man nice'.\n# export HBASE_NICENESS=10\n\n# The directory where pid files are stored. /tmp by default.\nexport HBASE_PID_DIR={{pid_dir}}\n\n# Seconds to sleep between slave commands. Unset by default. This\n# can be useful in large clusters, where, e.g., slave rsyncs can\n# otherwise arrive faster than the master can service them.\n# export HBASE_SLAVE_SLEEP=0.1\n\n# Tell HBase whether it should manage it's own instance of Zookeeper or not.\nexport HBASE_MANAGES_ZK=false\n\n{% if java_version < 8 %}\nJDK_DEPENDED_OPTS=\"-XX:PermSize=128m -XX:MaxPermSize=128m -XX:ReservedCodeCacheSize=256m\"\n{% endif %}\n\n# Set common JVM configuration\nexport HBASE_OPTS=\"$HBASE_OPTS -XX:+UseG1GC -XX:MaxGCPauseMillis=100 -XX:-ResizePLAB -XX:ErrorFile={{log_dir}}/hs_err_pid%p.log -Djava.io.tmpdir={{java_io_tmpdir}}\"\nexport HBASE_MASTER_OPTS=\"$HBASE_MASTER_OPTS -Xmx{{master_heapsize}} -XX:ParallelGCThreads={{parallel_gc_threads}} $JDK_DEPENDED_OPTS \"\nexport HBASE_REGIONSERVER_OPTS=\"$HBASE_REGIONSERVER_OPTS -Xms{{regionserver_heapsize}} -Xmx{{regionserver_heapsize}} -XX:ParallelGCThreads={{parallel_gc_threads}} $JDK_DEPENDED_OPTS\"\n\n# Add Kerberos authentication-related configuration\n{% if security_enabled %}\nexport HBASE_OPTS=\"$HBASE_OPTS -Djava.security.auth.login.config={{client_jaas_config_file}} {{zk_security_opts}}\"\nexport HBASE_MASTER_OPTS=\"$HBASE_MASTER_OPTS -Djava.security.auth.login.config={{master_jaas_config_file}} -Djavax.security.auth.useSubjectCredsOnly=false\"\nexport HBASE_REGIONSERVER_OPTS=\"$HBASE_REGIONSERVER_OPTS -Djava.security.auth.login.config={{regionserver_jaas_config_file}} -Djavax.security.auth.useSubjectCredsOnly=false\"\nexport HBASE_SERVER_JAAS_OPTS=\"-Djava.security.auth.login.config={{client_jaas_config_file}}\"\n{% endif %}\n\n# HBase off-heap MaxDirectMemorySize\nexport HBASE_REGIONSERVER_OPTS=\"$HBASE_REGIONSERVER_OPTS {% if hbase_max_direct_memory_size %} -XX:MaxDirectMemorySize={{hbase_max_direct_memory_size}}m {% endif %}\"\nexport HBASE_MASTER_OPTS=\"$HBASE_MASTER_OPTS {% if hbase_max_direct_memory_size %} -XX:MaxDirectMemorySize={{hbase_max_direct_memory_size}}m {% endif %}\"",
                    "hbase.atlas.hook":"false",
                    "hbase_java_io_tmpdir":"/tmp",
                    "hbase_log_dir":"/var/log/hbase",
                    "hbase_master_heapsize":"1024",
                    "hbase_max_direct_memory_size":"",
                    "hbase_parallel_gc_threads":"8",
                    "hbase_pid_dir":"/var/run/hbase",
                    "hbase_principal_name":null,
                    "hbase_regionserver_heapsize":"8192",
                    "hbase_regionserver_shutdown_timeout":"30",
                    "hbase_regionserver_xmn_max":"4000",
                    "hbase_regionserver_xmn_ratio":"0.2",
                    "hbase_user_keytab":null,
                    "hbase_user_nofile_limit":"32000",
                    "hbase_user_nproc_limit":"16000",
                    "phoenix_sql_enabled":"false",
                    "hbase_user":"hbase"
                }
            }
        },
        {
            "hbase-log4j":{
                "properties_attributes":{

                },
                "properties":{
                    "content":"\n# Licensed to the Apache Software Foundation (ASF) under one\n# or more contributor license agreements.  See the NOTICE file\n# distributed with this work for additional information\n# regarding copyright ownership.  The ASF licenses this file\n# to you under the Apache License, Version 2.0 (the\n# \"License\"); you may not use this file except in compliance\n# with the License.  You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n\n# Define some default values that can be overridden by system properties\nhbase.root.logger=INFO,console\nhbase.security.logger=INFO,console\nhbase.log.dir=.\nhbase.log.file=hbase.log\n\n# Define the root logger to the system property \"hbase.root.logger\".\nlog4j.rootLogger=${hbase.root.logger}\n\n# Logging Threshold\nlog4j.threshold=ALL\n\n#\n# Daily Rolling File Appender\n#\nlog4j.appender.DRFA=org.apache.log4j.DailyRollingFileAppender\nlog4j.appender.DRFA.File=${hbase.log.dir}/${hbase.log.file}\n\n# Rollver at midnight\nlog4j.appender.DRFA.DatePattern=.yyyy-MM-dd\n\n# 30-day backup\n#log4j.appender.DRFA.MaxBackupIndex=30\nlog4j.appender.DRFA.layout=org.apache.log4j.PatternLayout\n\n# Pattern format: Date LogLevel LoggerName LogMessage\nlog4j.appender.DRFA.layout.ConversionPattern=%d{ISO8601} %-5p [%t] %c{2}: %m%n\n\n# Rolling File Appender properties\nhbase.log.maxfilesize={{hbase_log_maxfilesize}}MB\nhbase.log.maxbackupindex={{hbase_log_maxbackupindex}}\n\n# Rolling File Appender\nlog4j.appender.RFA=org.apache.log4j.RollingFileAppender\nlog4j.appender.RFA.File=${hbase.log.dir}/${hbase.log.file}\n\nlog4j.appender.RFA.MaxFileSize=${hbase.log.maxfilesize}\nlog4j.appender.RFA.MaxBackupIndex=${hbase.log.maxbackupindex}\n\nlog4j.appender.RFA.layout=org.apache.log4j.PatternLayout\nlog4j.appender.RFA.layout.ConversionPattern=%d{ISO8601} %-5p [%t] %c{2}: %m%n\n\n#\n# Security audit appender\n#\nhbase.security.log.file=SecurityAuth.audit\nhbase.security.log.maxfilesize={{hbase_security_log_maxfilesize}}MB\nhbase.security.log.maxbackupindex={{hbase_security_log_maxbackupindex}}\nlog4j.appender.RFAS=org.apache.log4j.RollingFileAppender\nlog4j.appender.RFAS.File=${hbase.log.dir}/${hbase.security.log.file}\nlog4j.appender.RFAS.MaxFileSize=${hbase.security.log.maxfilesize}\nlog4j.appender.RFAS.MaxBackupIndex=${hbase.security.log.maxbackupindex}\nlog4j.appender.RFAS.layout=org.apache.log4j.PatternLayout\nlog4j.appender.RFAS.layout.ConversionPattern=%d{ISO8601} %p %c: %m%n\nlog4j.category.SecurityLogger=${hbase.security.logger}\nlog4j.additivity.SecurityLogger=false\n#log4j.logger.SecurityLogger.org.apache.hadoop.hbase.security.access.AccessController=TRACE\n\n#\n# Null Appender\n#\nlog4j.appender.NullAppender=org.apache.log4j.varia.NullAppender\n\n#\n# console\n# Add \"console\" to rootlogger above if you want to use this\n#\nlog4j.appender.console=org.apache.log4j.ConsoleAppender\nlog4j.appender.console.target=System.err\nlog4j.appender.console.layout=org.apache.log4j.PatternLayout\nlog4j.appender.console.layout.ConversionPattern=%d{ISO8601} %-5p [%t] %c{2}: %m%n\n\n# Custom Logging levels\n\nlog4j.logger.org.apache.zookeeper=INFO\n#log4j.logger.org.apache.hadoop.fs.FSNamesystem=DEBUG\nlog4j.logger.org.apache.hadoop.hbase=INFO\n# Make these two classes INFO-level. Make them DEBUG to see more zk debug.\nlog4j.logger.org.apache.hadoop.hbase.zookeeper.ZKUtil=INFO\nlog4j.logger.org.apache.hadoop.hbase.zookeeper.ZooKeeperWatcher=INFO\n#log4j.logger.org.apache.hadoop.dfs=DEBUG\n# Set this class to log INFO only otherwise its OTT\n# Enable this to get detailed connection error/retry logging.\n# log4j.logger.org.apache.hadoop.hbase.client.HConnectionManager$HConnectionImplementation=TRACE\n\n\n# Uncomment this line to enable tracing on _every_ RPC call (this can be a lot of output)\n#log4j.logger.org.apache.hadoop.ipc.HBaseServer.trace=DEBUG\n\n# Uncomment the below if you want to remove logging of client region caching'\n# and scan of .META. messages\n# log4j.logger.org.apache.hadoop.hbase.client.HConnectionManager$HConnectionImplementation=INFO\n# log4j.logger.org.apache.hadoop.hbase.client.MetaScanner=INFO",
                    "hbase_log_maxbackupindex":"20",
                    "hbase_log_maxfilesize":"256",
                    "hbase_security_log_maxbackupindex":"20",
                    "hbase_security_log_maxfilesize":"256"
                }
            }
        },
        {
            "hbase-policy":{
                "properties_attributes":{

                },
                "properties":{
                    "security.admin.protocol.acl":"*",
                    "security.client.protocol.acl":"*",
                    "security.masterregion.protocol.acl":"*"
                }
            }
        },
        {
            "hbase-site":{
                "properties_attributes":{

                },
                "properties":{
                    "dfs.domain.socket.path":"/var/lib/hadoop-hdfs/dn_socket",
                    "hbase.bucketcache.ioengine":"",
                    "hbase.bucketcache.percentage.in.combinedcache":"",
                    "hbase.bucketcache.size":"",
                    "hbase.bulkload.staging.dir":"/apps/hbase/staging",
                    "hbase.client.keyvalue.maxsize":"1048576",
                    "hbase.client.retries.number":"35",
                    "hbase.client.scanner.caching":"100",
                    "hbase.cluster.distributed":"true",
                    "hbase.coprocessor.master.classes":"",
                    "hbase.coprocessor.region.classes":"org.apache.hadoop.hbase.security.access.SecureBulkLoadEndpoint",
                    "hbase.coprocessor.regionserver.classes":"",
                    "hbase.defaults.for.version.skip":"true",
                    "hbase.hregion.majorcompaction":"604800000",
                    "hbase.hregion.majorcompaction.jitter":"0.50",
                    "hbase.hregion.max.filesize":"10737418240",
                    "hbase.hregion.memstore.block.multiplier":"4",
                    "hbase.hregion.memstore.flush.size":"134217728",
                    "hbase.hregion.memstore.mslab.enabled":"true",
                    "hbase.hstore.blockingStoreFiles":"100",
                    "hbase.hstore.compaction.max":"10",
                    "hbase.hstore.compactionThreshold":"3",
                    "hbase.local.dir":"${hbase.tmp.dir}/local",
                    "hbase.master.info.bindAddress":"0.0.0.0",
                    "hbase.master.info.port":"16010",
                    "hbase.master.namespace.init.timeout":"2400000",
                    "hbase.master.port":"16000",
                    "hbase.master.ui.readonly":"false",
                    "hbase.master.wait.on.regionservers.timeout":"30000",
                    "hbase.region.server.rpc.scheduler.factory.class":"",
                    "hbase.regionserver.executor.openregion.threads":"20",
                    "hbase.regionserver.global.memstore.size":"0.4",
                    "hbase.regionserver.handler.count":"30",
                    "hbase.regionserver.info.port":"16030",
                    "hbase.regionserver.port":"16020",
                    "hbase.regionserver.wal.codec":"org.apache.hadoop.hbase.regionserver.wal.WALCellCodec",
                    "hbase.rootdir":"/apps/hbase/data",
                    "hbase.rpc.controllerfactory.class":"",
                    "hbase.rpc.protection":"authentication",
                    "hbase.rpc.timeout":"90000",
                    "hbase.security.authentication":"simple",
                    "hbase.security.authorization":"false",
                    "hbase.superuser":"hbase",
                    "hbase.tmp.dir":"/tmp/hbase-${user.name}",
                    "hbase.zookeeper.property.clientPort":"2181",
                    "hbase.zookeeper.quorum":"ambari-server",
                    "hbase.zookeeper.useMulti":"true",
                    "hfile.block.cache.size":"0.4",
                    "phoenix.functions.allowUserDefinedFunctions":" ",
                    "phoenix.query.timeoutMs":"60000",
                    "phoenix.rpc.index.handler.count":"10",
                    "zookeeper.recovery.retry":"6",
                    "zookeeper.session.timeout":"90000",
                    "zookeeper.znode.parent":"/hbase-unsecure"
                }
            }
        },
        {
            "ranger-hbase-audit":{
                "properties_attributes":{

                },
                "properties":{
                    "ranger.plugin.hbase.ambari.cluster.name":"{{cluster_name}}",
                    "xasecure.audit.destination.hdfs":"true",
                    "xasecure.audit.destination.hdfs.batch.filespool.dir":"/var/log/hbase/audit/hdfs/spool",
                    "xasecure.audit.destination.hdfs.dir":"hdfs://NAMENODE_HOSTNAME:8020/ranger/audit",
                    "xasecure.audit.destination.solr":"false",
                    "xasecure.audit.destination.solr.batch.filespool.dir":"/var/log/hbase/audit/solr/spool",
                    "xasecure.audit.destination.solr.urls":"",
                    "xasecure.audit.destination.solr.zookeepers":"NONE",
                    "xasecure.audit.is.enabled":"true",
                    "xasecure.audit.provider.summary.enabled":"true"
                }
            }
        },
        {
            "ranger-hbase-plugin-properties":{
                "properties_attributes":{

                },
                "properties":{
                    "REPOSITORY_CONFIG_PASSWORD":"hbase",
                    "REPOSITORY_CONFIG_USERNAME":"hbase",
                    "common.name.for.certificate":"",
                    "external_admin_password":"",
                    "external_admin_username":"",
                    "external_ranger_admin_password":"",
                    "external_ranger_admin_username":"",
                    "policy_user":"ambari-qa",
                    "ranger-hbase-plugin-enabled":"No"
                }
            }
        },
        {
            "ranger-hbase-policymgr-ssl":{
                "properties_attributes":{

                },
                "properties":{
                    "xasecure.policymgr.clientssl.keystore":"",
                    "xasecure.policymgr.clientssl.keystore.credential.file":"jceks://file{{credential_file}}",
                    "xasecure.policymgr.clientssl.keystore.password":"myKeyFilePassword",
                    "xasecure.policymgr.clientssl.truststore":"",
                    "xasecure.policymgr.clientssl.truststore.credential.file":"jceks://file{{credential_file}}",
                    "xasecure.policymgr.clientssl.truststore.password":"changeit"
                }
            }
        },
        {
            "ranger-hbase-security":{
                "properties_attributes":{

                },
                "properties":{
                    "ranger.plugin.hbase.policy.cache.dir":"/etc/ranger/{{repo_name}}/policycache",
                    "ranger.plugin.hbase.policy.pollIntervalMs":"30000",
                    "ranger.plugin.hbase.policy.rest.ssl.config.file":"/etc/hbase/conf/ranger-policymgr-ssl.xml",
                    "ranger.plugin.hbase.policy.rest.url":"{{policymgr_mgr_url}}",
                    "ranger.plugin.hbase.policy.source.impl":"org.apache.ranger.admin.client.RangerAdminRESTClient",
                    "ranger.plugin.hbase.service.name":"{{repo_name}}",
                    "xasecure.hbase.update.xapolicies.on.grant.revoke":"true"
                }
            }
        },
        {
            "zoo.cfg":{
                "properties_attributes":{

                },
                "properties":{
                    "4lw.commands.whitelist":"ruok",
                    "admin.enableServer":"true",
                    "admin.serverPort":"9393",
                    "autopurge.purgeInterval":"24",
                    "autopurge.snapRetainCount":"30",
                    "clientPort":"2181",
                    "dataDir":"/hadoop/zookeeper",
                    "initLimit":"10",
                    "syncLimit":"5",
                    "tickTime":"3000"
                }
            }
        },
        {
            "zookeeper-env":{
                "properties_attributes":{

                },
                "properties":{
                    "content":"\nexport JAVA_HOME={{java64_home}}\nexport ZOOKEEPER_HOME={{zk_home}}\nexport ZOO_LOG_DIR={{zk_log_dir}}\nexport ZOOPIDFILE={{zk_pid_file}}\nexport SERVER_JVMFLAGS={{zk_server_heapsize}}\nexport JAVA=$JAVA_HOME/bin/java\nexport CLASSPATH=$CLASSPATH:/usr/share/zookeeper/*\n\n{% if security_enabled %}\nexport SERVER_JVMFLAGS=\"$SERVER_JVMFLAGS -Djava.security.auth.login.config={{zk_server_jaas_file}}\"\nexport CLIENT_JVMFLAGS=\"$CLIENT_JVMFLAGS -Djava.security.auth.login.config={{zk_client_jaas_file}} -Dzookeeper.sasl.client.username={{zk_principal_user}}\"\n{% endif %}",
                    "zk_log_dir":"/var/log/zookeeper",
                    "zk_pid_dir":"/var/run/zookeeper",
                    "zk_server_heapsize":"1024",
                    "zookeeper_keytab_path":null,
                    "zookeeper_principal_name":null,
                    "zk_user":"zookeeper"
                }
            }
        },
        {
            "zookeeper-log4j":{
                "properties_attributes":{

                },
                "properties":{
                    "content":"\n#\n#\n# Licensed to the Apache Software Foundation (ASF) under one\n# or more contributor license agreements.  See the NOTICE file\n# distributed with this work for additional information\n# regarding copyright ownership.  The ASF licenses this file\n# to you under the Apache License, Version 2.0 (the\n# \"License\"); you may not use this file except in compliance\n# with the License.  You may obtain a copy of the License at\n#\n#   http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing,\n# software distributed under the License is distributed on an\n# \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n# KIND, either express or implied.  See the License for the\n# specific language governing permissions and limitations\n# under the License.\n#\n#\n#\n\n#\n# ZooKeeper Logging Configuration\n#\n\n# DEFAULT: console appender only\nlog4j.rootLogger=INFO, CONSOLE, ROLLINGFILE\n\n# Example with rolling log file\n#log4j.rootLogger=DEBUG, CONSOLE, ROLLINGFILE\n\n# Example with rolling log file and tracing\n#log4j.rootLogger=TRACE, CONSOLE, ROLLINGFILE, TRACEFILE\n\n#\n# Log INFO level and above messages to the console\n#\nlog4j.appender.CONSOLE=org.apache.log4j.ConsoleAppender\nlog4j.appender.CONSOLE.Threshold=INFO\nlog4j.appender.CONSOLE.layout=org.apache.log4j.PatternLayout\nlog4j.appender.CONSOLE.layout.ConversionPattern=%d{ISO8601} - %-5p [%t:%C{1}@%L] - %m%n\n\n#\n# Add ROLLINGFILE to rootLogger to get log file output\n#    Log DEBUG level and above messages to a log file\nlog4j.appender.ROLLINGFILE=org.apache.log4j.RollingFileAppender\nlog4j.appender.ROLLINGFILE.Threshold=DEBUG\nlog4j.appender.ROLLINGFILE.File={{zk_log_dir}}/zookeeper.log\n\n# Max log file size of 10MB\nlog4j.appender.ROLLINGFILE.MaxFileSize={{zookeeper_log_max_backup_size}}MB\n# uncomment the next line to limit number of backup files\n#log4j.appender.ROLLINGFILE.MaxBackupIndex={{zookeeper_log_number_of_backup_files}}\n\nlog4j.appender.ROLLINGFILE.layout=org.apache.log4j.PatternLayout\nlog4j.appender.ROLLINGFILE.layout.ConversionPattern=%d{ISO8601} - %-5p [%t:%C{1}@%L] - %m%n\n\n\n#\n# Add TRACEFILE to rootLogger to get log file output\n#    Log DEBUG level and above messages to a log file\nlog4j.appender.TRACEFILE=org.apache.log4j.FileAppender\nlog4j.appender.TRACEFILE.Threshold=TRACE\nlog4j.appender.TRACEFILE.File=zookeeper_trace.log\n\nlog4j.appender.TRACEFILE.layout=org.apache.log4j.PatternLayout\n### Notice we are including log4j's NDC here (%x)\nlog4j.appender.TRACEFILE.layout.ConversionPattern=%d{ISO8601} - %-5p [%t:%C{1}@%L][%x] - %m%n",
                    "zookeeper_log_max_backup_size":"10",
                    "zookeeper_log_number_of_backup_files":"10"
                }
            }
        },
        {
            "ams-env":{
                "properties_attributes":{

                },
                "properties":{
                    "ams_classpath_additional":"",
                    "content":"\n# Set environment variables here.\n\n# AMS instance name\nexport AMS_INSTANCE_NAME={{hostname}}\n\n# The java implementation to use. Java 1.6 required.\nexport JAVA_HOME={{java64_home}}\n\n# Collector Log directory for log4j\nexport AMS_COLLECTOR_LOG_DIR={{ams_collector_log_dir}}\n\n# Monitor Log directory for outfile\nexport AMS_MONITOR_LOG_DIR={{ams_monitor_log_dir}}\n\n# Collector pid directory\nexport AMS_COLLECTOR_PID_DIR={{ams_collector_pid_dir}}\n\n# Monitor pid directory\nexport AMS_MONITOR_PID_DIR={{ams_monitor_pid_dir}}\n\n# AMS HBase pid directory\nexport AMS_HBASE_PID_DIR={{hbase_pid_dir}}\n\n# AMS Collector heapsize\nexport AMS_COLLECTOR_HEAPSIZE={{metrics_collector_heapsize}}\n\n# HBase Tables Initialization check enabled\nexport AMS_HBASE_INIT_CHECK_ENABLED={{ams_hbase_init_check_enabled}}\n\n# AMS Collector options\nexport AMS_COLLECTOR_OPTS=\"-Djava.library.path=/usr/lib/ams-hbase/lib/hadoop-native\"\n{% if security_enabled %}\nexport AMS_COLLECTOR_OPTS=\"$AMS_COLLECTOR_OPTS -Dzookeeper.sasl.client.username={{zk_principal_user}} -Djava.security.auth.login.config={{ams_collector_jaas_config_file}}\"\n{% endif %}\n\n# AMS Collector GC options\nexport AMS_COLLECTOR_GC_OPTS=\"-XX:+UseConcMarkSweepGC -verbose:gc -XX:+PrintGCDetails -XX:+PrintGCDateStamps -Xloggc:{{ams_collector_log_dir}}/collector-gc.log-`date +'%Y%m%d%H%M'`\"\nexport AMS_COLLECTOR_OPTS=\"$AMS_COLLECTOR_OPTS $AMS_COLLECTOR_GC_OPTS\"\n\n# Metrics collector host will be blacklisted for specified number of seconds if metric monitor failed to connect to it.\nexport AMS_FAILOVER_STRATEGY_BLACKLISTED_INTERVAL={{failover_strategy_blacklisted_interval}}\n\n# Extra Java CLASSPATH elements for Metrics Collector. Optional.\nexport COLLECTOR_ADDITIONAL_CLASSPATH={{ams_classpath_additional}}",
                    "failover_strategy_blacklisted_interval":"300",
                    "metrics_collector_heapsize":"512",
                    "metrics_collector_log_dir":"/var/log/ambari-metrics-collector",
                    "metrics_collector_pid_dir":"/var/run/ambari-metrics-collector",
                    "metrics_monitor_log_dir":"/var/log/ambari-metrics-monitor",
                    "metrics_monitor_pid_dir":"/var/run/ambari-metrics-monitor",
                    "min_ambari_metrics_hadoop_sink_version":"3.0.0",
                    "timeline.metrics.host.inmemory.aggregation.jvm.arguments":"-Xmx256m -Xms128m -XX:PermSize=68m",
                    "timeline.metrics.skip.disk.metrics.patterns":"true",
                    "timeline.metrics.skip.network.interfaces.patterns":"None",
                    "timeline.metrics.skip.virtual.interfaces":"false",
                    "ambari_metrics_user":"ams"
                }
            }
        },
        {
            "ams-grafana-env":{
                "properties_attributes":{

                },
                "properties":{
                    "content":"\n# Set environment variables here.\n\n# AMS UI Server Home Dir\nexport AMS_GRAFANA_HOME_DIR={{ams_grafana_home_dir}}\n\n# AMS UI Server Data Dir\nexport AMS_GRAFANA_DATA_DIR={{ams_grafana_data_dir}}\n\n# AMS UI Server Log Dir\nexport AMS_GRAFANA_LOG_DIR={{ams_grafana_log_dir}}\n\n# AMS UI Server PID Dir\nexport AMS_GRAFANA_PID_DIR={{ams_grafana_pid_dir}}",
                    "metrics_grafana_connect_attempts":"15",
                    "metrics_grafana_connect_retry_delay":"20",
                    "metrics_grafana_data_dir":"/var/lib/ambari-metrics-grafana",
                    "metrics_grafana_log_dir":"/var/log/ambari-metrics-grafana",
                    "metrics_grafana_password":"admin",
                    "metrics_grafana_pid_dir":"/var/run/ambari-metrics-grafana",
                    "metrics_grafana_username":"admin"
                }
            }
        },
        {
            "ams-grafana-ini":{
                "properties_attributes":{

                },
                "properties":{
                    "ca_cert":"",
                    "cert_file":"/etc/ambari-metrics-grafana/conf/ams-grafana.crt",
                    "cert_key":"/etc/ambari-metrics-grafana/conf/ams-grafana.key",
                    "content":"\n##################### Grafana Configuration Example #####################\n#\n# Everything has defaults so you only need to uncomment things you want to\n# change\n\n# possible values : production, development\n; app_mode = production\n\n# instance name, defaults to HOSTNAME environment variable value or hostname if HOSTNAME var is empty\n; instance_name = ${HOSTNAME}\n\n#################################### Paths ####################################\n[paths]\n# Path to where grafana can store temp files, sessions, and the sqlite3 db (if that is used)\n#\n;data = /var/lib/grafana\ndata = {{ams_grafana_data_dir}}\n#\n# Directory where grafana can store logs\n#\n;logs = /var/log/grafana\nlogs = {{ams_grafana_log_dir}}\n#\n# Directory where grafana will automatically scan and look for plugins\n#\nplugins = /var/lib/ambari-metrics-grafana/plugins\n\n#################################### Server ####################################\n[server]\n# Protocol (http or https)\nprotocol = {{ams_grafana_protocol}}\n\n# The ip address to bind to, empty will bind to all interfaces\n;http_addr =\n\n# The http port  to use\nhttp_port = {{ams_grafana_port}}\n\n# The public facing domain name used to access grafana from a browser\n;domain = localhost\n\n# Redirect to correct domain if host header does not match domain\n# Prevents DNS rebinding attacks\n;enforce_domain = false\n\n# The full public facing url\n;root_url = %(protocol)s://%(domain)s:%(http_port)s/\n\n# Log web requests\n;router_logging = false\n\n# the path relative working path\nstatic_root_path = /usr/lib/ambari-metrics-grafana/public\n\n# enable gzip\n;enable_gzip = false\n\n# https certs & key file\ncert_file = {{ams_grafana_cert_file}}\ncert_key = {{ams_grafana_cert_key}}\n\n# Unix socket path\n;socket =\n#################################### Database ####################################\n[database]\n# Either \"mysql\", \"postgres\" or \"sqlite3\", it's your choice\n;type = sqlite3\n;host = 127.0.0.1:3306\n;name = grafana\n;user = root\n# If the password contains # or ; you have to wrap it with triple quotes. Ex \"\"\"#password;\"\"\"\n;password =\n\n# Use either URL or the previous fields to configure the database\n# Example: mysql://user:secret@host:port/database\n;url =\n\n# For \"postgres\" only, either \"disable\", \"require\" or \"verify-full\"\n;ssl_mode = disable\n\n# For \"sqlite3\" only, path relative to data_path setting\n;path = grafana.db\n\n#################################### Session ####################################\n[session]\n# Either \"memory\", \"file\", \"redis\", \"mysql\", \"postgres\", default is \"file\"\n;provider = file\n\n# Provider config options\n# memory: not have any config yet\n# file: session dir path, is relative to grafana data_path\n# redis: config like redis server e.g. `addr=127.0.0.1:6379,pool_size=100,db=grafana`\n# mysql: go-sql-driver/mysql dsn config string, e.g. `user:password@tcp(127.0.0.1:3306)/database_name`\n# postgres: user=a password=b host=localhost port=5432 dbname=c sslmode=disable\n;provider_config = sessions\n\n# Session cookie name\n;cookie_name = grafana_sess\n\n# If you use session in https only, default is false\n;cookie_secure = false\n\n# Session life time, default is 86400\n;session_life_time = 86400\n\n#################################### Data proxy ###########################\n[dataproxy]\n\n# This enables data proxy logging, default is false\n;logging = false\n\n#################################### Analytics ####################################\n[analytics]\n# Server reporting, sends usage counters to stats.grafana.org every 24 hours.\n# No ip addresses are being tracked, only simple counters to track\n# running instances, dashboard and error counts. It is very helpful to us.\n# Change this option to false to disable reporting.\n;reporting_enabled = true\n\n# Set to false to disable all checks to https://grafana.net\n# for new versions (grafana itself and plugins), check is used\n# in some UI views to notify that grafana or plugin update exists\n# This option does not cause any auto updates, nor send any information\n# only a GET request to http://grafana.com to get latest versions\n;check_for_updates = true\n\n# Google Analytics universal tracking code, only enabled if you specify an id here\n;google_analytics_ua_id =\n\n#################################### Security ####################################\n[security]\n# default admin user, created on startup\nadmin_user = {{ams_grafana_admin_user}}\n\n# default admin password, can be changed before first start of grafana,  or in profile settings\n;admin_password =\n\n# used for signing\n;secret_key = SW2YcwTIb9zpOOhoPsMm\n\n# Auto-login remember days\n;login_remember_days = 7\n;cookie_username = grafana_user\n;cookie_remember_name = grafana_remember\n\n# disable gravatar profile images\n;disable_gravatar = false\n\n# data source proxy whitelist (ip_or_domain:port seperated by spaces)\n;data_source_proxy_whitelist =\n\n[snapshots]\n# snapshot sharing options\n;external_enabled = true\n;external_snapshot_url = https://snapshots-origin.raintank.io\n;external_snapshot_name = Publish to snapshot.raintank.io\n\n# remove expired snapshot\n;snapshot_remove_expired = true\n\n# remove snapshots after 90 days\n;snapshot_TTL_days = 90\n\n#################################### Users ####################################\n[users]\n# disable user signup / registration\n;allow_sign_up = true\n\n# Allow non admin users to create organizations\n;allow_org_create = true\n\n# Set to true to automatically assign new users to the default organization (id 1)\n;auto_assign_org = true\n\n# Default role new users will be automatically assigned (if disabled above is set to true)\n;auto_assign_org_role = Viewer\n\n# Background text for the user field on the login page\n;login_hint = email or username\n\n# Default UI theme (\"dark\" or \"light\")\n;default_theme = dark\n\n# External user management, these options affect the organization users view\n;external_manage_link_url =\n;external_manage_link_name =\n;external_manage_info =\n\n[auth]\n# Set to true to disable (hide) the login form, useful if you use OAuth, defaults to false\n;disable_login_form = false\n\n# Set to true to disable the sign out link in the side menu. useful if you use auth.proxy, defaults to false\n;disable_signout_menu = false\n\n#################################### Anonymous Auth ##########################\n[auth.anonymous]\n# enable anonymous access\nenabled = true\n\n# specify organization name that should be used for unauthenticated users\norg_name = Main Org.\n\n# specify role for unauthenticated users\n;org_role = Admin\n\n#################################### Github Auth ##########################\n[auth.github]\n;enabled = false\n;allow_sign_up = false\n;client_id = some_id\n;client_secret = some_secret\n;scopes = user:email,read:org\n;auth_url = https://github.com/login/oauth/authorize\n;token_url = https://github.com/login/oauth/access_token\n;api_url = https://api.github.com/user\n;team_ids =\n;allowed_organizations =\n\n#################################### Google Auth ##########################\n[auth.google]\n;enabled = false\n;allow_sign_up = false\n;client_id = some_client_id\n;client_secret = some_client_secret\n;scopes = https://www.googleapis.com/auth/userinfo.profile https://www.googleapis.com/auth/userinfo.email\n;auth_url = https://accounts.google.com/o/oauth2/auth\n;token_url = https://accounts.google.com/o/oauth2/token\n;api_url = https://www.googleapis.com/oauth2/v1/userinfo\n;allowed_domains =\n\n#################################### Generic OAuth ##########################\n[auth.generic_oauth]\n;enabled = false\n;name = OAuth\n;allow_sign_up = true\n;client_id = some_id\n;client_secret = some_secret\n;scopes = user:email,read:org\n;auth_url = https://foo.bar/login/oauth/authorize\n;token_url = https://foo.bar/login/oauth/access_token\n;api_url = https://foo.bar/user\n;team_ids =\n;allowed_organizations =\n\n#################################### Grafana.com Auth ####################\n[auth.grafana_com]\n;enabled = false\n;allow_sign_up = true\n;client_id = some_id\n;client_secret = some_secret\n;scopes = user:email\n;allowed_organizations =\n\n#################################### Auth Proxy ##########################\n[auth.proxy]\n;enabled = false\n;header_name = X-WEBAUTH-USER\n;header_property = username\n;auto_sign_up = true\n\n#################################### Basic Auth ##########################\n[auth.basic]\n;enabled = true\n\n#################################### Auth LDAP ##########################\n[auth.ldap]\n;enabled = false\n;config_file = /etc/grafana/ldap.toml\n\n#################################### SMTP / Emailing ##########################\n[smtp]\n;enabled = false\n;host = localhost:25\n;user =\n;password =\n;cert_file =\n;key_file =\n;skip_verify = false\n;from_address = admin@grafana.localhost\n;from_name = Grafana\n# EHLO identity in SMTP dialog (defaults to instance_name)\n;ehlo_identity = dashboard.example.com\n\n[emails]\n;welcome_email_on_sign_up = false\n\n#################################### Logging ##########################\n[log]\n# Either \"console\", \"file\", default is \"console\"\n# Use comma to separate multiple modes, e.g. \"console, file\"\n;mode = console, file\n\n# Either \"debug\", \"info\", \"warn\", \"error\", \"critical\", default is \"info\"\n;level = info\n\n# optional settings to set different levels for specific loggers. Ex filters = sqlstore:debug\n;filters =\n\n# For \"console\" mode only\n[log.console]\n;level =\n\n# For \"file\" mode only\n[log.file]\n;level =\n\n# This enables automated log rotate(switch of following options), default is true\n;log_rotate = true\n\n# Max line number of single file, default is 1000000\n;max_lines = 1000000\n\n# Max size shift of single file, default is 28 means 1 << 28, 256MB\n;max_lines_shift = 28\n\n# Segment log daily, default is true\n;daily_rotate = true\n\n# Expired days of log file(delete after max days), default is 7\n;max_days = 7\n\n#################################### AMPQ Event Publisher ##########################\n[event_publisher]\n;enabled = false\n;rabbitmq_url = amqp://localhost/\n;exchange = grafana_events\n\n;#################################### Dashboard JSON files ##########################\n[dashboards.json]\n;enabled = false\n;path = /var/lib/grafana/dashboards\npath = /usr/lib/ambari-metrics-grafana/public/dashboards",
                    "port":"3000",
                    "protocol":"http"
                }
            }
        },
        {
            "ams-hbase-env":{
                "properties_attributes":{

                },
                "properties":{
                    "content":"\n# Set environment variables here.\n\n# The java implementation to use. Java 1.6+ required.\nexport JAVA_HOME={{java64_home}}\n\n# HBase Configuration directory\nexport HBASE_CONF_DIR=${HBASE_CONF_DIR:-{{hbase_conf_dir}}}\n\n# Extra Java CLASSPATH elements. Optional.\nadditional_cp={{hbase_classpath_additional}}\nif [  -n \"$additional_cp\" ];\nthen\n  export HBASE_CLASSPATH=${HBASE_CLASSPATH}:$additional_cp\nelse\n  export HBASE_CLASSPATH=${HBASE_CLASSPATH}\nfi\n\n# The maximum amount of heap to use for hbase shell.\nexport HBASE_SHELL_OPTS=\"-Xmx256m\"\n\n# Extra Java runtime options.\n# Below are what we set by default. May only work with SUN JVM.\n# For more on why as well as other possible settings,\n# see http://wiki.apache.org/hadoop/PerformanceTuning\nexport HBASE_OPTS=\"-XX:+UseConcMarkSweepGC -XX:ErrorFile={{hbase_log_dir}}/hs_err_pid%p.log -Djava.io.tmpdir={{hbase_tmp_dir}}\"\nexport SERVER_GC_OPTS=\"-verbose:gc -XX:+PrintGCDetails -XX:+PrintGCDateStamps -Xloggc:{{hbase_log_dir}}/gc.log-`date +'%Y%m%d%H%M'`\"\n# Uncomment below to enable java garbage collection logging.\n# export HBASE_OPTS=\"$HBASE_OPTS -verbose:gc -XX:+PrintGCDetails -XX:+PrintGCDateStamps -Xloggc:$HBASE_HOME/logs/gc-hbase.log\"\n\n# Uncomment and adjust to enable JMX exporting\n# See jmxremote.password and jmxremote.access in $JRE_HOME/lib/management to configure remote password access.\n# More details at: http://java.sun.com/javase/6/docs/technotes/guides/management/agent.html\n#\n# export HBASE_JMX_BASE=\"-Dcom.sun.management.jmxremote.ssl=false -Dcom.sun.management.jmxremote.authenticate=false\"\n\n{% if java_version < 8 %}\nexport HBASE_MASTER_OPTS=\" -XX:PermSize=64m -XX:MaxPermSize={{hbase_master_maxperm_size}} -Xms{{hbase_heapsize}} -Xmx{{hbase_heapsize}} -Xmn{{hbase_master_xmn_size}} -XX:CMSInitiatingOccupancyFraction=70 -XX:+UseCMSInitiatingOccupancyOnly\"\nexport HBASE_REGIONSERVER_OPTS=\"-XX:MaxPermSize=128m -Xmn{{regionserver_xmn_size}} -XX:CMSInitiatingOccupancyFraction=70 -XX:+UseCMSInitiatingOccupancyOnly -Xms{{regionserver_heapsize}} -Xmx{{regionserver_heapsize}}\"\n{% else %}\nexport HBASE_MASTER_OPTS=\" -Xms{{hbase_heapsize}} -Xmx{{hbase_heapsize}} -Xmn{{hbase_master_xmn_size}} -XX:CMSInitiatingOccupancyFraction=70 -XX:+UseCMSInitiatingOccupancyOnly\"\nexport HBASE_REGIONSERVER_OPTS=\" -Xmn{{regionserver_xmn_size}} -XX:CMSInitiatingOccupancyFraction=70 -XX:+UseCMSInitiatingOccupancyOnly -Xms{{regionserver_heapsize}} -Xmx{{regionserver_heapsize}}\"\n{% endif %}\n\n\n# export HBASE_THRIFT_OPTS=\"$HBASE_JMX_BASE -Dcom.sun.management.jmxremote.port=10103\"\n# export HBASE_ZOOKEEPER_OPTS=\"$HBASE_JMX_BASE -Dcom.sun.management.jmxremote.port=10104\"\n\n# File naming hosts on which HRegionServers will run. $HBASE_HOME/conf/regionservers by default.\nexport HBASE_REGIONSERVERS=${HBASE_CONF_DIR}/regionservers\n\n# Extra ssh options. Empty by default.\n# export HBASE_SSH_OPTS=\"-o ConnectTimeout=1 -o SendEnv=HBASE_CONF_DIR\"\n\n# Where log files are stored. $HBASE_HOME/logs by default.\nexport HBASE_LOG_DIR={{hbase_log_dir}}\n\n# A string representing this instance of hbase. $USER by default.\n# export HBASE_IDENT_STRING=$USER\n\n# The scheduling priority for daemon processes. See 'man nice'.\n# export HBASE_NICENESS=10\n\n# The directory where pid files are stored. /tmp by default.\nexport HBASE_PID_DIR={{hbase_pid_dir}}\n\n# Seconds to sleep between slave commands. Unset by default. This\n# can be useful in large clusters, where, e.g., slave rsyncs can\n# otherwise arrive faster than the master can service them.\n# export HBASE_SLAVE_SLEEP=0.1\n\n# Tell HBase whether it should manage it's own instance of Zookeeper or not.\nexport HBASE_MANAGES_ZK=false\n\n{% if security_enabled %}\nexport HBASE_OPTS=\"$HBASE_OPTS -Dzookeeper.sasl.client.username={{zk_principal_user}} -Djava.security.auth.login.config={{client_jaas_config_file}}\"\nexport HBASE_MASTER_OPTS=\"$HBASE_MASTER_OPTS -Dzookeeper.sasl.client.username={{zk_principal_user}} -Djava.security.auth.login.config={{master_jaas_config_file}} -Djavax.security.auth.useSubjectCredsOnly=false\"\nexport HBASE_REGIONSERVER_OPTS=\"$HBASE_REGIONSERVER_OPTS -Dzookeeper.sasl.client.username={{zk_principal_user}} -Djava.security.auth.login.config={{regionserver_jaas_config_file}} -Djavax.security.auth.useSubjectCredsOnly=false\"\nexport HBASE_ZOOKEEPER_OPTS=\"$HBASE_ZOOKEEPER_OPTS -Dzookeeper.sasl.client.username={{zk_principal_user}} -Djava.security.auth.login.config={{ams_zookeeper_jaas_config_file}}\"\n{% endif %}\n\n# use embedded native libs\n_HADOOP_NATIVE_LIB=\"/usr/lib/ams-hbase/lib/hadoop-native/\"\nexport HBASE_OPTS=\"$HBASE_OPTS -Djava.library.path=${_HADOOP_NATIVE_LIB}\"\n\n# Unset HADOOP_HOME to avoid importing HADOOP installed cluster related configs like: /usr/hdp/2.2.0.0-2041/hadoop/conf/\nexport HADOOP_HOME={{ams_hbase_home_dir}}\n\n# Explicitly Setting HBASE_HOME for AMS HBase so that there is no conflict\nexport HBASE_HOME={{ams_hbase_home_dir}}",
                    "hbase_classpath_additional":"",
                    "hbase_log_dir":"/var/log/ambari-metrics-collector",
                    "hbase_master_heapsize":"512",
                    "hbase_master_maxperm_size":"128",
                    "hbase_master_xmn_size":"102",
                    "hbase_pid_dir":"/var/run/ambari-metrics-collector",
                    "hbase_regionserver_heapsize":"768",
                    "hbase_regionserver_shutdown_timeout":"30",
                    "hbase_regionserver_xmn_ratio":"0.2",
                    "max_open_files_limit":"32768",
                    "regionserver_xmn_size":"128"
                }
            }
        },
        {
            "ams-hbase-log4j":{
                "properties_attributes":{

                },
                "properties":{
                    "ams_hbase_log_maxbackupindex":"20",
                    "ams_hbase_log_maxfilesize":"256",
                    "ams_hbase_security_log_maxbackupindex":"20",
                    "ams_hbase_security_log_maxfilesize":"256",
                    "content":"\n# Licensed to the Apache Software Foundation (ASF) under one\n# or more contributor license agreements.  See the NOTICE file\n# distributed with this work for additional information\n# regarding copyright ownership.  The ASF licenses this file\n# to you under the Apache License, Version 2.0 (the\n# \"License\"); you may not use this file except in compliance\n# with the License.  You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n\n# Define some default values that can be overridden by system properties\nhbase.root.logger=INFO,console\nhbase.security.logger=INFO,console\nhbase.log.dir=.\nhbase.log.file=hbase.log\n\n# Define the root logger to the system property \"hbase.root.logger\".\nlog4j.rootLogger=${hbase.root.logger}\n\n# Logging Threshold\nlog4j.threshold=ALL\n\n#\n# Daily Rolling File Appender\n#\nlog4j.appender.DRFA=org.apache.log4j.DailyRollingFileAppender\nlog4j.appender.DRFA.File=${hbase.log.dir}/${hbase.log.file}\n\n# Rollver at midnight\nlog4j.appender.DRFA.DatePattern=.yyyy-MM-dd\n\n# 30-day backup\n#log4j.appender.DRFA.MaxBackupIndex=30\nlog4j.appender.DRFA.layout=org.apache.log4j.PatternLayout\n\n# Pattern format: Date LogLevel LoggerName LogMessage\nlog4j.appender.DRFA.layout.ConversionPattern=%d{ISO8601} %-5p [%t] %c{2}: %m%n\n\n# Rolling File Appender properties\nhbase.log.maxfilesize={{ams_hbase_log_maxfilesize}}MB\nhbase.log.maxbackupindex={{ams_hbase_log_maxbackupindex}}\n\n# Rolling File Appender\nlog4j.appender.RFA=org.apache.log4j.RollingFileAppender\nlog4j.appender.RFA.File=${hbase.log.dir}/${hbase.log.file}\n\nlog4j.appender.RFA.MaxFileSize=${hbase.log.maxfilesize}\nlog4j.appender.RFA.MaxBackupIndex=${hbase.log.maxbackupindex}\n\nlog4j.appender.RFA.layout=org.apache.log4j.PatternLayout\nlog4j.appender.RFA.layout.ConversionPattern=%d{ISO8601} %-5p [%t] %c{2}: %m%n\n\n#\n# Security audit appender\n#\nhbase.security.log.file=SecurityAuth.audit\nhbase.security.log.maxfilesize={{ams_hbase_security_log_maxfilesize}}MB\nhbase.security.log.maxbackupindex={{ams_hbase_security_log_maxbackupindex}}\nlog4j.appender.RFAS=org.apache.log4j.RollingFileAppender\nlog4j.appender.RFAS.File=${hbase.log.dir}/${hbase.security.log.file}\nlog4j.appender.RFAS.MaxFileSize=${hbase.security.log.maxfilesize}\nlog4j.appender.RFAS.MaxBackupIndex=${hbase.security.log.maxbackupindex}\nlog4j.appender.RFAS.layout=org.apache.log4j.PatternLayout\nlog4j.appender.RFAS.layout.ConversionPattern=%d{ISO8601} %p %c: %m%n\nlog4j.category.SecurityLogger=${hbase.security.logger}\nlog4j.additivity.SecurityLogger=false\n#log4j.logger.SecurityLogger.org.apache.hadoop.hbase.security.access.AccessController=TRACE\n\n#\n# Null Appender\n#\nlog4j.appender.NullAppender=org.apache.log4j.varia.NullAppender\n\n#\n# console\n# Add \"console\" to rootlogger above if you want to use this\n#\nlog4j.appender.console=org.apache.log4j.ConsoleAppender\nlog4j.appender.console.target=System.err\nlog4j.appender.console.layout=org.apache.log4j.PatternLayout\nlog4j.appender.console.layout.ConversionPattern=%d{ISO8601} %-5p [%t] %c{2}: %m%n\n\n# Custom Logging levels\n\nlog4j.logger.org.apache.zookeeper=INFO\n#log4j.logger.org.apache.hadoop.fs.FSNamesystem=DEBUG\nlog4j.logger.org.apache.hadoop.hbase=INFO\n# Make these two classes INFO-level. Make them DEBUG to see more zk debug.\nlog4j.logger.org.apache.hadoop.hbase.zookeeper.ZKUtil=INFO\nlog4j.logger.org.apache.hadoop.hbase.zookeeper.ZooKeeperWatcher=INFO\n#log4j.logger.org.apache.hadoop.dfs=DEBUG\n# Set this class to log INFO only otherwise its OTT\n# Enable this to get detailed connection error/retry logging.\n# log4j.logger.org.apache.hadoop.hbase.client.HConnectionManager$HConnectionImplementation=TRACE\n\n\n# Uncomment this line to enable tracing on _every_ RPC call (this can be a lot of output)\n#log4j.logger.org.apache.hadoop.ipc.HBaseServer.trace=DEBUG\n\n# Uncomment the below if you want to remove logging of client region caching'\n# and scan of .META. messages\n# log4j.logger.org.apache.hadoop.hbase.client.HConnectionManager$HConnectionImplementation=INFO\n# log4j.logger.org.apache.hadoop.hbase.client.MetaScanner=INFO"
                }
            }
        },
        {
            "ams-hbase-policy":{
                "properties_attributes":{

                },
                "properties":{
                    "security.admin.protocol.acl":"*",
                    "security.client.protocol.acl":"*",
                    "security.masterregion.protocol.acl":"*"
                }
            }
        },
        {
            "ams-hbase-security-site":{
                "properties_attributes":{

                },
                "properties":{
                    "ams.zookeeper.keytab":"",
                    "ams.zookeeper.principal":"",
                    "hadoop.security.authentication":"",
                    "hbase.coprocessor.master.classes":"",
                    "hbase.coprocessor.region.classes":"",
                    "hbase.master.kerberos.principal":"",
                    "hbase.master.keytab.file":"",
                    "hbase.myclient.keytab":"",
                    "hbase.myclient.principal":"",
                    "hbase.regionserver.kerberos.principal":"",
                    "hbase.regionserver.keytab.file":"",
                    "hbase.security.authentication":"",
                    "hbase.security.authorization":"",
                    "hbase.zookeeper.property.authProvider.1":"",
                    "hbase.zookeeper.property.jaasLoginRenew":"",
                    "hbase.zookeeper.property.kerberos.removeHostFromPrincipal":"",
                    "hbase.zookeeper.property.kerberos.removeRealmFromPrincipal":""
                }
            }
        },
        {
            "ams-hbase-site":{
                "properties_attributes":{

                },
                "properties":{
                    "dfs.client.read.shortcircuit":"true",
                    "hbase.client.scanner.caching":"10000",
                    "hbase.client.scanner.timeout.period":"300000",
                    "hbase.cluster.distributed":"true",
                    "hbase.hregion.majorcompaction":"0",
                    "hbase.hregion.max.filesize":"4294967296",
                    "hbase.hregion.memstore.block.multiplier":"4",
                    "hbase.hregion.memstore.flush.size":"134217728",
                    "hbase.hstore.blockingStoreFiles":"200",
                    "hbase.hstore.flusher.count":"2",
                    "hbase.local.dir":"${hbase.tmp.dir}/local",
                    "hbase.master.info.bindAddress":"0.0.0.0",
                    "hbase.master.info.port":"61310",
                    "hbase.master.normalizer.class":"org.apache.hadoop.hbase.master.normalizer.SimpleRegionNormalizer",
                    "hbase.master.port":"61300",
                    "hbase.master.wait.on.regionservers.mintostart":"1",
                    "hbase.normalizer.enabled":"false",
                    "hbase.normalizer.period":"600000",
                    "hbase.regionserver.global.memstore.lowerLimit":"0.3",
                    "hbase.regionserver.global.memstore.upperLimit":"0.35",
                    "hbase.regionserver.info.port":"61330",
                    "hbase.regionserver.port":"61320",
                    "hbase.regionserver.thread.compaction.large":"2",
                    "hbase.regionserver.thread.compaction.small":"3",
                    "hbase.replication":"false",
                    "hbase.rootdir":"hdfs://ambari-server:8020/user/ams/hbase",
                    "hbase.rpc.timeout":"300000",
                    "hbase.snapshot.enabled":"false",
                    "hbase.tmp.dir":"/var/lib/ambari-metrics-collector/hbase-tmp",
                    "hbase.unsafe.stream.capability.enforce":"false",
                    "hbase.zookeeper.leaderport":"61388",
                    "hbase.zookeeper.peerport":"61288",
                    "hbase.zookeeper.property.clientPort":"{{zookeeper_clientPort}}",
                    "hbase.zookeeper.property.dataDir":"${hbase.tmp.dir}/zookeeper",
                    "hbase.zookeeper.property.tickTime":"6000",
                    "hbase.zookeeper.quorum":"{{zookeeper_quorum_hosts}}",
                    "hfile.block.cache.size":"0.3",
                    "phoenix.coprocessor.maxMetaDataCacheSize":"20480000",
                    "phoenix.coprocessor.maxServerCacheTimeToLiveMs":"60000",
                    "phoenix.groupby.maxCacheSize":"307200000",
                    "phoenix.mutate.batchSize":"10000",
                    "phoenix.query.keepAliveMs":"300000",
                    "phoenix.query.maxGlobalMemoryPercentage":"15",
                    "phoenix.query.rowKeyOrderSaltedTable":"true",
                    "phoenix.query.spoolThresholdBytes":"20971520",
                    "phoenix.query.timeoutMs":"300000",
                    "phoenix.sequence.saltBuckets":"2",
                    "phoenix.spool.directory":"${hbase.tmp.dir}/phoenix-spool",
                    "zookeeper.session.timeout":"120000",
                    "zookeeper.session.timeout.localHBaseCluster":"120000",
                    "zookeeper.znode.parent":"/ams-hbase-unsecure"
                }
            }
        },
        {
            "ams-log4j":{
                "properties_attributes":{

                },
                "properties":{
                    "ams_log_max_backup_size":"80",
                    "ams_log_number_of_backup_files":"60",
                    "content":"\n#\n# Licensed to the Apache Software Foundation (ASF) under one\n# or more contributor license agreements.  See the NOTICE file\n# distributed with this work for additional information\n# regarding copyright ownership.  The ASF licenses this file\n# to you under the Apache License, Version 2.0 (the\n# \"License\"); you may not use this file except in compliance\n# with the License.  You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n#\n\n# Define some default values that can be overridden by system properties\nams.log.dir=.\nams.log.file=ambari-metrics-collector.log\n\n# Root logger option\nlog4j.rootLogger=INFO,file\n\n# Direct log messages to a log file\nlog4j.appender.file=org.apache.log4j.RollingFileAppender\nlog4j.appender.file.File=${ams.log.dir}/${ams.log.file}\nlog4j.appender.file.MaxFileSize={{ams_log_max_backup_size}}MB\nlog4j.appender.file.MaxBackupIndex={{ams_log_number_of_backup_files}}\nlog4j.appender.file.layout=org.apache.log4j.PatternLayout\nlog4j.appender.file.layout.ConversionPattern=%d{ISO8601} %p %c: %m%n"
                }
            }
        },
        {
            "ams-site":{
                "properties_attributes":{

                },
                "properties":{
                    "timeline.metrics.cluster.aggregate.splitpoints":"kafka.network.RequestMetrics.TotalTimeMs.request.JoinGroup.mean",
                    "timeline.metrics.service.handler.thread.count":"20",
                    "timeline.metrics.host.aggregate.splitpoints":"kafka.network.RequestMetrics.TotalTimeMs.request.JoinGroup.mean",
                    "cluster.zookeeper.property.clientPort":"{{cluster_zookeeper_clientPort}}",
                    "cluster.zookeeper.quorum":"{{cluster_zookeeper_quorum_hosts}}",
                    "failover.strategy":"round-robin",
                    "phoenix.query.maxGlobalMemoryPercentage":"25",
                    "phoenix.spool.directory":"/tmp",
                    "timeline.metrics.aggregator.checkpoint.dir":"/var/lib/ambari-metrics-collector/checkpoint",
                    "timeline.metrics.aggregators.skip.blockcache.enabled":"false",
                    "timeline.metrics.cache.commit.interval":"3",
                    "timeline.metrics.cache.enabled":"true",
                    "timeline.metrics.cache.size":"150",
                    "timeline.metrics.cluster.aggregation.sql.filters":"sdisk\\_%,boottime",
                    "timeline.metrics.cluster.aggregator.daily.checkpointCutOffMultiplier":"2",
                    "timeline.metrics.cluster.aggregator.daily.disabled":"false",
                    "timeline.metrics.cluster.aggregator.daily.interval":"86400",
                    "timeline.metrics.cluster.aggregator.daily.ttl":"63072000",
                    "timeline.metrics.cluster.aggregator.hourly.checkpointCutOffMultiplier":"2",
                    "timeline.metrics.cluster.aggregator.hourly.disabled":"false",
                    "timeline.metrics.cluster.aggregator.hourly.interval":"3600",
                    "timeline.metrics.cluster.aggregator.hourly.ttl":"31536000",
                    "timeline.metrics.cluster.aggregator.interpolation.enabled":"true",
                    "timeline.metrics.cluster.aggregator.minute.checkpointCutOffMultiplier":"2",
                    "timeline.metrics.cluster.aggregator.minute.disabled":"false",
                    "timeline.metrics.cluster.aggregator.minute.interval":"300",
                    "timeline.metrics.cluster.aggregator.minute.ttl":"2592000",
                    "timeline.metrics.cluster.aggregator.second.checkpointCutOffMultiplier":"2",
                    "timeline.metrics.cluster.aggregator.second.disabled":"false",
                    "timeline.metrics.cluster.aggregator.second.interval":"120",
                    "timeline.metrics.cluster.aggregator.second.timeslice.interval":"30",
                    "timeline.metrics.cluster.aggregator.second.ttl":"259200",
                    "timeline.metrics.daily.aggregator.minute.interval":"86400",
                    "timeline.metrics.downsampler.event.metric.patterns":"",
                    "timeline.metrics.downsampler.topn.function":"max",
                    "timeline.metrics.downsampler.topn.metric.patterns":"",
                    "timeline.metrics.downsampler.topn.value":"10",
                    "timeline.metrics.hbase.compression.scheme":"SNAPPY",
                    "timeline.metrics.hbase.data.block.encoding":"FAST_DIFF",
                    "timeline.metrics.hbase.init.check.enabled":"true",
                    "timeline.metrics.host.aggregator.daily.checkpointCutOffMultiplier":"2",
                    "timeline.metrics.host.aggregator.daily.disabled":"false",
                    "timeline.metrics.host.aggregator.daily.ttl":"31536000",
                    "timeline.metrics.host.aggregator.hourly.checkpointCutOffMultiplier":"2",
                    "timeline.metrics.host.aggregator.hourly.disabled":"false",
                    "timeline.metrics.host.aggregator.hourly.interval":"3600",
                    "timeline.metrics.host.aggregator.hourly.ttl":"2592000",
                    "timeline.metrics.host.aggregator.minute.checkpointCutOffMultiplier":"2",
                    "timeline.metrics.host.aggregator.minute.disabled":"false",
                    "timeline.metrics.host.aggregator.minute.interval":"300",
                    "timeline.metrics.host.aggregator.minute.ttl":"604800",
                    "timeline.metrics.host.aggregator.ttl":"86400",
                    "timeline.metrics.host.inmemory.aggregation":"false",
                    "timeline.metrics.host.inmemory.aggregation.http.policy":"HTTP_ONLY",
                    "timeline.metrics.host.inmemory.aggregation.port":"61888",
                    "timeline.metrics.service.checkpointDelay":"60",
                    "timeline.metrics.service.cluster.aggregator.appIds":"datanode,nodemanager,hbase",
                    "timeline.metrics.service.default.result.limit":"5760",
                    "timeline.metrics.service.http.policy":"HTTP_ONLY",
                    "timeline.metrics.service.metadata.filters":"ContainerResource",
                    "timeline.metrics.service.operation.mode":"distributed",
                    "timeline.metrics.service.resultset.fetchSize":"2000",
                    "timeline.metrics.service.rpc.address":"0.0.0.0:60200",
                    "timeline.metrics.service.use.groupBy.aggregators":"true",
                    "timeline.metrics.service.watcher.delay":"30",
                    "timeline.metrics.service.watcher.disabled":"true",
                    "timeline.metrics.service.watcher.initial.delay":"600",
                    "timeline.metrics.service.watcher.timeout":"30",
                    "timeline.metrics.service.webapp.address":"ambari-server:6188",
                    "timeline.metrics.sink.report.interval":"60",
                    "timeline.metrics.transient.metric.patterns":"topology\\.%,dfs.NNTopUserOpCounts.windowMs=60000.op=__%.user=%,dfs.NNTopUserOpCounts.windowMs=300000.op=__%.user=%,dfs.NNTopUserOpCounts.windowMs=1500000.op=__%.user=%",
                    "timeline.metrics.whitelisting.enabled":"false"
                }
            }
        },
        {
            "ams-ssl-client":{
                "properties_attributes":{

                },
                "properties":{
                    "ssl.client.truststore.location":"/etc/security/clientKeys/all.jks",
                    "ssl.client.truststore.password":"bigdata",
                    "ssl.client.truststore.type":"jks"
                }
            }
        },
        {
            "ams-ssl-server":{
                "properties_attributes":{

                },
                "properties":{
                    "ssl.server.keystore.keypassword":"bigdata",
                    "ssl.server.keystore.location":"/etc/security/serverKeys/keystore.jks",
                    "ssl.server.keystore.password":"bigdata",
                    "ssl.server.keystore.type":"jks",
                    "ssl.server.truststore.location":"/etc/security/serverKeys/all.jks",
                    "ssl.server.truststore.password":"bigdata",
                    "ssl.server.truststore.reload.interval":"10000",
                    "ssl.server.truststore.type":"jks"
                }
            }
        },
        {
            "kafka-broker":{
                "properties_attributes":{

                },
                "properties":{
                    "auto.create.topics.enable":"true",
                    "auto.leader.rebalance.enable":"true",
                    "compression.type":"producer",
                    "controlled.shutdown.enable":"true",
                    "controlled.shutdown.max.retries":"3",
                    "controlled.shutdown.retry.backoff.ms":"5000",
                    "controller.message.queue.size":"10",
                    "controller.socket.timeout.ms":"30000",
                    "default.replication.factor":"1",
                    "delete.topic.enable":"true",
                    "external.kafka.metrics.exclude.prefix":"kafka.network.RequestMetrics,kafka.server.DelayedOperationPurgatory,kafka.server.BrokerTopicMetrics.BytesRejectedPerSec",
                    "external.kafka.metrics.include.prefix":"kafka.network.RequestMetrics.ResponseQueueTimeMs.request.OffsetCommit.98percentile,kafka.network.RequestMetrics.ResponseQueueTimeMs.request.Offsets.95percentile,kafka.network.RequestMetrics.ResponseSendTimeMs.request.Fetch.95percentile,kafka.network.RequestMetrics.RequestsPerSec.request",
                    "fetch.purgatory.purge.interval.requests":"10000",
                    "kafka.ganglia.metrics.group":"kafka",
                    "kafka.ganglia.metrics.host":"localhost",
                    "kafka.ganglia.metrics.port":"8671",
                    "kafka.ganglia.metrics.reporter.enabled":"true",
                    "kafka.metrics.reporters":"{{metrics_reporters}}",
                    "kafka.timeline.metrics.host_in_memory_aggregation":"{{host_in_memory_aggregation}}",
                    "kafka.timeline.metrics.host_in_memory_aggregation_port":"{{host_in_memory_aggregation_port}}",
                    "kafka.timeline.metrics.host_in_memory_aggregation_protocol":"{{host_in_memory_aggregation_protocol}}",
                    "kafka.timeline.metrics.hosts":"{{ams_collector_hosts}}",
                    "kafka.timeline.metrics.maxRowCacheSize":"10000",
                    "kafka.timeline.metrics.port":"{{metric_collector_port}}",
                    "kafka.timeline.metrics.protocol":"{{metric_collector_protocol}}",
                    "kafka.timeline.metrics.reporter.enabled":"true",
                    "kafka.timeline.metrics.reporter.sendInterval":"5900",
                    "kafka.timeline.metrics.truststore.password":"{{metric_truststore_password}}",
                    "kafka.timeline.metrics.truststore.path":"{{metric_truststore_path}}",
                    "kafka.timeline.metrics.truststore.type":"{{metric_truststore_type}}",
                    "leader.imbalance.check.interval.seconds":"300",
                    "leader.imbalance.per.broker.percentage":"10",
                    "listeners":"PLAINTEXT://localhost:9092",
                    "log.cleanup.interval.mins":"10",
                    "log.dirs":"/kafka-logs",
                    "log.index.interval.bytes":"4096",
                    "log.index.size.max.bytes":"10485760",
                    "log.retention.bytes":"-1",
                    "log.retention.check.interval.ms":"600000",
                    "log.retention.hours":"168",
                    "log.roll.hours":"168",
                    "log.segment.bytes":"1073741824",
                    "message.max.bytes":"1000000",
                    "min.insync.replicas":"1",
                    "num.io.threads":"8",
                    "num.network.threads":"3",
                    "num.partitions":"1",
                    "num.recovery.threads.per.data.dir":"1",
                    "num.replica.fetchers":"1",
                    "offset.metadata.max.bytes":"4096",
                    "offsets.commit.required.acks":"-1",
                    "offsets.commit.timeout.ms":"5000",
                    "offsets.load.buffer.size":"5242880",
                    "offsets.retention.check.interval.ms":"600000",
                    "offsets.retention.minutes":"86400000",
                    "offsets.topic.compression.codec":"0",
                    "offsets.topic.num.partitions":"50",
                    "offsets.topic.replication.factor":"1",
                    "offsets.topic.segment.bytes":"104857600",
                    "producer.metrics.enable":"false",
                    "producer.purgatory.purge.interval.requests":"10000",
                    "queued.max.requests":"500",
                    "raw.listeners":"",
                    "replica.fetch.max.bytes":"1048576",
                    "replica.fetch.min.bytes":"1",
                    "replica.fetch.wait.max.ms":"500",
                    "replica.high.watermark.checkpoint.interval.ms":"5000",
                    "replica.lag.max.messages":"4000",
                    "replica.lag.time.max.ms":"10000",
                    "replica.socket.receive.buffer.bytes":"65536",
                    "replica.socket.timeout.ms":"30000",
                    "sasl.enabled.mechanisms":"GSSAPI",
                    "sasl.mechanism.inter.broker.protocol":"GSSAPI",
                    "security.inter.broker.protocol":"PLAINTEXT",
                    "socket.receive.buffer.bytes":"102400",
                    "socket.request.max.bytes":"104857600",
                    "socket.send.buffer.bytes":"102400",
                    "ssl.client.auth":"none",
                    "ssl.key.password":"",
                    "ssl.keystore.location":"",
                    "ssl.keystore.password":"",
                    "ssl.truststore.location":"",
                    "ssl.truststore.password":"",
                    "zookeeper.connect":"ambari-server:2181",
                    "zookeeper.connection.timeout.ms":"25000",
                    "zookeeper.session.timeout.ms":"30000",
                    "zookeeper.sync.time.ms":"2000"
                }
            }
        },
        {
            "kafka-env":{
                "properties_attributes":{

                },
                "properties":{
                    "content":"\n  #!/bin/bash\n  \n  # Set KAFKA specific environment variables here.\n  \n  # The java implementation to use.\n  export JAVA_HOME={{java64_home}}\n  export PATH=$PATH:$JAVA_HOME/bin\n  export PID_DIR={{kafka_pid_dir}}\n  export LOG_DIR={{kafka_log_dir}}\n  {% if kerberos_security_enabled or kafka_other_sasl_enabled %}\n  export KAFKA_OPTS=\"-Djavax.security.auth.useSubjectCredsOnly=false {{kafka_kerberos_params}}\"\n  {% else %}\n  export KAFKA_OPTS={{kafka_kerberos_params}}\n  {% endif %}\n  # Add kafka sink to classpath and related depenencies\n  if [ -e \"/usr/lib/ambari-metrics-kafka-sink/ambari-metrics-kafka-sink.jar\" ]; then\n    export CLASSPATH=$CLASSPATH:/usr/lib/ambari-metrics-kafka-sink/ambari-metrics-kafka-sink.jar\n    export CLASSPATH=$CLASSPATH:/usr/lib/ambari-metrics-kafka-sink/lib/*\n  fi\n  {% if stack_supports_kafka_env_include_ranger_script %}\n  if [ -f /etc/kafka/conf/kafka-ranger-env.sh ]; then\n  . /etc/kafka/conf/kafka-ranger-env.sh\n  fi\n  {% else %}\n        export CLASSPATH=$CLASSPATH:{{conf_dir}}\n  {% endif %}",
                    "kafka_keytab":null,
                    "kafka_log_dir":"/var/log/kafka",
                    "kafka_pid_dir":"/var/run/kafka",
                    "kafka_principal_name":null,
                    "kafka_user_nofile_limit":"128000",
                    "kafka_user_nproc_limit":"65536",
                    "kerberos_merge_advertised_listeners":"true",
                    "kafka_user":"kafka"
                }
            }
        },
        {
            "kafka-log4j":{
                "properties_attributes":{

                },
                "properties":{
                    "content":"\n #\n #\n # Licensed to the Apache Software Foundation (ASF) under one\n # or more contributor license agreements.  See the NOTICE file\n # distributed with this work for additional information\n # regarding copyright ownership.  The ASF licenses this file\n # to you under the Apache License, Version 2.0 (the\n # \"License\"); you may not use this file except in compliance\n # with the License.  You may obtain a copy of the License at\n #\n #   http://www.apache.org/licenses/LICENSE-2.0\n #\n # Unless required by applicable law or agreed to in writing,\n # software distributed under the License is distributed on an\n # \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n # KIND, either express or implied.  See the License for the\n # specific language governing permissions and limitations\n # under the License.\n #\n #\n #\n kafka.logs.dir=logs\n \n log4j.rootLogger=INFO, stdout\n \n log4j.appender.stdout=org.apache.log4j.ConsoleAppender\n log4j.appender.stdout.layout=org.apache.log4j.PatternLayout\n log4j.appender.stdout.layout.ConversionPattern=[%d{ISO8601}] %p %m (%c)%n\n \n log4j.appender.kafkaAppender=org.apache.log4j.DailyRollingFileAppender\n log4j.appender.kafkaAppender.DatePattern='.'yyyy-MM-dd-HH\n log4j.appender.kafkaAppender.File=${kafka.logs.dir}/server.log\n log4j.appender.kafkaAppender.layout=org.apache.log4j.PatternLayout\n log4j.appender.kafkaAppender.layout.ConversionPattern=[%d{ISO8601}] %p %m (%c)%n\n log4j.appender.kafkaAppender.MaxFileSize = {{kafka_log_maxfilesize}}MB\n log4j.appender.kafkaAppender.MaxBackupIndex = {{kafka_log_maxbackupindex}}\n \n log4j.appender.stateChangeAppender=org.apache.log4j.DailyRollingFileAppender\n log4j.appender.stateChangeAppender.DatePattern='.'yyyy-MM-dd-HH\n log4j.appender.stateChangeAppender.File=${kafka.logs.dir}/state-change.log\n log4j.appender.stateChangeAppender.layout=org.apache.log4j.PatternLayout\n log4j.appender.stateChangeAppender.layout.ConversionPattern=[%d{ISO8601}] %p %m (%c)%n\n \n log4j.appender.requestAppender=org.apache.log4j.DailyRollingFileAppender\n log4j.appender.requestAppender.DatePattern='.'yyyy-MM-dd-HH\n log4j.appender.requestAppender.File=${kafka.logs.dir}/kafka-request.log\n log4j.appender.requestAppender.layout=org.apache.log4j.PatternLayout\n log4j.appender.requestAppender.layout.ConversionPattern=[%d{ISO8601}] %p %m (%c)%n\n \n log4j.appender.cleanerAppender=org.apache.log4j.DailyRollingFileAppender\n log4j.appender.cleanerAppender.DatePattern='.'yyyy-MM-dd-HH\n log4j.appender.cleanerAppender.File=${kafka.logs.dir}/log-cleaner.log\n log4j.appender.cleanerAppender.layout=org.apache.log4j.PatternLayout\n log4j.appender.cleanerAppender.layout.ConversionPattern=[%d{ISO8601}] %p %m (%c)%n\n \n log4j.appender.controllerAppender=org.apache.log4j.DailyRollingFileAppender\n log4j.appender.controllerAppender.DatePattern='.'yyyy-MM-dd-HH\n log4j.appender.controllerAppender.File=${kafka.logs.dir}/controller.log\n log4j.appender.controllerAppender.layout=org.apache.log4j.PatternLayout\n log4j.appender.controllerAppender.layout.ConversionPattern=[%d{ISO8601}] %p %m (%c)%n\n log4j.appender.controllerAppender.MaxFileSize = {{controller_log_maxfilesize}}MB\n log4j.appender.controllerAppender.MaxBackupIndex = {{controller_log_maxbackupindex}}\n # Turn on all our debugging info\n #log4j.logger.kafka.producer.async.DefaultEventHandler=DEBUG, kafkaAppender\n #log4j.logger.kafka.client.ClientUtils=DEBUG, kafkaAppender\n #log4j.logger.kafka.perf=DEBUG, kafkaAppender\n #log4j.logger.kafka.perf.ProducerPerformance$ProducerThread=DEBUG, kafkaAppender\n #log4j.logger.org.I0Itec.zkclient.ZkClient=DEBUG\n log4j.logger.kafka=INFO, kafkaAppender\n log4j.logger.kafka.network.RequestChannel$=WARN, requestAppender\n log4j.additivity.kafka.network.RequestChannel$=false\n \n #log4j.logger.kafka.network.Processor=TRACE, requestAppender\n #log4j.logger.kafka.server.KafkaApis=TRACE, requestAppender\n #log4j.additivity.kafka.server.KafkaApis=false\n log4j.logger.kafka.request.logger=WARN, requestAppender\n log4j.additivity.kafka.request.logger=false\n \n log4j.logger.kafka.controller=TRACE, controllerAppender\n log4j.additivity.kafka.controller=false\n \n log4j.logger.kafka.log.LogCleaner=INFO, cleanerAppender\n log4j.additivity.kafka.log.LogCleaner=false\n \n log4j.logger.state.change.logger=TRACE, stateChangeAppender\n log4j.additivity.state.change.logger=false",
                    "controller_log_maxbackupindex":"20",
                    "controller_log_maxfilesize":"256",
                    "kafka_log_maxbackupindex":"20",
                    "kafka_log_maxfilesize":"256"
                }
            }
        },
        {
            "kafka_client_jaas_conf":{
                "properties_attributes":{

                },
                "properties":{
                    "content":"\n  {% if kerberos_security_enabled %}\n  KafkaClient {\n  com.sun.security.auth.module.Krb5LoginModule required\n  useTicketCache=true\n  renewTicket=true\n  serviceName=\"{{kafka_bare_jaas_principal}}\";\n  };\n  {% endif %}"
                }
            }
        },
        {
            "kafka_jaas_conf":{
                "properties_attributes":{

                },
                "properties":{
                    "content":"\n          /**\n          * Example of SASL/PLAIN Configuration\n          *\n          * KafkaServer {\n          *   org.apache.kafka.common.security.plain.PlainLoginModule required\n          *   username=\"admin\"\n          *   password=\"admin-secret\"\n          *   user_admin=\"admin-secret\"\n          *   user_alice=\"alice-secret\";\n          *   };\n          *\n          * Example of SASL/SCRAM\n          *\n          * KafkaServer {\n          *   org.apache.kafka.common.security.scram.ScramLoginModule required\n          *   username=\"admin\"\n          *   password=\"admin-secret\"\n          *   };\n          *\n          * Example of Enabling multiple SASL mechanisms in a broker:\n          *\n          *   KafkaServer {\n          *\n          *    com.sun.security.auth.module.Krb5LoginModule required\n          *    useKeyTab=true\n          *    storeKey=true\n          *    keyTab=\"/etc/security/keytabs/kafka_server.keytab\"\n          *    principal=\"kafka/kafka1.hostname.com@EXAMPLE.COM\";\n          *\n          *    org.apache.kafka.common.security.plain.PlainLoginModule required\n          *    username=\"admin\"\n          *    password=\"admin-secret\"\n          *    user_admin=\"admin-secret\"\n          *    user_alice=\"alice-secret\";\n          *\n          *    org.apache.kafka.common.security.scram.ScramLoginModule required\n          *    username=\"scram-admin\"\n          *    password=\"scram-admin-secret\";\n          *    };\n          *\n          **/\n  \n          {% if kerberos_security_enabled %}\n  \n          KafkaServer {\n          com.sun.security.auth.module.Krb5LoginModule required\n          useKeyTab=true\n          keyTab=\"{{kafka_keytab_path}}\"\n          storeKey=true\n          useTicketCache=false\n          serviceName=\"{{kafka_bare_jaas_principal}}\"\n          principal=\"{{kafka_jaas_principal}}\";\n          };\n          KafkaClient {\n          com.sun.security.auth.module.Krb5LoginModule required\n          useTicketCache=true\n          renewTicket=true\n          serviceName=\"{{kafka_bare_jaas_principal}}\";\n          };\n          Client {\n          com.sun.security.auth.module.Krb5LoginModule required\n          useKeyTab=true\n          keyTab=\"{{kafka_keytab_path}}\"\n          storeKey=true\n          useTicketCache=false\n          serviceName=\"zookeeper\"\n          principal=\"{{kafka_jaas_principal}}\";\n          };\n          com.sun.security.jgss.krb5.initiate {\n          com.sun.security.auth.module.Krb5LoginModule required\n          renewTGT=false\n          doNotPrompt=true\n          useKeyTab=true\n          keyTab=\"{{kafka_keytab_path}}\"\n          storeKey=true\n          useTicketCache=false\n          serviceName=\"{{kafka_bare_jaas_principal}}\"\n          principal=\"{{kafka_jaas_principal}}\";\n          };\n  \n          {% endif %}"
                }
            }
        },
        {
            "spark-defaults":{
                "properties_attributes":{

                },
                "properties":{
                    "spark.eventLog.dir":"hdfs:///spark-history/",
                    "spark.eventLog.enabled":"true",
                    "spark.executor.extraJavaOptions":"-XX:+UseNUMA",
                    "spark.hadoop.cacheConf":"false",
                    "spark.history.fs.cleaner.enabled":"true",
                    "spark.history.fs.cleaner.interval":"7d",
                    "spark.history.fs.cleaner.maxAge":"90d",
                    "spark.history.fs.logDirectory":"hdfs:///spark-history/",
                    "spark.history.kerberos.keytab":"none",
                    "spark.history.kerberos.principal":"none",
                    "spark.history.provider":"org.apache.spark.deploy.history.FsHistoryProvider",
                    "spark.history.store.path":"/var/lib/spark/shs_db",
                    "spark.history.ui.port":"18081",
                    "spark.io.compression.lz4.blockSize":"128kb",
                    "spark.master":"yarn",
                    "spark.scheduler.allocation.file":"file:///{{spark_conf_dir}}/spark-thrift-fairscheduler.xml",
                    "spark.scheduler.mode":"FAIR",
                    "spark.shuffle.file.buffer":"1m",
                    "spark.shuffle.io.backLog":"8192",
                    "spark.sql.autoBroadcastJoinThreshold":"10MB",
                    "spark.sql.hive.convertMetastoreOrc":"true",
                    "spark.sql.hive.metastore.jars":"{{hive_home}}/lib/*",
                    "spark.sql.hive.metastore.version":"3.1.3",
                    "spark.sql.orc.filterPushdown":"true",
                    "spark.sql.statistics.fallBackToHdfs":"true",
                    "spark.sql.warehouse.dir":"{{spark_warehouse_dir}}",
                    "spark.yarn.executor.failuresValidityInterval":"2h",
                    "spark.yarn.historyServer.address":"{{spark_history_server_host}}:{{spark_history_ui_port}}",
                    "spark.yarn.maxAppAttempts":"1",
                    "spark.yarn.queue":"default"
                }
            }
        },
        {
            "spark-env":{
                "properties_attributes":{

                },
                "properties":{
                    "content":"\n#!/usr/bin/env bash\n\n# This file is sourced when running various Spark programs.\n# Copy it as spark-env.sh and edit that to configure Spark for your site.\n\n# Options read in YARN client mode\n#SPARK_EXECUTOR_INSTANCES=\"2\" #Number of workers to start (Default: 2)\n#SPARK_EXECUTOR_CORES=\"1\" #Number of cores for the workers (Default: 1).\n#SPARK_EXECUTOR_MEMORY=\"1G\" #Memory per Worker (e.g. 1000M, 2G) (Default: 1G)\n#SPARK_DRIVER_MEMORY=\"512M\" #Memory for Master (e.g. 1000M, 2G) (Default: 512 Mb)\n#SPARK_YARN_APP_NAME=\"spark\" #The name of your application (Default: Spark)\n#SPARK_YARN_QUEUE=\"default\" #The hadoop queue to use for allocation requests (Default: default)\n#SPARK_YARN_DIST_FILES=\"\" #Comma separated list of files to be distributed with the job.\n#SPARK_YARN_DIST_ARCHIVES=\"\" #Comma separated list of archives to be distributed with the job.\n\n{% if security_enabled %}\nexport SPARK_HISTORY_OPTS='-Dspark.ui.filters=org.apache.hadoop.security.authentication.server.AuthenticationFilter -Dspark.org.apache.hadoop.security.authentication.server.AuthenticationFilter.params=\"type=kerberos,kerberos.principal={{spnego_principal}},kerberos.keytab={{spnego_keytab}}\"'\n{% endif %}\n\n\n# Generic options for the daemons used in the standalone deploy mode\n\n# Alternate conf dir. (Default: ${SPARK_HOME}/conf)\nexport SPARK_CONF_DIR=${SPARK_CONF_DIR:-{{spark_home}}/conf}\n\n# Where log files are stored.(Default:${SPARK_HOME}/logs)\n#export SPARK_LOG_DIR=${SPARK_HOME:-{{spark_home}}}/logs\nexport SPARK_LOG_DIR={{spark_log_dir}}\n\n# Where the pid file is stored. (Default: /tmp)\nexport SPARK_PID_DIR={{spark_pid_dir}}\n\n#Memory for Master, Worker and history server (default: 1024MB)\nexport SPARK_DAEMON_MEMORY={{spark_daemon_memory}}m\n\n# A string representing this instance of spark.(Default: $USER)\nSPARK_IDENT_STRING=$USER\n\n# The scheduling priority for daemons. (Default: 0)\nSPARK_NICENESS=0\n\nexport HADOOP_HOME=${HADOOP_HOME:-{{hadoop_home}}}\nexport HADOOP_CONF_DIR=${HADOOP_CONF_DIR:-{{hadoop_conf_dir}}}\n\nexport SPARK_DIST_CLASSPATH=$(${HADOOP_HOME}/bin/hadoop classpath)\n\n# The java implementation to use.\nexport JAVA_HOME={{java_home}}",
                    "hive_kerberos_keytab":"{{hive_kerberos_keytab}}",
                    "hive_kerberos_principal":"{{hive_kerberos_principal}}",
                    "spark_daemon_memory":"2048",
                    "spark_log_dir":"/var/log/spark",
                    "spark_pid_dir":"/var/run/spark",
                    "spark_thrift_cmd_opts":"",
                    "spark_group":"spark",
                    "spark_user":"spark"
                }
            }
        },
        {
            "spark-hive-site-override":{
                "properties_attributes":{

                },
                "properties":{
                    "hive.exec.scratchdir":"/tmp/spark",
                    "hive.load.data.owner":"spark",
                    "hive.metastore.client.connect.retry.delay":"5",
                    "hive.metastore.client.socket.timeout":"1800",
                    "hive.server2.enable.doAs":"false",
                    "hive.server2.thrift.http.port":"10002",
                    "hive.server2.thrift.port":"10016",
                    "hive.server2.transport.mode":"binary",
                    "metastore.catalog.default":"hive"
                }
            }
        },
        {
            "spark-log4j-properties":{
                "properties_attributes":{

                },
                "properties":{
                    "content":"\n# Set everything to be logged to the console\nlog4j.rootCategory=INFO, console\nlog4j.appender.console=org.apache.log4j.ConsoleAppender\nlog4j.appender.console.target=System.err\nlog4j.appender.console.layout=org.apache.log4j.PatternLayout\nlog4j.appender.console.layout.ConversionPattern=%d{yy/MM/dd HH:mm:ss} %p %c{1}: %m%n\n\n# Settings to quiet third party logs that are too verbose\nlog4j.logger.org.eclipse.jetty=WARN\nlog4j.logger.org.eclipse.jetty.util.component.AbstractLifeCycle=ERROR\nlog4j.logger.org.apache.spark.repl.SparkIMain$exprTyper=INFO\nlog4j.logger.org.apache.spark.repl.SparkILoop$SparkILoopInterpreter=INFO"
                }
            }
        },
        {
            "spark-metrics-properties":{
                "properties_attributes":{

                },
                "properties":{
                    "content":"\n# syntax: [instance].sink|source.[name].[options]=[value]\n\n# This file configures Spark's internal metrics system. The metrics system is\n# divided into instances which correspond to internal components.\n# Each instance can be configured to report its metrics to one or more sinks.\n# Accepted values for [instance] are \"master\", \"worker\", \"executor\", \"driver\",\n# and \"applications\". A wild card \"*\" can be used as an instance name, in\n# which case all instances will inherit the supplied property.\n#\n# Within an instance, a \"source\" specifies a particular set of grouped metrics.\n# there are two kinds of sources:\n# 1. Spark internal sources, like MasterSource, WorkerSource, etc, which will\n# collect a Spark component's internal state. Each instance is paired with a\n# Spark source that is added automatically.\n# 2. Common sources, like JvmSource, which will collect low level state.\n# These can be added through configuration options and are then loaded\n# using reflection.\n#\n# A \"sink\" specifies where metrics are delivered to. Each instance can be\n# assigned one or more sinks.\n#\n# The sink|source field specifies whether the property relates to a sink or\n# source.\n#\n# The [name] field specifies the name of source or sink.\n#\n# The [options] field is the specific property of this source or sink. The\n# source or sink is responsible for parsing this property.\n#\n# Notes:\n# 1. To add a new sink, set the \"class\" option to a fully qualified class\n# name (see examples below).\n# 2. Some sinks involve a polling period. The minimum allowed polling period\n# is 1 second.\n# 3. Wild card properties can be overridden by more specific properties.\n# For example, master.sink.console.period takes precedence over\n# *.sink.console.period.\n# 4. A metrics specific configuration\n# \"spark.metrics.conf=${SPARK_HOME}/conf/metrics.properties\" should be\n# added to Java properties using -Dspark.metrics.conf=xxx if you want to\n# customize metrics system. You can also put the file in ${SPARK_HOME}/conf\n# and it will be loaded automatically.\n# 5. MetricsServlet is added by default as a sink in master, worker and client\n# driver, you can send http request \"/metrics/json\" to get a snapshot of all the\n# registered metrics in json format. For master, requests \"/metrics/master/json\" and\n# \"/metrics/applications/json\" can be sent seperately to get metrics snapshot of\n# instance master and applications. MetricsServlet may not be configured by self.\n#\n\n## List of available sinks and their properties.\n\n# org.apache.spark.metrics.sink.ConsoleSink\n# Name: Default: Description:\n# period 10 Poll period\n# unit seconds Units of poll period\n\n# org.apache.spark.metrics.sink.CSVSink\n# Name: Default: Description:\n# period 10 Poll period\n# unit seconds Units of poll period\n# directory /tmp Where to store CSV files\n\n# org.apache.spark.metrics.sink.GangliaSink\n# Name: Default: Description:\n# host NONE Hostname or multicast group of Ganglia server\n# port NONE Port of Ganglia server(s)\n# period 10 Poll period\n# unit seconds Units of poll period\n# ttl 1 TTL of messages sent by Ganglia\n# mode multicast Ganglia network mode ('unicast' or 'multicast')\n\n# org.apache.spark.metrics.sink.JmxSink\n\n# org.apache.spark.metrics.sink.MetricsServlet\n# Name: Default: Description:\n# path VARIES* Path prefix from the web server root\n# sample false Whether to show entire set of samples for histograms ('false' or 'true')\n#\n# * Default path is /metrics/json for all instances except the master. The master has two paths:\n# /metrics/aplications/json # App information\n# /metrics/master/json # Master information\n\n# org.apache.spark.metrics.sink.GraphiteSink\n# Name: Default: Description:\n# host NONE Hostname of Graphite server\n# port NONE Port of Graphite server\n# period 10 Poll period\n# unit seconds Units of poll period\n# prefix EMPTY STRING Prefix to prepend to metric name\n\n## Examples\n# Enable JmxSink for all instances by class name\n#*.sink.jmx.class=org.apache.spark.metrics.sink.JmxSink\n\n# Enable ConsoleSink for all instances by class name\n#*.sink.console.class=org.apache.spark.metrics.sink.ConsoleSink\n\n# Polling period for ConsoleSink\n#*.sink.console.period=10\n\n#*.sink.console.unit=seconds\n\n# Master instance overlap polling period\n#master.sink.console.period=15\n\n#master.sink.console.unit=seconds\n\n# Enable CsvSink for all instances\n#*.sink.csv.class=org.apache.spark.metrics.sink.CsvSink\n\n# Polling period for CsvSink\n#*.sink.csv.period=1\n\n#*.sink.csv.unit=minutes\n\n# Polling directory for CsvSink\n#*.sink.csv.directory=/tmp/\n\n# Worker instance overlap polling period\n#worker.sink.csv.period=10\n\n#worker.sink.csv.unit=minutes\n\n# Enable jvm source for instance master, worker, driver and executor\n#master.source.jvm.class=org.apache.spark.metrics.source.JvmSource\n\n#worker.source.jvm.class=org.apache.spark.metrics.source.JvmSource\n\n#driver.source.jvm.class=org.apache.spark.metrics.source.JvmSource\n\n#executor.source.jvm.class=org.apache.spark.metrics.source.JvmSource"
                }
            }
        },
        {
            "spark-thrift-fairscheduler":{
                "properties_attributes":{

                },
                "properties":{
                    "fairscheduler_content":"<?xml version=\"1.0\"?>\n<allocations>\n    <pool name=\"default\">\n        <schedulingMode>FAIR</schedulingMode>\n        <weight>1</weight>\n        <minShare>2</minShare>\n    </pool>\n</allocations>"
                }
            }
        },
        {
            "zeppelin-env":{
                "properties_attributes":{

                },
                "properties":{
                    "hbase_conf_dir":"/etc/hbase/conf",
                    "hbase_home":"/usr/lib/hbase",
                    "spark_home":"/usr/lib/spark",
                    "zeppelin.executor.instances":null,
                    "zeppelin.executor.mem":null,
                    "zeppelin.server.kerberos.keytab":null,
                    "zeppelin.server.kerberos.principal":null,
                    "zeppelin.spark.jar.dir":null,
                    "zeppelin_env_content":"\n      # export JAVA_HOME=\n      export JAVA_HOME={{java64_home}}\n      # export MASTER=                              # Spark master url. eg. spark://master_addr:7077. Leave empty if you want to use local mode.\n      export MASTER=yarn-client\n\n      # export ZEPPELIN_JAVA_OPTS                   # Additional jvm options. for example, export ZEPPELIN_JAVA_OPTS=\"-Dspark.executor.memory=8g -Dspark.cores.max=16\"\n      # export ZEPPELIN_MEM                         # Zeppelin jvm mem options Default -Xms1024m -Xmx1024m -XX:MaxPermSize=512m\n      # export ZEPPELIN_INTP_MEM                    # zeppelin interpreter process jvm mem options. Default -Xms1024m -Xmx1024m -XX:MaxPermSize=512m\n      # export ZEPPELIN_INTP_JAVA_OPTS              # zeppelin interpreter process jvm options.\n      # export ZEPPELIN_SSL_PORT                    # ssl port (used when ssl environment variable is set to true)\n\n      # export ZEPPELIN_LOG_DIR                     # Where log files are stored.  PWD by default.\n      export ZEPPELIN_LOG_DIR={{zeppelin_log_dir}}\n      # export ZEPPELIN_PID_DIR                     # The pid files are stored. ${ZEPPELIN_HOME}/run by default.\n      export ZEPPELIN_PID_DIR={{zeppelin_pid_dir}}\n      # export ZEPPELIN_WAR_TEMPDIR                 # The location of jetty temporary directory.\n      export ZEPPELIN_WAR_TEMPDIR={{zeppelin_war_tempdir}}\n      # export ZEPPELIN_NOTEBOOK_DIR                # Where notebook saved\n      export ZEPPELIN_NOTEBOOK_DIR={{zeppelin_notebook_dir}}\n      # export ZEPPELIN_NOTEBOOK_HOMESCREEN         # Id of notebook to be displayed in homescreen. ex) 2A94M5J1Z\n      # export ZEPPELIN_NOTEBOOK_HOMESCREEN_HIDE    # hide homescreen notebook from list when this value set to \"true\". default \"false\"\n      # export ZEPPELIN_NOTEBOOK_S3_BUCKET          # Bucket where notebook saved\n      # export ZEPPELIN_NOTEBOOK_S3_ENDPOINT        # Endpoint of the bucket\n      # export ZEPPELIN_NOTEBOOK_S3_USER            # User in bucket where notebook saved. For example bucket/user/notebook/2A94M5J1Z/note.json\n      # export ZEPPELIN_IDENT_STRING                # A string representing this instance of zeppelin. $USER by default.\n      # export ZEPPELIN_NICENESS                    # The scheduling priority for daemons. Defaults to 0.\n      # export ZEPPELIN_INTERPRETER_LOCALREPO       # Local repository for interpreter's additional dependency loading\n      # export ZEPPELIN_NOTEBOOK_STORAGE            # Refers to pluggable notebook storage class, can have two classes simultaneously with a sync between them (e.g. local and remote).\n      # export ZEPPELIN_NOTEBOOK_ONE_WAY_SYNC       # If there are multiple notebook storages, should we treat the first one as the only source of truth?\n      # export ZEPPELIN_NOTEBOOK_PUBLIC             # Make notebook public by default when created, private otherwise\n      export ZEPPELIN_INTP_CLASSPATH_OVERRIDES=\"{{external_dependency_conf}}\"\n      #### Spark interpreter configuration ####\n\n      ## Kerberos ticket refresh setting\n      ##\n      export KINIT_FAIL_THRESHOLD=5\n      export KERBEROS_REFRESH_INTERVAL=1d\n\n      ## Use provided spark installation ##\n      ## defining SPARK_HOME makes Zeppelin run spark interpreter process using spark-submit\n      ##\n      # export SPARK_HOME                           # (required) When it is defined, load it instead of Zeppelin embedded Spark libraries\n      export SPARK_HOME={{spark_home}}\n      # export SPARK_SUBMIT_OPTIONS                 # (optional) extra options to pass to spark submit. eg) \"--driver-memory 512M --executor-memory 1G\".\n      # export SPARK_APP_NAME                       # (optional) The name of spark application.\n\n      ## Use embedded spark binaries ##\n      ## without SPARK_HOME defined, Zeppelin still able to run spark interpreter process using embedded spark binaries.\n      ## however, it is not encouraged when you can define SPARK_HOME\n      ##\n      # Options read in YARN client mode\n      # export HADOOP_CONF_DIR                      # yarn-site.xml is located in configuration directory in HADOOP_CONF_DIR.\n      export HADOOP_CONF_DIR=/etc/hadoop/conf\n      # Pyspark (supported with Spark 1.2.1 and above)\n      # To configure pyspark, you need to set spark distribution's path to 'spark.home' property in Interpreter setting screen in Zeppelin GUI\n      # export PYSPARK_PYTHON                       # path to the python command. must be the same path on the driver(Zeppelin) and all workers.\n      # export PYTHONPATH\n\n      ## Spark interpreter options ##\n      ##\n      # export ZEPPELIN_SPARK_USEHIVECONTEXT        # Use HiveContext instead of SQLContext if set true. true by default.\n      # export ZEPPELIN_SPARK_CONCURRENTSQL         # Execute multiple SQL concurrently if set true. false by default.\n      # export ZEPPELIN_SPARK_IMPORTIMPLICIT        # Import implicits, UDF collection, and sql if set true. true by default.\n      # export ZEPPELIN_SPARK_MAXRESULT             # Max number of Spark SQL result to display. 1000 by default.\n      # export ZEPPELIN_WEBSOCKET_MAX_TEXT_MESSAGE_SIZE       # Size in characters of the maximum text message to be received by websocket. Defaults to 1024000\n\n\n      #### HBase interpreter configuration ####\n\n      ## To connect to HBase running on a cluster, either HBASE_HOME or HBASE_CONF_DIR must be set\n\n      # export HBASE_HOME=                          # (require) Under which HBase scripts and configuration should be\n      # export HBASE_CONF_DIR=                      # (optional) Alternatively, configuration directory can be set to point to the directory that has hbase-site.xml\n      export HBASE_HOME={{hbase_home}}\n      export HBASE_CONF_DIR={{hbase_conf_dir}}\n\n      # export ZEPPELIN_IMPERSONATE_CMD             # Optional, when user want to run interpreter as end web user. eg) 'sudo -H -u ${ZEPPELIN_IMPERSONATE_USER} bash -c '",
                    "zeppelin_log_dir":"/var/log/zeppelin",
                    "zeppelin_notebook_dir":"/user/zeppelin/notebook",
                    "zeppelin_pid_dir":"/var/run/zeppelin",
                    "zeppelin_war_tempdir":"/var/run/zeppelin/webapps",
                    "zeppelin_group":"zeppelin",
                    "zeppelin_user":"zeppelin"
                }
            }
        },
        {
            "zeppelin-log4j-properties":{
                "properties_attributes":{

                },
                "properties":{
                    "log4j_properties_content":"\nlog4j.rootLogger = INFO, dailyfile\nlog4j.appender.stdout = org.apache.log4j.ConsoleAppender\nlog4j.appender.stdout.layout = org.apache.log4j.PatternLayout\nlog4j.appender.stdout.layout.ConversionPattern=%5p [%d{ISO8601}] ({%t} %F[%M]:%L) - %m%n\nlog4j.appender.dailyfile.DatePattern=.yyyy-MM-dd\nlog4j.appender.dailyfile.Threshold = INFO\nlog4j.appender.dailyfile = org.apache.log4j.DailyRollingFileAppender\nlog4j.appender.dailyfile.File = ${zeppelin.log.file}\nlog4j.appender.dailyfile.layout = org.apache.log4j.PatternLayout\nlog4j.appender.dailyfile.layout.ConversionPattern=%5p [%d{ISO8601}] ({%t} %F[%M]:%L) - %m%n"
                }
            }
        },
        {
            "zeppelin-shiro-ini":{
                "properties_attributes":{

                },
                "properties":{
                    "shiro_ini_content":"\n[users]\n# List of users with their password allowed to access Zeppelin.\n# To use a different strategy (LDAP / Database / ...) check the shiro doc at http://shiro.apache.org/configuration.html#Configuration-INISections\nadmin = $shiro1$SHA-256$500000$p6Be9+t2hdUXJQj2D0b1fg==$bea5JIMqcVF3J6eNZGWQ/3eeDByn5iEZDuGsEip06+M=, admin\nuser1 = $shiro1$SHA-256$500000$G2ymy/qmuZnGY6or4v2KfA==$v9fabqWgCNCgechtOUqAQenGDs0OSLP28q2wolPT4wU=, role1, role2\nuser2 = $shiro1$SHA-256$500000$aHBgiuwSgAcP3Xt5mEzeFw==$KosBnN2BNKA9/KHBL0hnU/woJFl+xzJFj12NQ0fnjCU=, role3\nuser3 = $shiro1$SHA-256$500000$nf0GzH10GbYVoxa7DOlOSw==$ov/IA5W8mRWPwvAoBjNYxg3udJK0EmrVMvFCwcr9eAs=, role2\n\n# Sample LDAP configuration, for user Authentication, currently tested for single Realm\n[main]\n### A sample for configuring Active Directory Realm\n#activeDirectoryRealm = org.apache.zeppelin.realm.ActiveDirectoryGroupRealm\n#activeDirectoryRealm.systemUsername = userNameA\n\n#use either systemPassword or hadoopSecurityCredentialPath, more details in http://zeppelin.apache.org/docs/latest/security/shiroauthentication.html\n#activeDirectoryRealm.systemPassword = passwordA\n#activeDirectoryRealm.hadoopSecurityCredentialPath = jceks://file/user/zeppelin/zeppelin.jceks\n#activeDirectoryRealm.searchBase = CN=Users,DC=SOME_GROUP,DC=COMPANY,DC=COM\n#activeDirectoryRealm.url = ldap://ldap.test.com:389\n#activeDirectoryRealm.groupRolesMap = \"CN=admin,OU=groups,DC=SOME_GROUP,DC=COMPANY,DC=COM\":\"admin\",\"CN=finance,OU=groups,DC=SOME_GROUP,DC=COMPANY,DC=COM\":\"finance\",\"CN=hr,OU=groups,DC=SOME_GROUP,DC=COMPANY,DC=COM\":\"hr\"\n#activeDirectoryRealm.authorizationCachingEnabled = false\n\n### A sample for configuring LDAP Directory Realm\n#ldapRealm = org.apache.zeppelin.realm.LdapGroupRealm\n## search base for ldap groups (only relevant for LdapGroupRealm):\n#ldapRealm.contextFactory.environment[ldap.searchBase] = dc=COMPANY,dc=COM\n#ldapRealm.contextFactory.url = ldap://ldap.test.com:389\n#ldapRealm.userDnTemplate = uid={0},ou=Users,dc=COMPANY,dc=COM\n#ldapRealm.contextFactory.authenticationMechanism = SIMPLE\n\n### A sample PAM configuration\n#pamRealm=org.apache.zeppelin.realm.PamRealm\n#pamRealm.service=sshd\n\n## To be commented out when not using [user] block / paintext\npasswordMatcher = org.apache.shiro.authc.credential.PasswordMatcher\niniRealm.credentialsMatcher = $passwordMatcher\n\nsessionManager = org.apache.shiro.web.session.mgt.DefaultWebSessionManager\n### If caching of user is required then uncomment below lines\ncacheManager = org.apache.shiro.cache.MemoryConstrainedCacheManager\nsecurityManager.cacheManager = $cacheManager\n\ncookie = org.apache.shiro.web.servlet.SimpleCookie\ncookie.name = JSESSIONID\n#Uncomment the line below when running Zeppelin-Server in HTTPS mode\n#cookie.secure = true\ncookie.httpOnly = true\nsessionManager.sessionIdCookie = $cookie\n\nsecurityManager.sessionManager = $sessionManager\n# 86,400,000 milliseconds = 24 hour\nsecurityManager.sessionManager.globalSessionTimeout = 86400000\nshiro.loginUrl = /api/login\n\n[roles]\nrole1 = *\nrole2 = *\nrole3 = *\nadmin = *\n\n[urls]\n# This section is used for url-based security.\n# You can secure interpreter, configuration and credential information by urls. Comment or uncomment the below urls that you want to hide.\n# anon means the access is anonymous.\n# authc means Form based Auth Security\n# To enfore security, comment the line below and uncomment the next one\n/api/version = anon\n#/api/interpreter/** = authc, roles[admin]\n#/api/configurations/** = authc, roles[admin]\n#/api/credential/** = authc, roles[admin]\n#/** = anon\n/** = authc"
                }
            }
        },
        {
            "zeppelin-site":{
                "properties_attributes":{

                },
                "properties":{
                    "zeppelin.anonymous.allowed":"false",
                    "zeppelin.config.fs.dir":"conf",
                    "zeppelin.config.storage.class":"org.apache.zeppelin.storage.FileSystemConfigStorage",
                    "zeppelin.interpreter.config.upgrade":"true",
                    "zeppelin.interpreter.connect.timeout":"30000",
                    "zeppelin.interpreter.dir":"interpreter",
                    "zeppelin.interpreter.group.order":"spark,angular,jdbc,livy,md,sh",
                    "zeppelin.interpreters":"org.apache.zeppelin.spark.SparkInterpreter,org.apache.zeppelin.spark.PySparkInterpreter,org.apache.zeppelin.spark.SparkSqlInterpreter,org.apache.zeppelin.spark.DepInterpreter,org.apache.zeppelin.markdown.Markdown,org.apache.zeppelin.angular.AngularInterpreter,org.apache.zeppelin.shell.ShellInterpreter,org.apache.zeppelin.jdbc.JDBCInterpreter,org.apache.zeppelin.phoenix.PhoenixInterpreter,org.apache.zeppelin.livy.LivySparkInterpreter,org.apache.zeppelin.livy.LivyPySparkInterpreter,org.apache.zeppelin.livy.LivySparkRInterpreter,org.apache.zeppelin.livy.LivySparkSQLInterpreter",
                    "zeppelin.notebook.dir":"notebook",
                    "zeppelin.notebook.homescreen":" ",
                    "zeppelin.notebook.homescreen.hide":"false",
                    "zeppelin.notebook.public":"false",
                    "zeppelin.notebook.s3.bucket":"zeppelin",
                    "zeppelin.notebook.s3.user":"user",
                    "zeppelin.notebook.storage":"org.apache.zeppelin.notebook.repo.FileSystemNotebookRepo",
                    "zeppelin.server.addr":"0.0.0.0",
                    "zeppelin.server.allowed.origins":"*",
                    "zeppelin.server.port":"9995",
                    "zeppelin.server.ssl.port":"9995",
                    "zeppelin.ssl":"false",
                    "zeppelin.ssl.client.auth":"false",
                    "zeppelin.ssl.key.manager.password":"change me",
                    "zeppelin.ssl.keystore.password":"change me",
                    "zeppelin.ssl.keystore.path":"conf/keystore",
                    "zeppelin.ssl.keystore.type":"JKS",
                    "zeppelin.ssl.truststore.password":"change me",
                    "zeppelin.ssl.truststore.path":"conf/truststore",
                    "zeppelin.ssl.truststore.type":"JKS",
                    "zeppelin.websocket.max.text.message.size":"1024000"
                }
            }
        },
        {
            "flink-conf":{
                "properties_attributes":{

                },
                "properties":{
                    "content":"\n################################################################################\n#  Licensed to the Apache Software Foundation (ASF) under one\n#  or more contributor license agreements.  See the NOTICE file\n#  distributed with this work for additional information\n#  regarding copyright ownership.  The ASF licenses this file\n#  to you under the Apache License, Version 2.0 (the\n#  \"License\"); you may not use this file except in compliance\n#  with the License.  You may obtain a copy of the License at\n#\n#      http://www.apache.org/licenses/LICENSE-2.0\n#\n#  Unless required by applicable law or agreed to in writing, software\n#  distributed under the License is distributed on an \"AS IS\" BASIS,\n#  WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n#  See the License for the specific language governing permissions and\n# limitations under the License.\n################################################################################\n\n\n#==============================================================================\n# Common\n#==============================================================================\n\n# The external address of the host on which the JobManager runs and can be\n# reached by the TaskManagers and any clients which want to connect. This setting\n# is only used in Standalone mode and may be overwritten on the JobManager side\n# by specifying the --host hostname parameter of the bin/jobmanager.sh executable.\n# In high availability mode, if you use the bin/start-cluster.sh script and setup\n# the conf/masters file, this will be taken care of automatically. Yarn\n# automatically configure the host name based on the hostname of the node where the\n# JobManager runs.\n\njobmanager.rpc.address: localhost\n\n# The RPC port where the JobManager is reachable.\n\njobmanager.rpc.port: 6123\n\n# The host interface the JobManager will bind to. My default, this is localhost, and will prevent\n# the JobManager from communicating outside the machine/container it is running on.\n# On YARN this setting will be ignored if it is set to 'localhost', defaulting to 0.0.0.0.\n# On Kubernetes this setting will be ignored, defaulting to 0.0.0.0.\n#\n# To enable this, set the bind-host address to one that has access to an outside facing network\n# interface, such as 0.0.0.0.\n\njobmanager.bind-host: localhost\n\n\n# The total process memory size for the JobManager.\n#\n# Note this accounts for all memory usage within the JobManager process, including JVM metaspace and other overhead.\n\njobmanager.memory.process.size: 1024m\n\n# The host interface the TaskManager will bind to. By default, this is localhost, and will prevent\n# the TaskManager from communicating outside the machine/container it is running on.\n# On YARN this setting will be ignored if it is set to 'localhost', defaulting to 0.0.0.0.\n# On Kubernetes this setting will be ignored, defaulting to 0.0.0.0.\n#\n# To enable this, set the bind-host address to one that has access to an outside facing network\n# interface, such as 0.0.0.0.\n\ntaskmanager.bind-host: localhost\n\n# The address of the host on which the TaskManager runs and can be reached by the JobManager and\n# other TaskManagers. If not specified, the TaskManager will try different strategies to identify\n# the address.\n#\n# Note this address needs to be reachable by the JobManager and forward traffic to one of\n# the interfaces the TaskManager is bound to (see 'taskmanager.bind-host').\n#\n# Note also that unless all TaskManagers are running on the same machine, this address needs to be\n# configured separately for each TaskManager.\n\ntaskmanager.host: localhost\n\n# The total process memory size for the TaskManager.\n#\n# Note this accounts for all memory usage within the TaskManager process, including JVM metaspace and other overhead.\n\ntaskmanager.memory.process.size: 1024m\n\n# To exclude JVM metaspace and overhead, please, use total Flink memory size instead of 'taskmanager.memory.process.size'.\n# It is not recommended to set both 'taskmanager.memory.process.size' and Flink memory.\n#\n# taskmanager.memory.flink.size: 1280m\n\n# The number of task slots that each TaskManager offers. Each slot runs one parallel pipeline.\n\ntaskmanager.numberOfTaskSlots: 1\n\n# The parallelism used for programs that did not specify and other parallelism.\n\nparallelism.default: 1\n\n# The default file system scheme and authority.\n#\n# By default file paths without scheme are interpreted relative to the local\n# root file system 'file:///'. Use this to override the default and interpret\n# relative paths relative to a different file system,\n# for example 'hdfs://mynamenode:12345'\n#\n# fs.default-scheme\n\n#==============================================================================\n# JVM and Logging Options\n#==============================================================================\n#Java runtime to use\nenv.java.home: {{java_home}}\n\n#Path to hadoop configuration directory. It is required to read HDFS and/or YARN configuration.\n#You can also set it via environment variable.\nenv.hadoop.conf.dir: {{hadoop_conf_dir}}\n\n#Defines the directory where the flink-<host>-<process>.pid files are saved.\nenv.pid.dir: {{flink_pid_dir}}\n\n#Defines the directory where the Flink logs are saved.\nenv.log.dir: {{flink_log_dir}}\n\n#==============================================================================\n# High Availability\n#==============================================================================\n\n# The high-availability mode. Possible options are 'NONE' or 'zookeeper'.\n#\n# high-availability: zookeeper\n\n# The path where metadata for master recovery is persisted. While ZooKeeper stores\n# the small ground truth for checkpoint and leader election, this location stores\n# the larger objects, like persisted dataflow graphs.\n#\n# Must be a durable file system that is accessible from all nodes\n# (like HDFS, S3, Ceph, nfs, ...)\n#\n# high-availability.storageDir: hdfs:///flink/ha/\n\n# The list of ZooKeeper quorum peers that coordinate the high-availability\n# setup. This must be a list of the form:\n# \"host1:clientPort,host2:clientPort,...\" (default clientPort: 2181)\n#\n# high-availability.zookeeper.quorum: localhost:2181\n\n\n# ACL options are based on https://zookeeper.apache.org/doc/r3.1.2/zookeeperProgrammers.html#sc_BuiltinACLSchemes\n# It can be either \"creator\" (ZOO_CREATE_ALL_ACL) or \"open\" (ZOO_OPEN_ACL_UNSAFE)\n# The default value is \"open\" and it can be changed to \"creator\" if ZK security is enabled\n#\n# high-availability.zookeeper.client.acl: open\n\n#==============================================================================\n# Fault tolerance and checkpointing\n#==============================================================================\n\n# The backend that will be used to store operator state checkpoints if\n# checkpointing is enabled. Checkpointing is enabled when execution.checkpointing.interval > 0.\n#\n# Execution checkpointing related parameters. Please refer to CheckpointConfig and ExecutionCheckpointingOptions for more details.\n#\n# execution.checkpointing.interval: 3min\n# execution.checkpointing.externalized-checkpoint-retention: [DELETE_ON_CANCELLATION, RETAIN_ON_CANCELLATION]\n# execution.checkpointing.max-concurrent-checkpoints: 1\n# execution.checkpointing.min-pause: 0\n# execution.checkpointing.mode: [EXACTLY_ONCE, AT_LEAST_ONCE]\n# execution.checkpointing.timeout: 10min\n# execution.checkpointing.tolerable-failed-checkpoints: 0\n# execution.checkpointing.unaligned: false\n#\n# Supported backends are 'hashmap', 'rocksdb', or the\n# <class-name-of-factory>.\n#\n# state.backend: hashmap\n\n# Directory for checkpoints filesystem, when using any of the default bundled\n# state backends.\n#\n# state.checkpoints.dir: hdfs://namenode-host:port/flink-checkpoints\n\n# Default target directory for savepoints, optional.\n#\n# state.savepoints.dir: hdfs://namenode-host:port/flink-savepoints\n\n# Flag to enable/disable incremental checkpoints for backends that\n# support incremental checkpoints (like the RocksDB state backend).\n#\n# state.backend.incremental: false\n\n# The failover strategy, i.e., how the job computation recovers from task failures.\n# Only restart tasks that may have been affected by the task failure, which typically includes\n# downstream tasks and potentially upstream tasks if their produced data is no longer available for consumption.\n\njobmanager.execution.failover-strategy: region\n\n#==============================================================================\n# REST & web frontend\n#==============================================================================\n\n# The port to which the REST client connects to. If rest.bind-port has\n# not been specified, then the server will bind to this port as well.\n#\n#rest.port: 8081\n\n# The address to which the REST client will connect to\n#\nrest.address: localhost\n\n# Port range for the REST and web server to bind to.\n#\n#rest.bind-port: 8080-8090\n\n# The address that the REST & web server binds to\n# By default, this is localhost, which prevents the REST & web server from\n# being able to communicate outside of the machine/container it is running on.\n#\n# To enable this, set the bind address to one that has access to outside-facing\n# network interface, such as 0.0.0.0.\n#\nrest.bind-address: localhost\n\n# Flag to specify whether job submission is enabled from the web-based\n# runtime monitor. Uncomment to disable.\n\n#web.submit.enable: false\n\n# Flag to specify whether job cancellation is enabled from the web-based\n# runtime monitor. Uncomment to disable.\n\n#web.cancel.enable: false\n\n#==============================================================================\n# Advanced\n#==============================================================================\n\n# Override the directories for temporary files. If not specified, the\n# system-specific Java temporary directory (java.io.tmpdir property) is taken.\n#\n# For framework setups on Yarn, Flink will automatically pick up the\n# containers' temp directories without any need for configuration.\n#\n# Add a delimited list for multiple directories, using the system directory\n# delimiter (colon ':' on unix) or a comma, e.g.:\n#     /data1/tmp:/data2/tmp:/data3/tmp\n#\n# Note: Each directory entry is read from and written to by a different I/O\n# thread. You can include the same directory multiple times in order to create\n# multiple I/O threads against that directory. This is for example relevant for\n# high-throughput RAIDs.\n#\n# io.tmp.dirs: /tmp\n\n# The classloading resolve order. Possible values are 'child-first' (Flink's default)\n# and 'parent-first' (Java's default).\n#\n# Child first classloading allows users to use different dependency/library\n# versions in their application than those in the classpath. Switching back\n# to 'parent-first' may help with debugging dependency issues.\n#\n# classloader.resolve-order: child-first\n\n# The amount of memory going to the network stack. These numbers usually need\n# no tuning. Adjusting them may be necessary in case of an \"Insufficient number\n# of network buffers\" error. The default min is 64MB, the default max is 1GB.\n#\n# taskmanager.memory.network.fraction: 0.1\n# taskmanager.memory.network.min: 64mb\n# taskmanager.memory.network.max: 1gb\n\n#==============================================================================\n# Flink Cluster Security Configuration\n#==============================================================================\n\n# Kerberos authentication for various components - Hadoop, ZooKeeper, and connectors -\n# may be enabled in four steps:\n# 1. configure the local krb5.conf file\n# 2. provide Kerberos credentials (either a keytab or a ticket cache w/ kinit)\n# 3. make the credentials available to various JAAS login contexts\n# 4. configure the connector to use JAAS/SASL\n\n# The below configure how Kerberos credentials are provided. A keytab will be used instead of\n# a ticket cache if the keytab path and principal are set.\n\n{% if security_enabled %}\nsecurity.kerberos.login.use-ticket-cache: true\nsecurity.kerberos.login.keytab: {{security_kerberos_login_keytab}}\nsecurity.kerberos.login.principal: {{security_kerberos_login_principal}}\n{% else %}\n# security.kerberos.login.use-ticket-cache: true\n# security.kerberos.login.keytab: /path/to/kerberos/keytab\n# security.kerberos.login.principal: flink-user\n{% endif %}\n# The configuration below defines which JAAS login contexts\n\n# security.kerberos.login.contexts: Client,KafkaClient\n\n#==============================================================================\n# ZK Security Configuration\n#==============================================================================\n\n# Below configurations are applicable if ZK ensemble is configured for security\n\n# Override below configuration to provide custom ZK service name if configured\n# zookeeper.sasl.service-name: zookeeper\n\n# The configuration below must match one of the values set in \"security.kerberos.login.contexts\"\n# zookeeper.sasl.login-context-name: Client\n\n#==============================================================================\n# HistoryServer\n#==============================================================================\n\n# The HistoryServer is started and stopped via bin/historyserver.sh (start|stop)\n\n# Directory to upload completed jobs to. Add this directory to the list of\n# monitored directories of the HistoryServer as well (see below).\njobmanager.archive.fs.dir: {{jobmanager_archive_fs_dir}}\n\n# The address under which the web-based HistoryServer listens.\n#historyserver.web.address: 0.0.0.0\n\n# The port under which the web-based HistoryServer listens.\nhistoryserver.web.port: {{historyserver_web_port}}\n\n# Comma separated list of directories to monitor for completed jobs.\nhistoryserver.archive.fs.dir: {{historyserver_archive_fs_dir}}\n\n# Interval in milliseconds for refreshing the monitored directories.\nhistoryserver.archive.fs.refresh-interval: {{historyserver_archive_fs_refresh_interval}}",
                    "historyserver.archive.fs.dir":"hdfs:///completed-jobs/",
                    "historyserver.archive.fs.refresh-interval":"10000",
                    "historyserver.web.port":"8082",
                    "jobmanager.archive.fs.dir":"hdfs:///completed-jobs/",
                    "security.kerberos.login.keytab":"none",
                    "security.kerberos.login.principal":"none"
                }
            }
        },
        {
            "flink-env":{
                "properties_attributes":{

                },
                "properties":{
                    "flink_log_dir":"/var/log/flink",
                    "flink_pid_dir":"/var/run/flink",
                    "flink_group":"flink",
                    "flink_user":"flink"
                }
            }
        },
        {
            "flink-log4j-cli-properties":{
                "properties_attributes":{

                },
                "properties":{
                    "content":"\n#########################################################################\n#  Licensed to the Apache Software Foundation (ASF) under one\n#  or more contributor license agreements.  See the NOTICE file\n#  distributed with this work for additional information\n#  regarding copyright ownership.  The ASF licenses this file\n#  to you under the Apache License, Version 2.0 (the\n#  \"License\"); you may not use this file except in compliance\n#  with the License.  You may obtain a copy of the License at\n#\n#      http://www.apache.org/licenses/LICENSE-2.0\n#\n#  Unless required by applicable law or agreed to in writing, software\n#  distributed under the License is distributed on an \"AS IS\" BASIS,\n#  WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n#  See the License for the specific language governing permissions and\n# limitations under the License.\n################################################################################\n\n# Allows this configuration to be modified at runtime. The file will be checked every 30 seconds.\nmonitorInterval=30\n\nrootLogger.level = INFO\nrootLogger.appenderRef.file.ref = FileAppender\n\n# Log all infos in the given file\nappender.file.name = FileAppender\nappender.file.type = FILE\nappender.file.append = false\nappender.file.fileName = ${sys:log.file}\nappender.file.layout.type = PatternLayout\nappender.file.layout.pattern = %d{yyyy-MM-dd HH:mm:ss,SSS} %-5p %-60c %x - %m%n\n\n# Log output from org.apache.flink.yarn to the console. This is used by the\n# CliFrontend class when using a per-job YARN cluster.\nlogger.yarn.name = org.apache.flink.yarn\nlogger.yarn.level = INFO\nlogger.yarn.appenderRef.console.ref = ConsoleAppender\nlogger.yarncli.name = org.apache.flink.yarn.cli.FlinkYarnSessionCli\nlogger.yarncli.level = INFO\nlogger.yarncli.appenderRef.console.ref = ConsoleAppender\nlogger.hadoop.name = org.apache.hadoop\nlogger.hadoop.level = INFO\nlogger.hadoop.appenderRef.console.ref = ConsoleAppender\n\n# Make sure hive logs go to the file.\nlogger.hive.name = org.apache.hadoop.hive\nlogger.hive.level = INFO\nlogger.hive.additivity = false\nlogger.hive.appenderRef.file.ref = FileAppender\n\n# Log output from org.apache.flink.kubernetes to the console.\nlogger.kubernetes.name = org.apache.flink.kubernetes\nlogger.kubernetes.level = INFO\nlogger.kubernetes.appenderRef.console.ref = ConsoleAppender\n\nappender.console.name = ConsoleAppender\nappender.console.type = CONSOLE\nappender.console.layout.type = PatternLayout\nappender.console.layout.pattern = %d{yyyy-MM-dd HH:mm:ss,SSS} %-5p %-60c %x - %m%n\n\n# suppress the warning that hadoop native libraries are not loaded (irrelevant for the client)\nlogger.hadoopnative.name = org.apache.hadoop.util.NativeCodeLoader\nlogger.hadoopnative.level = OFF\n\n# Suppress the irrelevant (wrong) warnings from the Netty channel handler\nlogger.netty.name = org.jboss.netty.channel.DefaultChannelPipeline\nlogger.netty.level = OFF"
                }
            }
        },
        {
            "flink-log4j-console-properties":{
                "properties_attributes":{

                },
                "properties":{
                    "content":"\n################################################################################\n#  Licensed to the Apache Software Foundation (ASF) under one\n#  or more contributor license agreements.  See the NOTICE file\n#  distributed with this work for additional information\n#  regarding copyright ownership.  The ASF licenses this file\n#  to you under the Apache License, Version 2.0 (the\n#  \"License\"); you may not use this file except in compliance\n#  with the License.  You may obtain a copy of the License at\n#\n#      http://www.apache.org/licenses/LICENSE-2.0\n#\n#  Unless required by applicable law or agreed to in writing, software\n#  distributed under the License is distributed on an \"AS IS\" BASIS,\n#  WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n#  See the License for the specific language governing permissions and\n# limitations under the License.\n################################################################################\n\n# Allows this configuration to be modified at runtime. The file will be checked every 30 seconds.\nmonitorInterval=30\n\n# This affects logging for both user code and Flink\nrootLogger.level = INFO\nrootLogger.appenderRef.console.ref = ConsoleAppender\nrootLogger.appenderRef.rolling.ref = RollingFileAppender\n\n# Uncomment this if you want to _only_ change Flink's logging\n#logger.flink.name = org.apache.flink\n#logger.flink.level = INFO\n\n# The following lines keep the log level of common libraries/connectors on\n# log level INFO. The root logger does not override this. You have to manually\n# change the log levels here.\nlogger.akka.name = akka\nlogger.akka.level = INFO\nlogger.kafka.name= org.apache.kafka\nlogger.kafka.level = INFO\nlogger.hadoop.name = org.apache.hadoop\nlogger.hadoop.level = INFO\nlogger.zookeeper.name = org.apache.zookeeper\nlogger.zookeeper.level = INFO\nlogger.shaded_zookeeper.name = org.apache.flink.shaded.zookeeper3\nlogger.shaded_zookeeper.level = INFO\n\n# Log all infos to the console\nappender.console.name = ConsoleAppender\nappender.console.type = CONSOLE\nappender.console.layout.type = PatternLayout\nappender.console.layout.pattern = %d{yyyy-MM-dd HH:mm:ss,SSS} %-5p %-60c %x - %m%n\n\n# Log all infos in the given rolling file\nappender.rolling.name = RollingFileAppender\nappender.rolling.type = RollingFile\nappender.rolling.append = true\nappender.rolling.fileName = ${sys:log.file}\nappender.rolling.filePattern = ${sys:log.file}.%i\nappender.rolling.layout.type = PatternLayout\nappender.rolling.layout.pattern = %d{yyyy-MM-dd HH:mm:ss,SSS} %-5p %-60c %x - %m%n\nappender.rolling.policies.type = Policies\nappender.rolling.policies.size.type = SizeBasedTriggeringPolicy\nappender.rolling.policies.size.size=100MB\nappender.rolling.policies.startup.type = OnStartupTriggeringPolicy\nappender.rolling.strategy.type = DefaultRolloverStrategy\nappender.rolling.strategy.max = ${env:MAX_LOG_FILE_NUMBER:-10}\n\n# Suppress the irrelevant (wrong) warnings from the Netty channel handler\nlogger.netty.name = org.jboss.netty.channel.DefaultChannelPipeline\nlogger.netty.level = OFF"
                }
            }
        },
        {
            "flink-log4j-properties":{
                "properties_attributes":{

                },
                "properties":{
                    "content":"\n################################################################################\n#  Licensed to the Apache Software Foundation (ASF) under one\n#  or more contributor license agreements.  See the NOTICE file\n#  distributed with this work for additional information\n#  regarding copyright ownership.  The ASF licenses this file\n#  to you under the Apache License, Version 2.0 (the\n#  \"License\"); you may not use this file except in compliance\n#  with the License.  You may obtain a copy of the License at\n#\n#      http://www.apache.org/licenses/LICENSE-2.0\n#\n#  Unless required by applicable law or agreed to in writing, software\n#  distributed under the License is distributed on an \"AS IS\" BASIS,\n#  WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n#  See the License for the specific language governing permissions and\n# limitations under the License.\n################################################################################\n\n# Allows this configuration to be modified at runtime. The file will be checked every 30 seconds.\nmonitorInterval=30\n\n# This affects logging for both user code and Flink\nrootLogger.level = INFO\nrootLogger.appenderRef.file.ref = MainAppender\n\n# Uncomment this if you want to _only_ change Flink's logging\n#logger.flink.name = org.apache.flink\n#logger.flink.level = INFO\n\n# The following lines keep the log level of common libraries/connectors on\n# log level INFO. The root logger does not override this. You have to manually\n# change the log levels here.\nlogger.akka.name = akka\nlogger.akka.level = INFO\nlogger.kafka.name= org.apache.kafka\nlogger.kafka.level = INFO\nlogger.hadoop.name = org.apache.hadoop\nlogger.hadoop.level = INFO\nlogger.zookeeper.name = org.apache.zookeeper\nlogger.zookeeper.level = INFO\nlogger.shaded_zookeeper.name = org.apache.flink.shaded.zookeeper3\nlogger.shaded_zookeeper.level = INFO\n\n# Log all infos in the given file\nappender.main.name = MainAppender\nappender.main.type = RollingFile\nappender.main.append = true\nappender.main.fileName = ${sys:log.file}\nappender.main.filePattern = ${sys:log.file}.%i\nappender.main.layout.type = PatternLayout\nappender.main.layout.pattern = %d{yyyy-MM-dd HH:mm:ss,SSS} %-5p %-60c %x - %m%n\nappender.main.policies.type = Policies\nappender.main.policies.size.type = SizeBasedTriggeringPolicy\nappender.main.policies.size.size = 100MB\nappender.main.policies.startup.type = OnStartupTriggeringPolicy\nappender.main.strategy.type = DefaultRolloverStrategy\nappender.main.strategy.max = ${env:MAX_LOG_FILE_NUMBER:-10}"
                }
            }
        },
        {
            "flink-log4j-session-properties":{
                "properties_attributes":{

                },
                "properties":{
                    "content":"\n################################################################################\n#  Licensed to the Apache Software Foundation (ASF) under one\n#  or more contributor license agreements.  See the NOTICE file\n#  distributed with this work for additional information\n#  regarding copyright ownership.  The ASF licenses this file\n#  to you under the Apache License, Version 2.0 (the\n#  \"License\"); you may not use this file except in compliance\n#  with the License.  You may obtain a copy of the License at\n#\n#      http://www.apache.org/licenses/LICENSE-2.0\n#\n#  Unless required by applicable law or agreed to in writing, software\n#  distributed under the License is distributed on an \"AS IS\" BASIS,\n#  WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n#  See the License for the specific language governing permissions and\n# limitations under the License.\n################################################################################\n\n# Allows this configuration to be modified at runtime. The file will be checked every 30 seconds.\nmonitorInterval=30\n\nrootLogger.level = INFO\nrootLogger.appenderRef.console.ref = ConsoleAppender\n\nappender.console.name = ConsoleAppender\nappender.console.type = CONSOLE\nappender.console.layout.type = PatternLayout\nappender.console.layout.pattern = %d{yyyy-MM-dd HH:mm:ss,SSS} %-5p %-60c %x - %m%n\n\n# Suppress the irrelevant (wrong) warnings from the Netty channel handler\nlogger.netty.name = org.jboss.netty.channel.DefaultChannelPipeline\nlogger.netty.level = OFF\nlogger.zookeeper.name = org.apache.zookeeper\nlogger.zookeeper.level = WARN\nlogger.shaded_zookeeper.name = org.apache.flink.shaded.zookeeper3\nlogger.shaded_zookeeper.level = WARN\nlogger.curator.name = org.apache.flink.shaded.org.apache.curator.framework\nlogger.curator.level = WARN\nlogger.runtimeutils.name= org.apache.flink.runtime.util.ZooKeeperUtils\nlogger.runtimeutils.level = WARN\nlogger.runtimeleader.name = org.apache.flink.runtime.leaderretrieval.ZooKeeperLeaderRetrievalDriver\nlogger.runtimeleader.level = WARN"
                }
            }
        },
        {
            "solr-env":{
                "properties_attributes":{

                },
                "properties":{
                    "content":"#!/bin/bash\n# Licensed to the Apache Software Foundation (ASF) under one or more\n# contributor license agreements. See the NOTICE file distributed with\n# this work for additional information regarding copyright ownership.\n# The ASF licenses this file to You under the Apache License, Version 2.0\n# (the \"License\"); you may not use this file except in compliance with\n# the License. You may obtain a copy of the License at\n#\n# http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n# By default the script will use JAVA_HOME to determine which java\n# to use, but you can set a specific path for Solr to use without\n# affecting other Java applications on your server/workstation.\nSOLR_JAVA_HOME={{java64_home}}\n\n# Increase Java Min/Max Heap as needed to support your indexing / query needs\nSOLR_JAVA_MEM=\"-Xms{{solr_min_mem}}m -Xmx{{solr_max_mem}}m\"\n\nSOLR_JAVA_STACK_SIZE=\"-Xss{{solr_java_stack_size}}m\"\n\nGC_LOG_OPTS=\"{{solr_gc_log_opts}} -Xloggc:{{solr_log_dir}}/solr_gc.log\"\n\nGC_TUNE=\"{{solr_gc_tune}}\"\n\n# Set the ZooKeeper connection string if using an external ZooKeeper ensemble\n# e.g. host1:2181,host2:2181/chroot\n# Leave empty if not using SolrCloud\nZK_HOST=\"{{zookeeper_quorum}}{{solr_znode}}\"\n\n# Set the ZooKeeper client timeout (for SolrCloud mode)\nZK_CLIENT_TIMEOUT=\"60000\"\n\n# By default the start script uses \"localhost\"; override the hostname here\n# for production SolrCloud environments to control the hostname exposed to cluster state\nSOLR_HOST=`hostname -f`\n\n# By default the start script uses UTC; override the timezone if needed\n#SOLR_TIMEZONE=\"UTC\"\n\n# Set to true to activate the JMX RMI connector to allow remote JMX client applications\n# to monitor the JVM hosting Solr; set to \"false\" to disable that behavior\n# (false is recommended in production environments)\nENABLE_REMOTE_JMX_OPTS=\"{{solr_jmx_enabled}}\"\n\n# The script will use SOLR_PORT+10000 for the RMI_PORT or you can set it here\nRMI_PORT={{solr_jmx_port}}\n\n# Anything you add to the SOLR_OPTS variable will be included in the java\n# start command line as-is, in ADDITION to other options. If you specify the\n# -a option on start script, those options will be appended as well. Examples:\n#SOLR_OPTS=\"$SOLR_OPTS -Dsolr.autoSoftCommit.maxTime=3000\"\n#SOLR_OPTS=\"$SOLR_OPTS -Dsolr.autoCommit.maxTime=60000\"\n#SOLR_OPTS=\"$SOLR_OPTS -Dsolr.clustering.enabled=true\"\nSOLR_OPTS=\"$SOLR_OPTS -Djava.rmi.server.hostname={{hostname}}\"\n{% if solr_extra_java_opts -%}\nSOLR_OPTS=\"$SOLR_OPTS {{solr_extra_java_opts}}\"\n{% endif %}\n\n# Location where the bin/solr script will save PID files for running instances\n# If not set, the script will create PID files in $SOLR_TIP/bin\nSOLR_PID_DIR={{solr_piddir}}\n\n# Path to a directory where Solr creates index files, the specified directory\n# must contain a solr.xml; by default, Solr will use server/solr\nSOLR_HOME={{solr_datadir}}\n\n# Solr provides a default Log4J configuration properties file in server/resources\n# however, you may want to customize the log settings and file appender location\n# so you can point the script to use a different log4j.properties file\nLOG4J_PROPS={{solr_conf}}/log4j2.xml\n\n# Location where Solr should write logs to; should agree with the file appender\n# settings in server/resources/log4j.properties\nSOLR_LOGS_DIR={{solr_log_dir}}\n\n# Sets the port Solr binds to, default is 8983\nSOLR_PORT={{solr_port}}\n\n# Be sure to update the paths to the correct keystore for your environment\n{% if solr_ssl_enabled %}\nSOLR_SSL_KEY_STORE={{solr_keystore_location}}\nSOLR_SSL_KEY_STORE_PASSWORD={{solr_keystore_password}}\nSOLR_SSL_TRUST_STORE={{solr_truststore_location}}\nSOLR_SSL_TRUST_STORE_PASSWORD={{solr_truststore_password}}\nSOLR_SSL_NEED_CLIENT_AUTH=false\nSOLR_SSL_WANT_CLIENT_AUTH=false\n{% endif %}\n\n# Uncomment to set a specific SSL port (-Djetty.ssl.port=N); if not set\n# and you are using SSL, then the start script will use SOLR_PORT for the SSL port\n#SOLR_SSL_PORT=\n\n{% if security_enabled -%}\nSOLR_JAAS_FILE={{solr_jaas_file}}\nSOLR_KERB_KEYTAB={{solr_web_kerberos_keytab}}\nSOLR_KERB_PRINCIPAL={{solr_web_kerberos_principal}}\nSOLR_OPTS=\"$SOLR_OPTS -Dsolr.hdfs.security.kerberos.principal={{solr_kerberos_principal}}\"\nSOLR_OPTS=\"$SOLR_OPTS {{zk_security_opts}}\"\n\nSOLR_AUTH_TYPE=\"kerberos\"\nSOLR_AUTHENTICATION_OPTS=\" -DauthenticationPlugin=org.apache.solr.security.KerberosPlugin -Djava.security.auth.login.config=$SOLR_JAAS_FILE -Dsolr.kerberos.principal=${SOLR_KERB_PRINCIPAL} -Dsolr.kerberos.keytab=${SOLR_KERB_KEYTAB} -Dsolr.kerberos.cookie.domain=${SOLR_HOST}\"\n{% endif %}",
                    "solr_datadir":"/var/lib/solr/data",
                    "solr_extra_java_opts":"",
                    "solr_gc_log_opts":"-verbose:gc -XX:+PrintHeapAtGC -XX:+PrintGCDetails -XX:+PrintGCDateStamps -XX:+PrintGCTimeStamps -XX:+PrintTenuringDistribution -XX:+PrintGCApplicationStoppedTime -XX:+UseGCLogFileRotation -XX:NumberOfGCLogFiles=15 -XX:GCLogFileSize=200M",
                    "solr_gc_tune":"-XX:NewRatio=3 -XX:SurvivorRatio=4 -XX:TargetSurvivorRatio=90 -XX:MaxTenuringThreshold=8 -XX:+UseConcMarkSweepGC -XX:+UseParNewGC -XX:ConcGCThreads=4 -XX:ParallelGCThreads=4 -XX:+CMSScavengeBeforeRemark -XX:PretenureSizeThreshold=64m -XX:+UseCMSInitiatingOccupancyOnly -XX:CMSInitiatingOccupancyFraction=50 -XX:CMSMaxAbortablePrecleanTime=6000 -XX:+CMSParallelRemarkEnabled -XX:+ParallelRefProcEnabled",
                    "solr_java_stack_size":"1",
                    "solr_jmx_enabled":"false",
                    "solr_jmx_port":"18983",
                    "solr_kerberos_keytab":"/etc/security/keytabs/solr.service.keytab",
                    "solr_kerberos_name_rules":"DEFAULT",
                    "solr_kerberos_principal":"solr",
                    "solr_keystore_location":"/etc/security/serverKeys/infra.solr.keyStore.jks",
                    "solr_keystore_password":"bigdata",
                    "solr_keystore_type":"jks",
                    "solr_log_dir":"/var/log/solr",
                    "solr_maxmem":"2048",
                    "solr_minmem":"1024",
                    "solr_pid_dir":"/var/run/solr",
                    "solr_port":"8983",
                    "solr_ssl_enabled":"false",
                    "solr_truststore_location":"/etc/security/serverKeys/infra.solr.trustStore.jks",
                    "solr_truststore_password":"bigdata",
                    "solr_truststore_type":"jks",
                    "solr_user_nofile_limit":"128000",
                    "solr_user_nproc_limit":"65536",
                    "solr_web_kerberos_keytab":"/etc/security/keytabs/spnego.service.keytab",
                    "solr_web_kerberos_principal":"HTTP/_HOST@EXAMPLE.COM",
                    "solr_znode":"/solr",
                    "solr_zookeeper_external_enabled":"false",
                    "solr_zookeeper_external_principal":"zookeeper/_HOST@EXAMPLE.COM",
                    "solr_zookeeper_quorum":"{{zookeeper_quorum}}",
                    "solr_user":"solr"
                }
            }
        },
        {
            "solr-log4j":{
                "properties_attributes":{

                },
                "properties":{
                    "content":"<!--\nLicensed to the Apache Software Foundation (ASF) under one or more\ncontributor license agreements.  See the NOTICE file distributed with\nthis work for additional information regarding copyright ownership.\nThe ASF licenses this file to You under the Apache License, Version 2.0\n(the \"License\"); you may not use this file except in compliance with\nthe License.  You may obtain a copy of the License at\n\nhttp://www.apache.org/licenses/LICENSE-2.0\n\nUnless required by applicable law or agreed to in writing, software\ndistributed under the License is distributed on an \"AS IS\" BASIS,\nWITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\nSee the License for the specific language governing permissions and\nlimitations under the License.\n-->\n<Configuration>\n  <Appenders>\n\n    <Console name=\"STDOUT\" target=\"SYSTEM_OUT\">\n      <PatternLayout>\n        <Pattern>\n          %d{ISO8601} [%t] %-5p [%X{collection} %X{shard} %X{replica} %X{core}] %C (%F:%L) - %m%n\n        </Pattern>\n      </PatternLayout>\n    </Console>\n\n    <RollingFile\n        name=\"RollingFile\"\n        fileName=\"{{solr_log_dir}}/solr.log\"\n        filePattern=\"{{solr_log_dir}}/solr.log.%i\" >\n      <PatternLayout>\n        <Pattern>\n          %d{ISO8601} [%t] %-5p [%X{collection} %X{shard} %X{replica} %X{core}] %C (%F:%L) - %m%n\n        </Pattern>\n      </PatternLayout>\n      <Policies>\n        <OnStartupTriggeringPolicy />\n        <SizeBasedTriggeringPolicy size=\"{{log_maxfilesize}} MB\"/>\n      </Policies>\n      <DefaultRolloverStrategy max=\"{{log_maxbackupindex}}\"/>\n    </RollingFile>\n\n    <RollingFile\n        name=\"SlowFile\"\n        fileName=\"{{solr_log_dir}}/solr_slow_requests.log\"\n        filePattern=\"{{solr_log_dir}}/solr_slow_requests.log.%i\" >\n      <PatternLayout>\n        <Pattern>\n          %d{ISO8601} [%t] %-5p [%X{collection} %X{shard} %X{replica} %X{core}] %C (%F:%L) - %m%n\n        </Pattern>\n      </PatternLayout>\n      <Policies>\n        <OnStartupTriggeringPolicy />\n        <SizeBasedTriggeringPolicy size=\"{{log_maxfilesize}} MB\"/>\n      </Policies>\n      <DefaultRolloverStrategy max=\"{{log_maxbackupindex}}\"/>\n    </RollingFile>\n\n  </Appenders>\n  <Loggers>\n    <Logger name=\"org.apache.hadoop\" level=\"warn\"/>\n    <Logger name=\"org.apache.solr.update.LoggingInfoStream\" level=\"off\"/>\n    <Logger name=\"org.apache.zookeeper\" level=\"warn\"/>\n    <Logger name=\"org.apache.solr.core.SolrCore.SlowRequest\" level=\"warn\" additivity=\"false\">\n      <AppenderRef ref=\"SlowFile\"/>\n    </Logger>\n\n    <Root level=\"warn\">\n      <AppenderRef ref=\"RollingFile\"/>\n      <!-- <AppenderRef ref=\"STDOUT\"/> -->\n    </Root>\n  </Loggers>\n</Configuration>",
                    "log_maxbackupindex":"9",
                    "log_maxfilesize":"10"
                }
            }
        },
        {
            "solr-security-json":{
                "properties_attributes":{

                },
                "properties":{
                    "content":"",
                    "solr_ranger_audit_service_users":"{default_ranger_audit_users}",
                    "solr_role_atlas":"atlas_user",
                    "solr_role_dev":"dev",
                    "solr_role_logfeeder":"logfeeder_user",
                    "solr_role_logsearch":"logsearch_user",
                    "solr_role_ranger_admin":"ranger_admin_user",
                    "solr_role_ranger_audit":"ranger_audit_user",
                    "solr_security_manually_managed":"false"
                }
            }
        },
        {
            "solr-xml":{
                "properties_attributes":{

                },
                "properties":{
                    "content":"<?xml version=\"1.0\" encoding=\"UTF-8\" ?>\n<!--\nLicensed to the Apache Software Foundation (ASF) under one or more\ncontributor license agreements.  See the NOTICE file distributed with\nthis work for additional information regarding copyright ownership.\nThe ASF licenses this file to You under the Apache License, Version 2.0\n(the \"License\"); you may not use this file except in compliance with\nthe License.  You may obtain a copy of the License at\n\nhttp://www.apache.org/licenses/LICENSE-2.0\n\nUnless required by applicable law or agreed to in writing, software\ndistributed under the License is distributed on an \"AS IS\" BASIS,\nWITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\nSee the License for the specific language governing permissions and\nlimitations under the License.\n-->\n<solr>\n\n  <int name=\"maxBooleanClauses\">${solr.max.booleanClauses:1024}</int>\n  <str name=\"sharedLib\">${solr.sharedLib:}</str>\n  <str name=\"allowPaths\">${solr.allowPaths:}</str>\n\n  <solrcloud>\n\n    <str name=\"host\">${host:}</str>\n    <int name=\"hostPort\">${solr.port.advertise:0}</int>\n    <str name=\"hostContext\">${hostContext:solr}</str>\n\n    <bool name=\"genericCoreNodeNames\">${genericCoreNodeNames:true}</bool>\n\n    <int name=\"zkClientTimeout\">${zkClientTimeout:30000}</int>\n    <int name=\"distribUpdateSoTimeout\">${distribUpdateSoTimeout:600000}</int>\n    <int name=\"distribUpdateConnTimeout\">${distribUpdateConnTimeout:60000}</int>\n    <str name=\"zkCredentialsProvider\">${zkCredentialsProvider:org.apache.solr.common.cloud.DefaultZkCredentialsProvider}</str>\n    <str name=\"zkACLProvider\">${zkACLProvider:org.apache.solr.common.cloud.DefaultZkACLProvider}</str>\n\n  </solrcloud>\n\n  <shardHandlerFactory name=\"shardHandlerFactory\"\n    class=\"HttpShardHandlerFactory\">\n    <int name=\"socketTimeout\">${socketTimeout:600000}</int>\n    <int name=\"connTimeout\">${connTimeout:60000}</int>\n    <str name=\"shardsWhitelist\">${solr.shardsWhitelist:}</str>\n  </shardHandlerFactory>\n\n  <metrics enabled=\"${metricsEnabled:true}\"/>\n\n</solr>"
                }
            }
        }
    ],
    "host_groups":[
        {
            "name":"host_group_0",
            "components":[
                {
                    "name":"NAMENODE"
                },
                {
                    "name":"SECONDARY_NAMENODE"
                },
                {
                    "name":"RESOURCEMANAGER"
                },
                {
                    "name":"HISTORYSERVER"
                },
                {
                    "name":"HIVE_METASTORE"
                },
                {
                    "name":"WEBHCAT_SERVER"
                },
                {
                    "name":"HIVE_SERVER"
                },
                {
                    "name":"HBASE_MASTER"
                },
                {
                    "name":"ZOOKEEPER_SERVER"
                },
                {
                    "name":"METRICS_COLLECTOR"
                },
                {
                    "name":"METRICS_GRAFANA"
                },
                {
                    "name":"KAFKA_BROKER"
                },
                {
                    "name":"SPARK_JOBHISTORYSERVER"
                },
                {
                    "name":"ZEPPELIN_SERVER"
                },
                {
                    "name":"FLINK_HISTORYSERVER"
                },
                {
                    "name":"SOLR_SERVER"
                },
                {
                    "name":"DATANODE"
                },
                {
                    "name":"NODEMANAGER"
                },
                {
                    "name":"HBASE_REGIONSERVER"
                },
                {
                    "name":"SPARK_THRIFTSERVER"
                },
                {
                    "name":"HDFS_CLIENT"
                },
                {
                    "name":"YARN_CLIENT"
                },
                {
                    "name":"MAPREDUCE2_CLIENT"
                },
                {
                    "name":"TEZ_CLIENT"
                },
                {
                    "name":"HCAT"
                },
                {
                    "name":"HIVE_CLIENT"
                },
                {
                    "name":"HBASE_CLIENT"
                },
                {
                    "name":"ZOOKEEPER_CLIENT"
                },
                {
                    "name":"SPARK_CLIENT"
                },
                {
                    "name":"FLINK_CLIENT"
                }
            ],
            "cardinality":"1"
        }
    ],
    "Blueprints":{
        "blueprint_name":"test",
        "stack_name":"BIGTOP",
        "stack_version":"3.2.0"
    }
}