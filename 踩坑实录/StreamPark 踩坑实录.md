## StreamPark 踩坑实录

* 所以我找到了 StreamPark [安装部署 | Apache StreamPark (incubating)](https://streampark.apache.org/zh-CN/docs/user-guide/deployment)
* 这里涉及到一个环境变量的问题，之前安装 ambari 的时候并没有注意环境变量，我是真的不知道 ambari 在安装的时候是往哪些目录里面安装的，所以这里我只能重新搞这两个镜像，这次一定要在安装界面截图保存。

docker run -itd --name='bigtop320' -p 2929:2929 registry.cn-hangzhou.aliyuncs.com/wujundi/centos-bigtop-3.2.0:ready-for-http

docker run -itd --name='ambari-server' --hostname='ambari-server' -p 8080:8080 -p 8440:8440 -p 8441:8441 registry.cn-hangzhou.aliyuncs.com/wujundi/centos-ambari-280:ready-for-install，然后我还发现。安装流程走到后面，其实会有一个 blueprint 确认的步骤，并且还可以下载 blueprint.zip 压缩包，这个压缩包里面的 json 文件详细的记录了各个组件的配置文档模板，还有各种各样的参数。我在里面搜索 HADOOP_HOME 发现其实有很多行都有这个关键字，看前后文的注释也能看出来，各个组件安装的时候，好像也经常需要先 export 一下这个 HADOOP_HOME 变量，由此我更加相信，ambari 一定是设置过 HADOOP_HOME 的。那么在哪里呢？我又想到像之前操作 HDFS 的时候，就需要切换到专有的用户 hdfs，然后才能操作。那么，会不会 ambari 在安装各个组件的时候，其实也是以各个特定用户的身份进行安装的呢？如果是这样的话，那么 /home 下属的各个文件夹，是不是正式这些用户的家目录呢？按其中时候会有 .bashrc 这类文件承担着配置系统变量的任务呢？答案是，没有。home下的这些文件夹虽然看起来像是用户家目录，但是其中的 .bashrc 等几个文件的内容却空空如也。

于是，我想到了，搜索大法，我用 vs code 从根目录开始，搜索全部带有 HADOOP_HOME 的文件，然后，我就打开了新世界的大门。不仅找到了很多 HADOOP_HOME 的赋值信息，还发现之前在 blueprint.json 里面见到的很多变量，比如 {{hadoop_home}}、{{hadoop_yarn_home}} 这些，在实际的文件中是已经被替换成文件目录了的。然后我从 root/.bashrc 里面看到有 source/etc/profile，进而找到了 /etc/profile，这里也是之前配置 JAVA_HOME 地方，按照 [安装部署 | Apache StreamPark (incubating)](https://streampark.apache.org/zh-CN/docs/user-guide/deployment/) 里面提到了，我配置了这几个环境变量，并且把他们的值替换成了在docker中搜索到的实际值。看起来都挺规整的，基本是 /usr/bigtop/3.2.0/usr/lib/ 下面的一级目录，只有 hbase 特殊一些，我搜到的是 export HBASE_HOME=/usr/bigtop/current/hbase-client，但我感觉是不是也和其他的是一个规则，应该是 exportHBASE_HOME=/usr/bigtop/3.2.0/usr/lib/hbase？？？先用后者吧，有问题记得回来看这一条。

* 在数据库的设置环节，参照之前 ambari 安装页面上的信息，修改了 driver-class-name: com.mysql.jdbc.Driver，并且依然沿用了 按照 [(73条消息) ambari错误及解决方案_smartsense2.7.6_董不懂22的博客-CSDN博客](https://blog.csdn.net/github_39319229/article/details/113052828) 的方式 yum install -y mysql-connector-java ,然后按照 StreamPark 的要求复制了一份到它的lib目录 cp /usr/share/java/mysql-connector-java.jar /home/apache-streampark_2.12-2.1.0-incubating-bin/lib/
* 参考 [Linux下通过shell进MySQL执行SQL或导入脚本 - zifeiy - 博客园 (cnblogs.com)](https://www.cnblogs.com/zifeiy/p/9981253.html) 执行数据初始化脚本。mysql < mysql-schema.sql ，报错 ERROR 1067 (42000) at line 28: Invalid default value for 'create_time'，搜了一下 [(86条消息) mysql为datetime类型的字段设置默认值current_timestamp，引发 Invalid default value for 错误_mysql datetime current_一滴水的眼泪的博客-CSDN博客](https://blog.csdn.net/qq_35112567/article/details/111677052) 所可能是版本太低，mysql --version 一下，果然 Ver 15.1 Distrib 5.5.68-MariaDB, for Linux (x86_64) using readline 5.1，那么，不动现有版本的情况下能解吗？参照 [mysql - MariaDB 中的日期时间 current_timestamp - IT工具网 (coder.work)](https://www.coder.work/article/2468851)，我把 current_timestamp 都改成了 current_timestamp() 的写法，结果还是报错
* 醉了，转而试试 postgresql，参考 [PostgreSQL基本使用方法 - 简书 (jianshu.com)](https://www.jianshu.com/p/814fc0f880b8)，psql < xxxx.sql 或者 psql -f xxxx.sql 都可以。执行倒是也不报错，但是怎么也看不到数据库里面数据有变化呢？在我导入SQL之后，我尝试在 psql 里面重新执行见表语句，提示我表已经存在，那么，建到哪里去了呢？在psql里面使用 \d 终于看到了，原来这个 public 库并不会真正建一个库，所以在 \l 命令的时候，不会限制 public ，而 \d 时候可以直接看到这些表。
* 在 streampark 的 bin 问价夹下面执行 startup.sh 遇到了报错 ERROR: streampark.workspace.local: "/opt/streampark_workspace" is invalid path, Please reconfigure in application.yml 和 NOTE: "streampark.workspace.local" Do not set under APP_HOME(/home/apache-streampark_2.12-2.1.0-incubating-bin). Set it to a secure directory outside of APP_HOME. 可能是因为目前还没有 /opt/streampark_workspace目录，有点搓哦，没有的话不能自动创建一下吗？创建了目录之后，问题解决。成功启动
* 最后，访问 10000 端口，没反应，丢
* 在 /home/apache-streampark_2.12-2.1.0-incubating-bin/logs/streampark.out 看了后台日志，org.postgresql.util.PSQLException: FATAL: Ident authentication failed for user "postgres"，感觉是没有 postgres 用户的权限，再往深了看，Caused by: org.postgresql.util.PSQLException: Connection to localhost:5432 refused. Check that the hostname and port are correct and that the postmaster is accepting TCP/IP connections. 这么说是端口没有映射的问题？映射了端口，也还是报 Ident authentication failed for user "postgres" 的错误。参考了 [【PostgreSQL】FATAL: Ident authentication failed for user - 简书 (jianshu.com)](https://www.jianshu.com/p/d41d09fe18e2) 找到了配置文件在 /var/lib/pgsql/data/pg_hba.conf，然后修改了一下，确实，这次就不再报相同的错误了。
* 新的报错是，database "streampark" does not exist，看起来是数据初始化做的不太对。我手动创建了数据库，然后再启动的时候又报错 relation "t_flink_app" does not exist，所以，是不是整套的见表和数据灌入都需要换到这个库？postgresql 有个模式的概念，库>模式>表，现在的状态是，库是 postgres，模式 public ,表是各个表名。然后 /home/apache-streampark_2.12-2.1.0-incubating-bin/conf/application-pgsql.yml 里面配置的链接属性，url 看起来是在查 streampark 库，但是之前的初始化 SQL，好像没有换库的操作，都是直接干，那么表和数据，其实就灌到了 postgresql 库下面。解决办法就是在见表和数据初始化的 SQL 最前面都加上 \c streampark，先转换到 streampark 库，再见表或者插入数据。
* 这些之后都可以了，在启动，日志报错是 Web server failed to start. Port 10000 was already in use. 看来只剩下端口问题了。然后我修改了 /home/apache-streampark_2.12-2.1.0-incubating-bin/conf/application.yml 里面的 port，修改成了 10086
