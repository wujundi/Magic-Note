# 流式数仓踩坑实录

## 统一NOAH的系统时间

找到 了 https://blog.csdn.net/weixin_55751581/article/details/131419643，这里面说了 spider-flow 的时区问题。按照这个思路，我需要在运行容器的时候指定时区，参考 https://blog.51cto.com/u_16175479/7649742 和 https://zhuanlan.zhihu.com/p/658390008，加上了 -e TZ=Asia/Shanghai 参数，问题解决。

---

## flink 运行环境与sql-client 环境的统一

我把hdfs下面所有的 jar 包都补充到了 flink/lib，运行之后仍然存在报错，java.lang.ClassNotFoundException: org.apache.hadoop.thirdparty.protobuf.Message，还是需要找一下这个类在 Hadoop 的什么模块下面。在源码里面一搜，有好多地方都引用了 import 了 org.apache.hadoop.thirdparty.protobuf.Message。在maven中心仓库搜索了一下，搜到的是 https://central.sonatype.com/search?q=org.apache.hadoop.thirdparty.protobuf，也在代码里面找了一下 pom 文件的版本，发现是 1.1.1，好的，那就下载吧。然后拷贝到了 /usr/bigtop/3.2.0/usr/lib/flink/lib/tmp/hadoop-shaded-protobuf_3_7-1.1.1.jar。这把OK了，在 sql-client 里面也可以创建 paimon_catalog 了。但是又两个遗留问题：

1、是在 exit 的时候会报个错误

Exception in thread "Thread-5" java.lang.IllegalStateException: Trying to access closed classloader. Please check if you store classloaders directly or indirectly in static fields. If the stacktrace suggests that the leak occurs in a third party library and cannot be fixed immediately, you can disable this check with the configuration 'classloader.check-leaked-classloader'.
        at org.apache.flink.runtime.execution.librarycache.FlinkUserCodeClassLoaders$SafetyNetWrapperClassLoader.ensureInner(FlinkUserCodeClassLoaders.java:164)
        at org.apache.flink.runtime.execution.librarycache.FlinkUserCodeClassLoaders$SafetyNetWrapperClassLoader.getResource(FlinkUserCodeClassLoaders.java:183)
        at org.apache.hadoop.conf.Configuration.getResource(Configuration.java:2830)
        at org.apache.hadoop.conf.Configuration.getStreamReader(Configuration.java:3104)
        at org.apache.hadoop.conf.Configuration.loadResource(Configuration.java:3063)
        at org.apache.hadoop.conf.Configuration.loadResources(Configuration.java:3036)
        at org.apache.hadoop.conf.Configuration.loadProps(Configuration.java:2914)
        at org.apache.hadoop.conf.Configuration.getProps(Configuration.java:2896)
        at org.apache.hadoop.conf.Configuration.get(Configuration.java:1246)
        at org.apache.hadoop.conf.Configuration.getTimeDuration(Configuration.java:1863)
        at org.apache.hadoop.conf.Configuration.getTimeDuration(Configuration.java:1840)
        at org.apache.hadoop.util.ShutdownHookManager.getShutdownTimeout(ShutdownHookManager.java:183)
        at org.apache.hadoop.util.ShutdownHookManager.shutdownExecutor(ShutdownHookManager.java:145)
        at org.apache.hadoop.util.ShutdownHookManager.access$300(ShutdownHookManager.java:65)
        at org.apache.hadoop.util.ShutdownHookManager$1.run(ShutdownHookManager.java:102)

在往上找到了 https://blog.csdn.net/h952520296/article/details/114327232，修改了相应的配置，但是重新启动 flink 之后，配置文件就被刷掉了。然后我尝试在 ambari 页面上修改 flink 的配置文件，重启 flink 的时候，底层文件居然同步发生变动了，卧槽，ambari 是有点高效啊，连配置文件都可以在页面上修改。

2、catalog 的创建好像只在 session 内生效，新的 session 里面仍然需要基于hdfs 目录声明 catalog，这个有点反直觉。暂时没找到确切的说法与持久化的选项，暂时先这样吧。
